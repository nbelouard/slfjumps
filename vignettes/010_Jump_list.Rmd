---
title: "#1: Isolating dispersal jumps and diffusive spread"
author: 
- Nadege Belouard^[Temple University, nadege.belouard@gmail.com]
- Sebastiona De Bona^[Temple University, sebastiano.debona@temple.edu]
date: "1/6/2021"
output:
  pdf_document:
    toc: TRUE
    toc_depth: 2
  html_document:
    toc: TRUE
    toc_depth: 3
params:
  display: FALSE
  run: TRUE
  loadfiles: FALSE
  savefiles: TRUE
editor_options: 
  chunk_output_type: console
---

# Aim and setup

The aim of this first vignette is to differentiate diffusive spread from jump dispersal using a simple and conservative method:  
- calculate the distance between each detection point and the introduction site  
- define a distance that SLF are unlikely to disperse autonomously - here, 15 kilometers  
- look for gaps larger than 15 kilometers in the distribution of the distance to the introduction site  
- every detection of SLF found after such a gap is considered to be a jump event, i.e. an event of anthropogenic dispersal potentially leading to the establishment of a new population, if it is situated at least 15 kilometers away from any previous jump.  

Considering that the expansion of the invasion is heterogeneous in space, we divided the invasion range into disk portions with the introduction site as the origin. We rotate disk portions to increase the accuracy of the detection. The optimization of the parameters (number of disk portions, number of rotations) is then described. Only established populations (detection involving more than one individual, as defined in `lycordata`) are used in analyses as they are the only ones susceptible to result in secondary invasions.


```{r setup for rendering, messages = F, warning = F}
# attaching necessary packages
library(tidyverse)
library(magrittr)
library(sf)
library(maps)
library(ggplot2)
library(knitr)
library(here)
library(slfjumps)
library(leaflet)
# library(DescTools)
# library(geosphere)
# library(purrr)
sf::sf_use_s2(FALSE)
```

## States map
```{r states names and centroid for global map, message = FALSE, warning = FALSE, echo = params$display}
# extracts a map of the States and recodes state labels to show the two-letter code rather than the full state name.

# obtaining simple feature objects for states and finding centroids for label positioning
states <- sf::st_as_sf(maps::map("state", plot = FALSE, fill = TRUE))
# sf::st_as_sf(maps::map("county", plot = TRUE, fill = FALSE))
states <- cbind(states, st_coordinates(st_centroid(states)))

# making table key for state 2-letter abbreviations
# the vectors state.abb and state.name contains strings of all
# US states and abbreviations
state_abbr <- tibble(state.name = str_to_lower(state.name), state.abb) %>%
  left_join(tibble(ID = states$ID), ., by = c(ID = "state.name")) %>%
  mutate(state.abb = replace_na(state.abb, ""))

# adding 2-letter codes to sf
states$code <- state_abbr$state.abb

ggplot() +
    geom_sf(data = states, alpha = 0) +
  geom_text(data = states, aes(X, Y, label = code), size = 2) 

```

\newpage


```{r packages and data, message = FALSE, warning = FALSE, echo = params$display}
#load the dataset from lycordata
# This is the last dataset without all the 2021 data
load(file.path(here(), "data", "tinyslf_v6.rda"))

slf_tiny <- tinyslf
dim(slf_tiny) # 327347 rows

# Remove one point in Maryland that is not correct
# This point should already be removed in this version of the dataset
# slf_tiny %>% filter((between(longitude, -77, -76) & between(latitude, 38.8, 39.2) & slf_established == T))
# dim(slf_tiny) #327346 rows

#what years are included?
table(slf_tiny$bio_year)
#we only want 2014 to 2020
slf_tiny %<>% filter(bio_year %in% c(2014:2020))
dim(slf_tiny) #324091 rows
table(slf_tiny$bio_year)
length(unique(slf_tiny$state))

head(slf_tiny)

# Create a folder if it does not exist
testNmkdir("exported-data")

write.csv(slf_tiny, file.path(here(), "exported-data", "slftiny.csv"), 
          row.names = F)
```


# 1. Data initialization

## Data reshaping

In `lycordata`, each survey appears in a row, and states whether SLF were present (one individual found) and/or established (more than one individual or an egg mass found). Multiple surveys were conducted at the same location during the same year, resulting in a complex and redundant dataset of `r dim(slf_tiny)[1]` rows (surveys).

We reshape the table to summarize the information by rounding the geographical coordinates to cells of 1 km^2 (1 km * 1 km), so that one line represents the detection status at a given location for a given year. The code is borrowed from Seba De Bona's `lycordata` vignette to homogenize our data.

Note: when several surveys indicate that SLF are "present" the same year at the same location, we could be tempted to categorize them in the "established" category. However, the category "present" often refers to dead individuals, although this information is not explicitly available. We use a conservative approach and kept the same categories while summarizing the data.

```{r rounding coordinates, echo = params$display, warnings = FALSE, message = FALSE, eval = params$run}
# specifying the width of the mesh, in km
size_of_grid <- 1

# rounding coordinates and summarizing surveys by location and year
# we round the latitude and longitude to 5 decimal places to avoid problems of memory limitations
#this is ok because the size of the grid is 0.009 (1/111) at the smallest (3 decimal places)
grid_data <- slf_tiny %>%
  mutate(latitude_grid = RoundTo(latitude, multiple = size_of_grid/111),
         longitude_grid = RoundTo(longitude, multiple = size_of_grid/85)) %>%
  mutate(latitude_rounded = round(latitude_grid, 5),
         longitude_rounded = round(longitude_grid, 5)) %>% 
  group_by(bio_year, latitude_rounded, longitude_rounded) %>%
  summarise(#slf_present = any(slf_present),
            slf_established = any(slf_established)) %>% 
  ungroup()

knitr::kable(head(grid_data))
dim(grid_data) #58331 rows

table(grid_data$bio_year)
```


Let's look at the points on a map
```{r map all points}
# Map surveys
map <- ggplot(data = states) +
    geom_point(data = grid_data,
             aes(x = longitude_rounded, y = latitude_rounded, 
                 col = slf_established)) + 
  geom_sf(data = states, alpha = 0) +
  geom_text(data = states, aes(X, Y, label = code), size = 2)

map 

# Create a folder if it does not exist
testNmkdir("figures")

ggsave(file.path(here(), "figures", "jump_list", "1.map_allpoints.jpg"), map,
       height = 5, width = 5)

# Number of positive surveys over all surveys (including negative surveys in CA!)
dim(grid_data %>% filter(slf_established == T))[1] #now 7044
# Percentage of positive surveys
dim(grid_data %>% filter(slf_established == T))[1]/dim(grid_data)[1] #12.08% 
```


## Introduction point
Pick the format you need
```{r introduction point}
#Coordinates of the introduction site, extracted from Barringer et al. 2015

# These are the coordinates of the introduction point:
# As a table (for distances calculation):
centroid_df <- data.frame(longitude_rounded = -75.675340, 
                        latitude_rounded = 40.415240) 
# As a vector (for jump analysis):
centroid_vec <- c(-75.675340, 40.415240)

# These are the coordinates of the centroid (very close to the introduction point!)
# As a data frame: 
# centroid <- data.frame(latitude_rounded = mean(grid_data$latitude_rounded),
                       # longitude_rounded = mean(grid_data$longitude_rounded)

# As a vector:
# centroid <- c(grid_data %>% filter(slf_established == T) %>%
#                 summarise(mean(longitude_rounded)) %>% pull(),
#               grid_data %>% filter(slf_established == T) %>% 
#                 summarise(mean(latitude_rounded)) %>% pull())

```


## Distances and status calculation

```{r calculate distances to the introduction point, echo = params$display, eval = params$run}

#Compute distances to the introduction point, in km
grid_data <- grid_data %>% 
  mutate(DistToIntro = distm(grid_data[,c(3,2)], centroid_df, fun=distGeo)/1000)


# We could create a new column containing a more detailed status (established vs present vs undetected)
# but since we only care about established populations, we can just keep the column slf_established!

# # Compute a single column with the SLF status at each point: undetected, present, or established
# grid_data <- grid_data %>%
#   mutate(Status = NA)
# 
# for (i in 1:length(grid_data$bio_year)) {
#   if (grid_data$slf_established[i]) {
#     grid_data$Status[i] = "Established"
#   } else if (grid_data$slf_present[i]) {
#     grid_data$Status[i] = "Present"
#   } else {
#     grid_data$Status[i] = "Undetected"
#   }
# }
# 
# # Put levels of Status in correct order for plots
# grid_data$Status <- factor(grid_data$Status, levels = c("Undetected", "Present", "Established"))

# Save the grid file = points with their distance to the introduction site
write.csv(grid_data, file.path(here(), "exported-data", "grid_data.csv"), 
          row.names = F)
dim(grid_data)[1]
# The number of rows must be 58,331 with 2014-2020 data

table(grid_data$bio_year)
pos <- grid_data %>% filter(slf_established == T)
table(pos$bio_year)
```

We first calculate the distance between each survey point and the introduction point (-75.675340, 40.415240, from Barringer et al., 2015). This distance will be the basis of all subsequent analyses. The summary of this distance is (in kilometers):  
`r summary(grid_data$DistToIntro)`
We also create a variable summarizing the status of the survey for each point: SLF `r levels(grid_data$Status)`.

\newpage


```{r figure sampling effort}

surveys <- as.data.frame(table(slf_tiny$bio_year))
points <- as.data.frame(table(grid_data$bio_year))
positive <- as.data.frame(table(pos$bio_year))

surveys %<>% add_column(Type = "Surveys")
points %<>% add_column(Type = "Points")
positive %<>% add_column(Type = "Positive points")

effort <- rbind(surveys, points, positive)
effort$Type <- factor(effort$Type, 
                      levels = c("Surveys", "Points", "Positive points"))

effort_plot <- ggplot() +
  geom_point(data = effort, aes(x = Var1, y = log(Freq), shape = Type), 
             size = 3) +
  theme_classic() +
  xlab("Year") + ylab("log(count)")
  

ggsave(file.path(here(), "figures", "jump_list", "00. number of surveys.jpg"), 
       effort_plot)

```



# 2. Differentiating diffusive spread and jump dispersal

A custom program searches for each year the distance at which the gap occurs, and returns both the survey before this threshold (the limit of diffusive spread) and a list of surveys found after this threshold (jump events).   
* Note that here, we consider that populations do not go extinct, so that the limit of the diffusive spread cannot be lower in year y than in year y+1. This is because fewer and fewer surveys are conducted near the introduction site over time, leading to the appearance of a false first gap near the introduction site.  
* If a jump event is identified closer than 15 km from a jump from the previous year, it is removed from the list, as SLF likely spread from the jump of the previous year.
* The function runs independently for each disk portion, then rotates the grid of disk portions and runs again as many times as indicated in the function argument.  


## Parameters optimization

```{r load dataset}
grid_data <- read.csv(file.path(here(), "exported-data", "grid_data.csv"), 
                      h = T)
dim(grid_data)
table(grid_data$bio_year)

```

Now we need to figure out the best set of parameters to find the accurate number of jumps, i.e. the highest number of jumps detected by the algorithm. To do so, we run several analysis with extreme sets of parameters to get closer to a plateau in the number of jumps that are found, before a finer optimization is done.

The parameter that increases most the number of jumps is the number of sectors, so we begin with this parameter and a fixed (high) number of rotations, and look for a plateau.

```{r first approximation of the number of sectors}

# First iteration:
i = 12
initial_number_rotations = 10
Results_prev = -1

# Attribute the geographical sector of each point for each rotation
slfdata <- attribute_sectors(grid_data, 
                             nb_sectors = i, 
                             centroid = centroid_vec, 
                             rotation = initial_number_rotations)

slfdata_long <- slfdata %>% 
  pivot_longer(cols = starts_with("rotation"), names_to = "rotation_nb", 
               values_to = "sectors_nb", names_prefix = "rotation", 
               names_transform = list(rotation_nb = as.integer))

# Find the jumps
Results <- find_jumps(slfdata_long, gap_size = 15, bio_year = c(2014:2020))

# Run the loop to find the approximate number of sectors recommended
while (dim(Results$Jump)[1] > Results_prev){
  Results_prev <- dim(Results$Jump)[1]
  j = i
  i = i + 8
  print(paste0("Sectors: ", i))

  slfdata <- attribute_sectors(grid_data, 
                               nb_sectors = i, 
                               centroid = centroid, 
                               rotation = initial_number_rotations)
  slfdata_long <- slfdata %>% 
    pivot_longer(cols = starts_with("rotation"), names_to = "rotation_nb", 
                 values_to = "sectors_nb", names_prefix = "rotation", 
                 names_transform = list(rotation_nb = as.integer)) 
  
  Results <- find_jumps(slfdata_long, gap_size = 15, bio_year = c(2014:2020))
  
  rm(slfdata)
  rm(slfdata_long)
}

print(paste0("The right number of sectors is likely between ", j, " and ", i))

rm(Results)
```

The right number of sectors is close to 20-28 (28-36 without centroid)

```{r first approximation of the number of rotations}
# Then, look at the number of rotations needed
# Run this on the highest number of sectors found above (i).
i = 28
r = 2
Results_prev = -1

slfdata <- attribute_sectors(grid_data, 
                             nb_sectors = i, 
                             centroid = centroid_vec, 
                             rotation = r)
slfdata_long <- slfdata %>% 
  pivot_longer(cols = starts_with("rotation"), names_to = "rotation_nb", 
               values_to = "sectors_nb", names_prefix = "rotation", 
               names_transform = list(rotation_nb = as.integer)) 
Results <- find_jumps(slfdata_long, gap_size = 15, bio_year = c(2014:2020))


while (dim(Results$Jump)[1] > Results_prev){
  Results_prev <- dim(Results$Jump)[1]
  k = r
  r = r + 5
  print(paste0("Rotations: ", r))

  slfdata <- attribute_sectors(grid_data, 
                               nb_sectors = i, 
                               centroid = centroid, 
                               rotation = r)
  slfdata_long <- slfdata %>% 
    pivot_longer(cols = starts_with("rotation"), names_to = "rotation_nb", 
               values_to = "sectors_nb", names_prefix = "rotation", 
               names_transform = list(rotation_nb = as.integer)) 
  Results <- find_jumps(slfdata_long, gap_size = 15, bio_year = c(2014:2020))
  
  rm(slfdata)
  rm(slfdata_long)

}

print(paste0("The number of rotations is likely between ", k, " and ", r))
```

The number of rotations is close to 2-7 (2-7 without centroid).
Use these approximations to run the optimization program!

```{r run optimization}
# Here we are going to run the number of sectors of every multiple of 4 between 20 and 32
# and the number of rotations between 1 and 10

sectors = c(16, 20, 24, 28, 32, 36)
rotations = c(1, 2, 3, 4, 5, 6, 7)
head(grid_data)

optim_list <- data.frame(s = NULL,
                         r = NULL,
                         DistToIntro = NULL,
                         bio_year = NULL,
                         latitude_rounded = NULL,
                         longitude_rounded = NULL,
                         # slf_present = NULL,
                         slf_established = NULL#,
                         # theta = NULL,
                         # DistToSLF = NULL
                         )


for (s in sectors){
  for (r in rotations){
    i = 1
    print(paste0("Sectors: ", s, ", rotations: ", r))
    slfdata <- attribute_sectors(grid_data, 
                                 nb_sectors = s, 
                                 centroid = centroid_vec, 
                                 rotation = r)
    slfdata_long <- slfdata %>% 
      pivot_longer(cols = starts_with("rotation"), names_to = "rotation_nb", 
                   values_to = "sectors_nb", names_prefix = "rotation", 
                   names_transform = list(rotation_nb = as.integer)) 
    Results <- find_jumps(slfdata_long, gap_size = 15, bio_year = c(2014:2020))
    Results$Jump %<>% add_column(r = r, s = s)
    optim_list <- rbind(optim_list, Results$Jump)
    i = i + 1
    
    rm(slfdata)
    rm(slfdata_long)
  }
}

# Create a folder if it does not exist
testNmkdir("tables")

# List of jumps
head(optim_list)
write.csv(optim_list, file.path(here(), "tables", "optim_list.csv"))

# List of the number of jumps found per combination
optim_sum <- optim_list %>% group_by(s, r) %>% summarise(jumps = n())
head(optim_sum)
max(optim_sum$jumps)

write.csv(optim_sum, file.path(here(), "tables", "optim_jumps.csv"))

```

The maximum number of jumps founds is 135.


```{r compare with and without centroid}

optim_list_good <- optim_list %>% filter(s == 24, r == 5) 
dim(optim_list_good)

optim_list_centroid <- read.csv(file.path(here(), "tables", "optim_list.csv"))
optim_list_centroid_good <- optim_list_centroid %>% filter(s == 20, r == 3) 
dim(optim_list_centroid_good)

# All jumps one set
ggplot(data = states) +
  geom_point(data = optim_list_good,
             aes(x = longitude_rounded, y = latitude_rounded),
             size = 4, shape = 19, col = "grey") +
  geom_point(data = optim_list_centroid_good,
             aes(x = longitude_rounded, y = latitude_rounded),
             size = 0.5, shape = 19, col = "black") +
  # geom_point(data = centroid, 
             # aes(x = longitude_rounded, y = latitude_rounded), 
             # col = "blue", shape = 4, size = 5) +
  # geom_point(data = centroid2,
             # aes(x = longitude_rounded, y = latitude_rounded), 
             # col = "red", shape = 4, size = 5) +
  geom_sf(data = states, alpha = 0) + 
  coord_sf(xlim = c(-82, -72), ylim = c(38, 43), expand = FALSE) + 
  theme(legend.position="top") +
  guides(fill = guide_legend("Year"))

```

Plot the optimization

```{r plot optimization}

# Plot it
optim_plot <- ggplot(data = optim_sum, 
                     aes(x = as.factor(r), y = jumps)) +
  geom_bar(lwd = .05, stat = "identity") +
  xlab("Number of rotations") +
  ylab("Number of jumps") +
  facet_wrap(~as.factor(s), nrow = 1) +
  theme_classic() +
  ggtitle("Number of sectors") +
  theme(legend.position = "bottom", text = element_text(size = 12), 
        axis.text.x = element_text(size = 5, angle = 90), 
        plot.title = element_text(size = 12, hjust = 0.5))

optim_plot

ggsave(file.path(here(), "figures", "jump_list", "4.optim_plot.jpg"), 
       optim_plot)
```


## Best dataset
The best result (highest number of jumps in the least amount of time) is given by the dataset with XX rotations and XX sectors.
Rerun it to save results.

```{r run best dataset and save results}

# Right combination:
optim_sum %>% filter(s == 20, r == 3)

slfdata <- attribute_sectors(grid_data, 
                             nb_sectors = 20, 
                             centroid = centroid_vec, 
                             rotation = 3)

slfdata_long <- slfdata %>% 
  pivot_longer(cols = starts_with("rotation"), names_to = "rotation_nb", 
               values_to = "sectors_nb", names_prefix = "rotation",
               names_transform = list(rotation_nb = as.integer)) 

Results <- find_jumps(slfdata_long, gap_size = 15, bio_year = c(2014:2020))

dim(Results$Jump)
head(Results$Dist)

write.csv(Results$Dist, file.path(here(), "exported-data", "thresholds.csv"), 
          row.names = F)
write.csv(Results$Jump, file.path(here(), "exported-data", "jumps.csv"), 
          row.names = F)
```


The jump dispersal events found by the function can be visualized on a map. Jump locations are colored according to their year of appearance, among all the established populations in grey. The introduction site is signaled by a blue cross. We note that most jump events occur in northern Virginia or western Pennsylvania. In Winchester (VA), a diffusive spread appears around jump events, indicating that a secondary invasion began in this area. A similar pattern is found around Harrisburg (PA), although the diffusive spread has now reached Harrisburg too.

```{r map all jumps per params set}

jumps <- Results$Jump

# All jumps one set
map_jumps <- ggplot(data = states) +
  geom_point(data = grid_data %>% 
               filter(slf_established == T, bio_year %in% c(2014:2020)),
             aes(x = longitude_rounded, y = latitude_rounded), 
             size = 1, shape = 19, col = "grey") +
  geom_point(data = centroid_df,
             aes(x = longitude_rounded, y = latitude_rounded), 
             col = "blue", shape = 4, size = 5) +
  geom_point(data = jumps, 
             aes(x = longitude_rounded, y = latitude_rounded, 
                 fill = as.factor(bio_year)), shape = 21, size = 2) +
  scale_fill_manual(values = c("#009E73", "#0072B2", "firebrick3", 
                               "gold2", "black")) +
  geom_text(data = states,
            aes(X, Y, label = code), size = 4) +
  labs(x = "Longitude", y = "Latitude")+
  geom_sf(data = states, alpha = 0) + 
  coord_sf(xlim = c(-82, -72), ylim = c(38, 43), expand = FALSE) + 
  theme(legend.position="top") +
  guides(fill = guide_legend("Year"))

map_jumps

ggsave(file.path(here(), "figures", "jump_list", "map_jumps.jpg"), 
       map_jumps , width = 8, height = 8)


```



\newpage


# 3. Rarefy outbreaks

```{r load datasets}

jumps <- read.csv(file.path(here(), "exported-data", "jumps.csv"), h = T)
grid_data <- read.csv(file.path(here(), "exported-data", "grid_data.csv"), 
                      h = T)
```


Find points with important outbreaks

```{r run group_jumps function to define groups}

# The function needs to run separately for each set of parameters since the list of jumps is not the same!
Jump_groups <- group_jumps(jumps, gap_size = 15)

#Check how many points there are per group
Jump_groups %>% group_by(bio_year, Group) %>% summarise(Nb = n()) %>% 
  arrange(-Nb) %>% 
  filter(Nb > 1)


#Check on a map if it worked!
# Zoom on groups to see if groups make sense
pal <- colorFactor(rep(rainbow(5), 8), 
                   domain = unique(Jump_groups$Group))

leaflet(data = Jump_groups) %>% 
      addTiles() %>% 
      addCircleMarkers(lng = ~longitude_rounded, 
                       lat = ~latitude_rounded, 
                       color = ~pal(Group), 
                       label = ~Group)  

write.csv(Jump_groups, file.path(here(), "exported-data", "jump_groups.csv"), 
          row.names = F)
```


Most jump events occurred in Harrisburg, PA, and Winchester, VA, in 2018. They might be true independent jumps, i.e. SLF hitchhiked multiple times to these locations the same year. Alternatively, they might be the result of SLF quickly spreading from a single jump event. Finally, they can be a mix between these two hypotheses. For the rest of the analyses, we will test the two most contrasted hypotheses in parallel in order to test whether results vary. For the first hypothesis (all points are independent introductions), the dataset consists of all jump points. For the second hypothesis (only one introduction in Harrisburg and Winchester), the dataset consists of each "grouped jumps" summarized each by their most central point.


```{r run rarify_groups function on all datasets}
Jumps_unique <- rarefy_groups(Jump_groups) %>% add_column(Rarefied = TRUE)
dim(Jumps_unique)

# Map it
map_rarefied <- ggplot(data = states) +
  geom_sf(data = states, fill = "white") +
  coord_sf(xlim = c(-82, -72), ylim = c(38, 43), expand = FALSE) +
  geom_point(data = Jump_groups,
             aes(x = longitude_rounded, y = latitude_rounded, 
                 col = as.factor(Group)), shape = 19, size = 3, 
             show.legend = F) +
  geom_point(data = Jumps_unique, 
             aes(x = longitude_rounded, y = latitude_rounded)) +
  labs(x = "Longitude", y = "Latitude")+
  theme(legend.position="right")

map_rarefied

ggsave(file.path(here(), "figures", "jump_list", "6.map_rarefication.jpg"),
       map_rarefied, width = 5, height = 5)
```


## Assemble datasets
Assemble the datasets to have only one big dataset with all jumpers: different sets of parameters, full, and reduced.

```{r assemble all datasets into one}

Jumps_unique %<>% 
  select("longitude_rounded", "latitude_rounded", "bio_year", "Rarefied")

Jumps_full_rarefied <- merge(jumps, Jumps_unique, 
                             by = c("latitude_rounded", "longitude_rounded", 
                                    "bio_year"), all = T)

dim(jumps)[1] == dim(Jumps_full_rarefied)[1]
dim(Jumps_full_rarefied %>% filter(Rarefied == TRUE))[1] == dim(Jumps_unique)[1]

write.csv(Jumps_full_rarefied, 
          file.path(here(), "exported-data", "jumps_full_rarefied.csv"), 
          row.names = F)
```


# 4. Preliminary analysis

Load all datasets
```{r load datasets for analysis}
Jumps <- read.csv(file.path(here(), "exported-data", "jumps_full_rarefied.csv"))
Jumps %<>% mutate(Rarefied = ifelse(is.na(Rarefied), "Full", "Rarefied"))
dim(Jumps)
Jumps %>% filter(bio_year == 2020)
Jumps$Rarefied <- factor(Jumps$Rarefied, levels = c( "Full", "Rarefied"))

Thresholds <- read.csv(file.path(here(), "exported-data", "thresholds.csv"))
grid_data <- read.csv(file.path(here(), "exported-data", "grid_data.csv"), h=T)
centroid <- data.frame(longitude_rounded = -75.675340, 
                       latitude_rounded = 40.415240)
jump_groups <- read.csv(file.path(here(), "exported-data", "jump_groups.csv"))
```


## Map of jumps

```{r jumps map}

map_rarefied <- ggplot(data = grid_data) +
  geom_sf(data = states, fill = "white") +
  geom_point(data = grid_data %>% filter(slf_established == T), 
             aes(x = longitude_rounded, y = latitude_rounded), 
             col = "lightgrey") +
  geom_sf(data = states, alpha = 0) +
  geom_point(data = centroid, aes(x = longitude_rounded, y = latitude_rounded), 
             col = "black", shape = 4, size = 5) +
  geom_point(data = Jumps,
             aes(x = longitude_rounded, y = latitude_rounded, 
                 col = as.factor(bio_year), stroke = Rarefied, group = Rarefied, 
                 shape = Rarefied), size = 4) +
  
  scale_discrete_manual(aesthetics = "stroke", 
                        values = c('Rarefied' = 1, 'Full' = 2)) +
  scale_discrete_manual(aesthetics = "shape", 
                        values = c('Rarefied' = 19, 'Full' = 21)) +
  scale_color_manual(values = c("gold2", "firebrick3", "#0072B2", "#009E73")) +
  scale_fill_manual(values = c("gold2", "firebrick3", "#0072B2", "#009E73")) +
  scale_alpha_manual(values = c(0.5, 1)) +
  
  coord_sf(xlim = c(-81, -73), ylim = c(38, 43)) +
  labs(x = "Longitude", y = "Latitude") +
  theme(legend.position="right", text = element_text(size = 10),
        panel.background = element_rect(fill = "white"),
        legend.key = element_rect(fill = "white")) +
   guides(colour = guide_legend("Biological year"), 
          alpha = guide_legend("Dataset"), 
          fill = guide_legend("Biological year"))

map_rarefied

ggsave(file.path(here(), "figures", "jump_list", "7.map_jumpsv2.jpg"), 
       map_rarefied, height = 8, width = 8)
```


## Number of jumps per year

```{r number of jumps per year}

# Bar plot of the number of jumps per year
Jumps_year <- Jumps %>% group_by(bio_year, Rarefied) %>% summarise(n = n())
 
jumps_plot <- ggplot() +
  geom_bar(data = Jumps_year, 
           aes(x = bio_year, y = n, fill = as.factor(bio_year), 
               group = Rarefied, col = as.factor(bio_year), 
               alpha = Rarefied), stat = "identity", lwd = .25) +
  scale_fill_manual(values = c("gold2", "firebrick3", "#0072B2", "#009E73")) +
  # scale_alpha_manual(values = c(0, 1)) +
  scale_color_manual(values = c("gold2", "firebrick3", "#0072B2", "#009E73")) +
  xlab("Biological year") +
  ylab("Number of jumps") +
  theme_classic() +
  guides(alpha = "none", fill = "none", col = "none") +
  theme(text = element_text(size = 10), legend.position = "top")

jumps_plot

ggsave(file.path(here(), "figures", "jump_list", "8.jumps_number.jpg"), 
       jumps_plot, height = 2.5, width = 4)
```


## Mean distance to the invasion front per year

```{r mean dist to threshold per year}

Thresholds %<>% filter(rotation_nb == 1) %>% 
  select(longitude_rounded, latitude_rounded, bio_year, sectors_nb) %>% 
  rename(sectors = sectors_nb,
         latitude_threshold = latitude_rounded,
         longitude_threshold = longitude_rounded)

# Attribute sectors to each jump: find sector for rotation 0
Jumps_sectors <- attribute_sectors(Jumps, 
                                   nb_sectors = 20, 
                                   centroid = c(-75.675340, 40.415240), 
                                   rotation = 1) 
Jumps_sectors %<>% select(-c(theta.1, sectors)) %>% rename(sectors = rotation1)

# Find corresponding threshold per bio year 
JumpLength <- left_join(Jumps_sectors, Thresholds)
JumpLength %<>% rowwise() %>%  
  mutate(DistToThreshold = as.vector(distm(c(longitude_rounded, latitude_rounded), 
                                           c(longitude_threshold, latitude_threshold), 
                                           fun = distGeo))/1000)

write.csv(JumpLength, file.path(here(), "exported-data", "DistToThreshold.csv"))

JumpLength_tot <- rbind(JumpLength %>% filter(Rarefied == "Rarefied"),
                        JumpLength %>% mutate(Rarefied = "Full"))
dim(JumpLength_tot)[1] == 37+135



# Jitter
JumpLength_tot$Rarefied <- factor(JumpLength_tot$Rarefied, 
                                  levels = c("Rarefied", "Full"))

MeanDist_jump <- ggplot() +
  geom_jitter(data = JumpLength_tot,
             aes(x = as.factor(bio_year), y = DistToThreshold, 
                 col = as.factor(bio_year),
                 fill = as.factor(bio_year),
                 alpha = Rarefied), size = 4) +
  scale_alpha_manual(values = c(1,0.4)) +
  scale_discrete_manual(aesthetics = "shape", 
                        values = c('Rarefied' = 19, 'Full' = 21)) +
  scale_color_manual(values = c("gold2", "firebrick3", "#0072B2", "#009E73")) +
  scale_fill_manual(values = c("gold2", "firebrick3", "#0072B2", "#009E73")) +
  labs(x = "Dataset", y = "Distance to the invasion front (km)") +
  theme_classic() +
  theme(legend.position="top", text = element_text(size = 10), 
        plot.tag.position = c(0.01, 1)) +
  guides(fill = "none", shape = "none", col = "none", alpha = "none")
MeanDist_jump

ggsave(file.path(here(), "figures", "jump_list", "9.jumps_number_jitter.jpg"), 
       MeanDist_jump, height = 2.5, width = 4)


# Boxplot
JumpLength_tot$Rarefied <- factor(JumpLength_tot$Rarefied, 
                                  levels = c("Rarefied", "Full"))
MeanDist_jump <- ggplot() +
  geom_boxplot(data = JumpLength_tot,
             aes(x = as.factor(bio_year), y = DistToThreshold, 
                 col = as.factor(bio_year),
                 fill = as.factor(bio_year),
                 alpha = Rarefied)) +
  scale_alpha_manual(values = c(0.7,0)) +
  scale_discrete_manual(aesthetics = "shape", 
                        values = c('Rarefied' = 19, 'Full' = 21)) +
  scale_color_manual(values = c("gold2", "firebrick3", "#0072B2", "#009E73")) +
  scale_fill_manual(values = c("gold2", "firebrick3", "#0072B2", "#009E73")) +
  labs(x = "Dataset", y = "Distance to the invasion front (km)") +
  theme_classic() +
  theme(legend.position="top", text = element_text(size = 10), 
        plot.tag.position = c(0.01, 1)) + 
  guides(fill = "none", shape = "none", col = "none", alpha = "none")
MeanDist_jump


ggsave(file.path(here(), "figures", "jump_list", "10.jumps_number_boxplot.jpg"), 
       MeanDist_jump, height = 2.5, width = 4)

# Summary
JumpLength_tot %>% group_by(Rarefied, bio_year) %>% 
  summarise(mean(DistToThreshold))
summary(aov(DistToThreshold ~ bio_year, data = JumpLength)) #p = 0.37

summary(aov(DistToThreshold ~ bio_year, data = JumpLength %>% 
              filter(Rarefied == "Rarefied"))) # p = 0.34

```



## Evolution of the radius of diffusive spread and jumps over time

We can now look at how the radius of the invasion increases over time, when differentiating diffusive spread and jump dispersal (Figure 6). In the westernmost disk portions, jump dispersal is responsible for the very high increase in the invasion radius. In the other disk portions, the spread seems to be mostly linked to diffusive dispersal. 

```{r figure total spread/diffusive spread for slf established, fig.cap = "Evolution of the radius of the invasion over time, when diffusive spread and jump dispersal are separated", echo=FALSE, message = FALSE}

#Data on total spread
Thresholds <- read.csv(file.path(here(), "exported-data", "thresholds.csv"))
Thresholds %<>% filter(rotation_nb == 1)


data <- grid_data %>% 
  filter(slf_established == T & bio_year %in% c(2014:2020)) 
threshold_data <- attribute_sectors(data, 
                                    nb_sectors = 20, 
                                    centroid = c(long = -75.675340, lat = 40.415240), 
                                    rotation = 4)

threshold_data %<>% group_by(bio_year, rotation1) %>% 
  summarise(MaxDist = max(DistToIntro)) %>% 
  rename(sectors_nb = rotation1)

test2 <- Thresholds %>%
  mutate(bio_year = as.factor(bio_year),
         sectors_nb = as.factor(sectors_nb)) %>% 
  complete(bio_year, sectors_nb, fill=list(DistToIntro=0)) %>% 
  group_by(sectors_nb) %>% 
  nest() %>% 
  mutate(data2 = purrr::map(
    data, function(x){x %>% mutate(next_year = lag(DistToIntro),
                                   next_year = replace_na(next_year, 0),
                                   progress = DistToIntro - next_year)}
  )
  )

test2 %<>% 
  select(data2) %>% 
  unnest(data2)

sd(test2$progress, na.rm=T)

test2 %>% 
  ggplot(aes(x = bio_year, y = progress))+
  geom_boxplot(aes(fill = bio_year))

spread <- ggplot() +
    geom_bar(data = threshold_data, aes(x = bio_year, y = MaxDist), 
             stat="identity", fill = "white", col = "black") +
  geom_bar(data = Thresholds, aes(x = bio_year , y = DistToIntro), 
           fill = "grey", col = "black", stat = "identity") +
  ylab("Invasion radius (km)")+
  xlab("Year") +
    facet_wrap(~sectors_nb) +
  theme_classic()

spread

ggsave(file.path(here(), "figures", "jump_list", "11.spread_radius.jpg"), 
       spread, height = 10, width = 8)
```


## Map of outbreaks

```{r jumps map}
hist(jump_groups %>% count(Group) %>% filter(n > 1) %>% pull(n), breaks = 100)

outbreaks <- jump_groups %>% count(Group) %>% filter(n > 1)

pal <- colorFactor(rep(rainbow(5), 8), 
                   domain = unique(jump_groups$Group))

leaflet(data = jump_groups) %>% 
      addTiles() %>% 
      addCircleMarkers(lng = ~longitude_rounded, 
                       lat = ~latitude_rounded, 
                       color = ~pal(Group), 
                       label = ~Group)  

```

Outbreaks always seem to be situated at places with BOTH roads and railroads (or railyards).

\newpage

# 5. Conclusion

The spread of the spotted lanternfly in the US is likely due both to diffusive spread and human-assisted jump dispersal. 75 jump occurrences have been identified, and most of them are situated in Winchester (north VA) and western Pennsylvania (especially Harrisburg). 

Jump events are likely be caused by SLF hitchhiking on human transports, and establishing near transport infrastructures: railroads, roads, and airports. In the next vignette, we will test the significance of the proximity between jump events and transport infrastructures by a comparison with a random distribution. We will also compare these distances to those of diffusers (SLF spread through diffusive spread) and of points where SLF were not detected, to check for a potential bias in survey locations.
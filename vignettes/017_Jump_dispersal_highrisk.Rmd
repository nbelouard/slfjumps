---
title: "Making the most of invasion records, the case of the spotted lanternfly, part III: Extending knowledge to northeastern US"
author: 
- Nadege Belouard^[Temple University, nadege.belouard@temple.edu]
- Sebastiona De Bona^[Temple University, seba.debona@temple.edu]
- Jocelyn E. Behm^[Temple University, jebehm@temple.edu]
- Matthew R. Helmus^[Temple University, mrhelmus@temple.edu]
date: "5/3/2021"
output:
  pdf_document:
    toc: TRUE
    toc_depth: 2
  html_document:
    toc: TRUE
    toc_depth: 3
params:
  show_code: FALSE
  export_figures: TRUE
editor_options: 
  chunk_output_type: console
---

```{r setup for rendering, include = F}
# here we set the images to png, to reduce the size of the output
# we set some global paramters in the yaml to allow us to switch the chunks
# of code on and off when displaying
knitr::opts_chunk$set(dpi = 300, echo = params$show_code)
```

\newpage

# 1. Aim and setup
 
For this third vignette, we want to investigate the relationship between properties generating or receiving high levels of traffic and the actual presence of established populations of SLF. A list of areas considered at high risk of colonization by SLF in PA has been built by the iEcolab.  

We will calculate the distance of jumpers, diffusers and non-detections to high-risk areas, and test whether these distances are shorter than random using the same methods as in the previous vignette.

We first want to check whether SLF are preferentially established near high-risk locations, and whether there is a difference between jumpers, diffusers and non-detections. Because we have listed high-risk areas in PA only, we subset the jumpers, diffusers and non-detections datasets to points situated in PA only.

## Load packages
```{r load packages}
library(tidyverse)
library(sf)
library(spData)
library(dplyr)
library(reshape2)
library(here)
library(magrittr)
```

## States map
```{r states names and centroid for global map, message = FALSE, warning = FALSE, echo = params$display}
# extracts a map of the States and recodes state labels to show the two-letter code rather than the full state name.

# obtaining simple feature objects for states and finding centroids for label positioning
states <- sf::st_as_sf(maps::map("state", plot = FALSE, fill = TRUE))
states <- cbind(states, st_coordinates(st_centroid(states)))
states <- st_transform(states, crs = "ESRI:102010")
st_crs(states)

# making table key for state 2-letter abbreviations
# the vectors state.abb and state.name contains strings of all
# US states and abbreviations
state_abbr <- tibble(state.name = str_to_lower(state.name), state.abb) %>% 
  left_join(tibble(ID = states$ID), ., by = c(ID = "state.name")) %>% 
  mutate(state.abb = replace_na(state.abb, ""))

# adding 2-letter codes to sf
states$code <- state_abbr$state.abb

# Visualize the data
# ggplot(data = states) +
#     geom_sf(data = states, fill = "white") +
#     geom_sf(data = Pennsylvania, fill = "blue") +
#     labs(x = "Longitude", y = "Latitude")
```



# 2. Calculate distances of SLF to high risk locations in PA

## Load the location of high risk data
```{r prepare dataset of high-risk areas for PA}
# Create a list with all files names
files <- list.files(path = file.path(here(), "figures", "GIS", "high-risk-areas-PA"), 
           pattern = NULL, all.files = FALSE,
           full.names = T, recursive = FALSE,
           ignore.case = FALSE, include.dirs = FALSE, no.. = FALSE)

# Add all the high risk locations in a single table
Highrisk <-  read.csv(files[1], sep=";")
file_name <- gsub("\\.csv", "", files[1])
Highrisk$category = gsub("C:/Users/labuser/Documents/Postdoc_SLF/SLF_jump_dispersal/slfjumps/figures/GIS/high-risk-areas-PA/jocelynb_pa_locations_webmap_table_", "", file_name) 

for (f in files[-1]) {
file <- read.csv(f, sep=";")
file_name = sub("\\.csv", "", f)
file$category = gsub("C:/Users/labuser/Documents/Postdoc_SLF/SLF_jump_dispersal/slfjumps/figures/GIS/high-risk-areas-PA/jocelynb_pa_locations_webmap_table_", "", file_name) 
Highrisk <- rbind(Highrisk, file)
} 

# dim(Highrisk)
unique(Highrisk$category)

write.csv(Highrisk, file.path(here(), "exported-data", "highrisk_locations.csv"), row.names = F)

```


Make categories
```{r calculate distances to high risk areas}

# Make more general categories
Highrisk <- read.csv(file.path(here(), "exported-data", "highrisk_locations.csv"))
unique(Highrisk$category)

Highrisk %<>% mutate(type = recode(category,
                                  "fedex" = "mail carriers", 
                                  "ups" = "mail carriers",
                                  "amazonFullfilment" = "mail carriers", 
                                  "landscape" = "wood", 
                                  "lumberYards" = "wood", 
                                  "sawmill" = "wood",
                                  "amusementParks" = "hobbies",
                                  "auctionCenter" = "hobbies",
                                  "campground" = "hobbies", 
                                  "casino" = "hobbies",
                                  "fairgrounds" = "hobbies",
                                  "fleamarket" = "hobbies",
                                  "racetracks" = "hobbies",
                                  "stadiums" = "hobbies", 
                                  "summerCamp" = "hobbies",
                                  "truckGarage" = "garages", 
                                  "autoRepair" = "garages",
                                  "boatLaunches" = "boat", 
                                  "marinas" = "boat",
                                  "movingCompanies" = "other", 
                                  "bottlingPlants" = "other", 
                                  "college" = "other", 
                                  "farmerMarket" = "other", 
                                  "truckStops" = "other",
                                  "distributionCenter" = "other", 
                                  "intermodal" = "other", 
                                  "armories" = "other"))
unique(Highrisk$type)

write.csv(Highrisk, file.path(here(), "exported-data", "highrisk_cat.csv"), row.names=F)

#Visualize data
Highrisk_layer <- st_as_sf(x = Highrisk, coords = c("Longitude", "Latitude"), crs = 4269)
Highrisk_proj <- st_transform(Highrisk_layer, crs = "ESRI:102010")

ggplot(data = states) +
    geom_sf(data = states, fill = "white") +
  geom_sf(data = , col = "blue") +
    labs(x = "Longitude", y = "Latitude")+
    theme(legend.position="top") +
  coord_sf(xlim = c(1000000, 2000000), ylim = c(-200000, 600000), expand = FALSE)
```


Create layers for landscape features
```{r make files per category}
# Subset this file in all categories & Apply buffers around points
unique(Highrisk_proj$type)

# Level of interest: high
# Mail carriers
mail <- Highrisk_proj %>% filter(type == "mail carriers")
mail_buffer <- st_buffer(mail, dist = 50)
dim(mail_buffer) #86

ggplot(data = states) +
    geom_sf(data = states, fill = "white") +
  geom_sf(data = mail_buffer, col = "blue", size = 2) +
    labs(x = "Longitude", y = "Latitude")+
  coord_sf(xlim = c(1000000, 2000000), ylim = c(-200000, 600000), expand = FALSE)


# Wood
wood <- Highrisk_proj %>% filter(type == "wood")
dim(wood)
wood_buffer <- st_buffer(wood, dist = 100)

ggplot(data = states) +
    geom_sf(data = states, fill = "white") +
  geom_sf(data = wood_buffer, col = "blue", size = 2) +
    labs(x = "Longitude", y = "Latitude") +
  coord_sf(xlim = c(1000000, 2000000), ylim = c(-200000, 600000), expand = FALSE)


# Wineries
wineries <- Highrisk_proj %>% filter(type == "winery")
wineries_buffer <- st_buffer(wineries, dist = 100)
dim(wineries)

ggplot(data = states) +
    geom_sf(data = states, fill = "white") +
  geom_sf(data = wineries_buffer, col = "blue", size = 2) +
    labs(x = "Longitude", y = "Latitude")+
  coord_sf(xlim = c(1000000, 2000000), ylim = c(-200000, 600000), expand = FALSE)



# Level of interest: medium
unique(Highrisk$type)
# Hobbies

# People hobbies
people <- Highrisk_proj %>% filter(type == "hobbies")
people_buffer <- st_buffer(people, dist = 100)
dim(people)

ggplot(data = states) +
    geom_sf(data = states, fill = "white") +
  geom_sf(data = people_buffer, col = "blue", size = 2) +
    labs(x = "Longitude", y = "Latitude")+
  coord_sf(xlim = c(1000000, 2000000), ylim = c(-200000, 600000), expand = FALSE)


# Level of interest: low
# Garages, Boat, Other
garages <- Highrisk_proj %>% filter(type == "garages")
garages_buffer <- st_buffer(garages, dist = 50)
dim(garages)

ggplot(data = states) +
    geom_sf(data = states, fill = "white") +
  geom_sf(data = garages_buffer, col = "blue", size = 2) +
    labs(x = "Longitude", y = "Latitude")+
  coord_sf(xlim = c(1000000, 2000000), ylim = c(-200000, 600000), expand = FALSE)



boats <- Highrisk_proj %>% filter(type ==  "boat")
boats_buffer <- st_buffer(boats, dist = 50)
dim(boats)

ggplot(data = states) +
    geom_sf(data = states, fill = "white") +
  geom_sf(data = boats_buffer, col = "blue", size = 2) +
    labs(x = "Longitude", y = "Latitude")+
  coord_sf(xlim = c(1000000, 2000000), ylim = c(-200000, 600000), expand = FALSE)


others <- Highrisk_proj %>% filter(type == "other")
others_buffer <- st_buffer(others, dist = 50)
dim(others_buffer)

ggplot(data = states) +
    geom_sf(data = states, fill = "white") +
  geom_sf(data = others_buffer, col = "blue", size = 2) +
    labs(x = "Longitude", y = "Latitude")+
  coord_sf(xlim = c(1000000, 2000000), ylim = c(-200000, 600000), expand = FALSE)



```    


Load data for ports and airports
```{r data for ports and airports}
airports <- st_read(file.path(here(), "figures", "GIS", "airports_primary_buffer.shp"), quiet = T)
st_crs(airports)
airports <- st_transform(airports, crs = "ESRI:102010")

ports <- st_read(file.path(here(), "figures", "GIS", "Ports_projected_buffer200.shp"), quiet = T)
st_crs(ports)
```


## Load SLF data

First load the dataset for the SLF data and clip the form of PA
```{r load datasets, message = FALSE, warning = FALSE}

# SLF data (in the folder SLF_datascience)
# First, load the dataset that contains the location of each survey
# Extract each point independently of the year it was sampled (for distance calculations)
grid_unique_chull <- read.csv(file.path(here(), "exported-data", "grid_unique_chull.csv"))
dim(grid_data)

# Make it a shapefile to visualize the data
grid_chull_layer <- st_as_sf(x = grid_unique_chull, coords = c("longitude_rounded", "latitude_rounded"), crs = "EPSG:4269", remove = F) %>% st_transform(crs = "ESRI:102010")

# Visualize the data
ggplot(data = states) +
  geom_sf(data = states, fill = "white") +
  geom_sf(data = grid_chull_layer, col = "red") +
  coord_sf(xlim = c(1000000, 2000000), ylim = c(-200000, 600000), expand = FALSE) +
  labs(x = "Longitude", y = "Latitude")

```


## Clip the form of PA

Form of PA
```{r PA form}
US <- st_read(file.path(here(), "figures", "GIS", "gadm36_Cont_USA_county.shp"), crs = "EPSG:4326", quiet = T)
st_crs(US)
US <- st_transform(US, crs = "ESRI:102010")
Pennsylvania <- US %>% filter(NAME_1 == "Pennsylvania")

# Visualize the SLF data
ggplot(data = US, fill = "white") +
  geom_sf() +
  geom_sf(data = Pennsylvania, col = "blue") +
  labs(x = "Longitude", y = "Latitude") +
  coord_sf(xlim = c(1000000, 2000000), ylim = c(-200000, 600000), expand = FALSE)
```

In grid data
```{r clip grid data in PA}

slfPA <- st_intersection(grid_chull_layer, Pennsylvania) 

# Visualize shapefiles
ggplot(data = states) +
    geom_sf(data = states, fill = "white") +
  geom_sf(data = grid_chull_layer, col = "red") +
  geom_sf(data = grid_layer, col = "blue") +
  geom_sf(data = slfPA, col = "blue") +
    labs(x = "Longitude", y = "Latitude")+
    theme(legend.position="top") +
  coord_sf(xlim = c(1000000, 2000000), ylim = c(-200000, 600000))
  
# This is PA clipped from the chull
write.csv(slfPA, file.path(here(), "exported-data", "slfPA.csv"), row.names=F)
```

In airports and ports
```{r clip airports and ports in PA}

airportsPA <- st_intersection(airports, Pennsylvania) 

# Visualize shapefiles
ggplot(data = states) +
    geom_sf(data = states, fill = "white") +
  geom_sf(data = airports) +
  geom_sf(data = airportsPA, col = "blue", size = 5) +
    labs(x = "Longitude", y = "Latitude")+
    theme(legend.position="top") +
  coord_sf(xlim = c(1000000, 2000000), ylim = c(-200000, 600000))


portsPA <- st_intersection(ports, Pennsylvania) 

# Visualize shapefiles
ggplot(data = states) +
    geom_sf(data = states, fill = "white") +
  geom_sf(data = ports) +
  geom_sf(data = portsPA, col = "blue", size = 5) +
    labs(x = "Longitude", y = "Latitude")+
    theme(legend.position="top") +
  coord_sf(xlim = c(1000000, 2000000), ylim = c(-200000, 600000))
```



## Calculate distances to landscape features
Calculate distances to points of interest in PA (a few hours overnight locally)
It includes:
- mail_buffer, people_buffer, others_buffer, wood_buffer, boats_buffer, garages_buffer, wineries_buffer
- airports, ports

```{r distance of SLF to points of interest, eval = FALSE}
unique(Highrisk$type)

# Create rows for distances
slfPA %<>% add_column(DistToMail = NA,
                      DistToWood = NA,
                      DistToWineries = NA,
                      DistToHobbies = NA,
                      DistToGarages = NA,
                      DistToBoats = NA,
                      DistToOthers = NA,
                      DistToAirports = NA,
                      DistToPorts = NA)


#Calculate their distance to transport infrastructures
for (j in 1000:length(slfPA$DistToMail)){ 
  # Print the row being considered
  print(j)
  
  # Calculate distance to the closest mail-related point
  dist_mail <- st_distance(x = slfPA[j,], y = mail_buffer)
  slfPA$DistToMail[j] <- min(dist_mail)
  
  # Calculate distance to the closest wood-related point
  dist_wood <- st_distance(x = slfPA[j,], y = wood_buffer)
  slfPA$DistToWood[j] <- min(dist_wood)
  
  # Calculate distance to the closest winery
  dist_wine <- st_distance(x = slfPA[j,], y = wineries_buffer)
  slfPA$DistToWineries[j] <- min(dist_wine)
  
  # Calculate distance to the closest location for people
  dist_people <- st_distance(x = slfPA[j,], y = people_buffer)
  slfPA$DistToHobbies[j] <- min(dist_people)
  
  # Calculate distance to the closest garage
  dist_garage <- st_distance(x = slfPA[j,], y = garages_buffer)
  slfPA$DistToGarages[j] <- min(dist_garage)

  # Calculate distance to the closest boat launch
  dist_boats <- st_distance(x = slfPA[j,], y = boats_buffer)
  slfPA$DistToBoats[j] <- min(dist_boats)
  
  # Calculate distance to other infrastructures
  dist_others <- st_distance(x = slfPA[j,], y = others_buffer)
  slfPA$DistToOthers[j] <- min(dist_others)
  
  # Calculate distance to airports
  dist_airports <- st_distance(x = slfPA[j,], y = airportsPA)
  slfPA$DistToAirports[j] <- min(dist_airports)
  
  # Calculate distance to ports
  dist_ports <- st_distance(x = slfPA[j,], y = portsPA)
  slfPA$DistToPorts[j] <- min(dist_ports)
  
  if (j %% 1000 == 0){
    st_write(slfPA, file.path(here(), "exported-data",  "distances_observed_structures.shp"), driver = "ESRI Shapefile", append = TRUE)  
    }
}


# Save file
st_geometry(slfPA) <- NULL
write.csv(slfPA, file.path(here(), "exported-data", "distances_observed_poi.csv"), row.names = F)
```


Make sure all distances have been calculated
```{r check distances}
# Distance data
grid_poi <- read.csv(file.path(here(), "exported-data", "distances_observed_poi.csv"), h=T)

# Check if there are points without distance
grid_poi %>% filter(DistToMail == Inf | is.na(DistToMail))
grid_poi %>% filter(DistToWood == Inf | is.na(DistToWood))
grid_poi %>% filter(DistToWineries == Inf | is.na(DistToWineries))
grid_poi %>% filter(DistToHobbies == Inf | is.na(DistToHobbies))
grid_poi %>% filter(DistToGarages == Inf | is.na(DistToGarages))
grid_poi %>% filter(DistToBoats == Inf | is.na(DistToBoats))
grid_poi %>% filter(DistToOthers == Inf | is.na(DistToOthers))
grid_poi %>% filter(DistToAirports == Inf | is.na(DistToAirports))
grid_poi %>% filter(DistToPorts == Inf | is.na(DistToPorts))


# #Calculate the distance for points where they are missing
# grid_transports <- st_as_sf(x = grid_data_chull, 
#                             coords = c("longitude_rounded", "latitude_rounded"), 
#                             crs = "EPSG:4269", remove = F)
# grid_transports <- st_transform(grid_transports, crs = "ESRI:102010")
#     
# # calculate the distance for missing points
# for (j in 1:length(grid_transports$DistToRail)){ 
#   if (grid_transports$DistToRail[j] > 50000){
#     print(j)
# 
#     # Calculate distance to the closest railroad
#     dist_rail <- st_distance(x = grid_transports[j,], y = rail_chull)
#     grid_transports$DistToRail[j] <- min(dist_rail)
#   }
#   
#   if (grid_transports$DistToRoad[j] > 50000){
#     print(j)
# 
#     # Calculate distance to the closest road
#     dist_road <- st_distance(x = grid_transports[j,], y = road_chull)
#     grid_transports$DistToRoad[j] <- min(dist_road)
#   }
# }
# 
# # Check result
# grid_transports %>% filter(DistToRail == Inf | is.na(DistToRail))
# grid_transports %>% filter(DistToRoad == Inf | is.na(DistToRoad))
# 
# # Save file
# st_geometry(grid_transports) <- NULL
# write.csv(grid_transports, file.path(here(), "exported-data", "distances_observed_transports_complete.csv"), row.names = F)
```

\newpage


# 3. Create datasets for jumpers/diffusers/negatives

We need to associate each point to the fact that it's a jumper, or diffuser, or negative point, by merging the table with observed distances and the table with status.

1. Prepare the tables
```{r choose which dataset you are testing}

grid_poi <- read.csv(file.path(here(), "exported-data", "distances_observed_poi.csv"))
head(grid_poi)
dim(grid_poi)

# Merge it with the grid data to get the whole dataset back, including column slf established
slfPA <- read.csv(file.path(here(), "exported-data", "slfPA.csv"))
dim(slfPA)
dim(slfPA)

slfPA %<>% left_join(grid_poi)
slfPA %<>% left_join(dist_poi) %>% select(-c(GID_0, NAME_0, GID_1, NAME_1, NL_NAME_1, GID_2, NAME_2, VARNAME_2, NL_NAME_2, TYPE_2, CC_2, HASC_2, ENGTYPE_2))
head(slfPA)
dim(slfPA)

# Verify if all points got a distance
missingdata <- slfPA %>% filter(DistToMail == Inf | is.na(DistToMail))
dim(missingdata)
slfPA %>% filter(DistToRoad == Inf | is.na(DistToRoad))
slfPA %>% filter(DistIntRlRd == Inf | is.na(DistIntRlRd))

# Visualize the SLF data
ggplot(data = US, fill = "white") +
  geom_sf() +
  geom_sf(data = missingdata, col = "blue") +
  labs(x = "Longitude", y = "Latitude") +
  coord_sf(xlim = c(1000000, 2000000), ylim = c(-200000, 600000), expand = FALSE)

# Create the categories diffusers (if established) or negatives (if not)
slfPA %<>% mutate(Category_out = ifelse(slf_established == TRUE, "Diffusers", "Negatives"))

# Next load the dataset that contains all jumpers identified by sets of parameters
jumpers <- read.csv(file.path(here(), "exported-data", "jumps_full_rarefied.csv"))
jumpers %<>% add_column(Category = "Jumpers")
```

2. Create full dataset (multiple jumps per location)
```{r create full dataset}
# Put jumpers into the grid data and complete with the rest
# Verify if all jumps are added or only PA!
slfPA_full <- slfPA %>% 
  left_join(jumpers %>% select(latitude_rounded, longitude_rounded, bio_year, Category, Status)) %>% 
  mutate(Category = ifelse(is.na(Category), Category_out, Category)) %>% 
  select(-Category_out)

# Verify we still have the right number of jumps
dim(slfPA_full %>% filter(Category == "Jumpers"))[1] == dim(jumpersPA)[1]

slfPA_full %<>% rename(Category_full = Category)
```

3. Create rarefied dataset (one jump per location)
```{r create rarefied dataset}
# Put jumpers into the grid data and complete with the rest
slfPA_rarefied <- slfPA %>% 
  left_join(jumpers %>% filter(Rarefied == TRUE) %>% 
                                    select(latitude_rounded, longitude_rounded, bio_year, Category, Status)) %>% 
  mutate(Category = ifelse(is.na(Category), Category_out, Category)) %>% 
  select(-Category_out)

# Verify we still have the right number of jumps
dim(slfPA_rarefied %>% filter(Category == "Jumpers"))[1] == dim(jumpersPA %>% filter(Rarefied == TRUE))[1]

slfPA_rarefied %<>% rename(Category_rarefied = Category) 
```


4. Reassemble in one long dataset with one column for the type of dataset
```{r reassemble full and rarefied dataset}
slfPA_cat <- merge(slfPA_full, slfPA_rarefied)

write.csv(slfPA_cat, file.path(here(), "exported-data", "slfPA_cat.csv"), row.names = F)
```

\newpage



# 4. Create dataset "as of today" or "most up to date"

Count each point only once as positive or negative (summarise data for each point)
```{r generate a grid of unique points for situation as of 2020}

slfPA_cat <- read.csv(file.path(here(), "exported-data", "slfPA_cat.csv"))
dim(slfPA_cat)

# Show the number of duplicates per point
slfPA_cat %>% group_by(latitude_rounded, longitude_rounded) %>% count() %>% 
  group_by(n) %>% count(n)

# Order the category level
unique(slfPA_cat$Category_rarefied)
slfPA_cat$Category_rarefied <- factor(slfPA_cat$Category_rarefied, levels = c("Negatives", "Diffusers", "Jumpers"))
slfPA_cat$Category_full <- factor(slfPA_cat$Category_full, levels = c("Negatives", "Diffusers", "Jumpers"))


# Translate factors into ordinal
slfPA_uptodate <- slfPA_cat %>% mutate(Category_rare_num = as.numeric(Category_rarefied),
                    Category_full_num = as.numeric(Category_full)) %>%  
  group_by(latitude_rounded, longitude_rounded, DistToMail, DistToWood, DistToWineries, DistToPeople, DistToGarages, DistToBoats, DistToOther, DistToAirport, DistToPort) %>% 
  summarize(Category_rare_max = max(Category_rare_num),
            Category_full_max = max(Category_full_num)) %>% 
  mutate(Category_rare = recode(Category_rare_max, "1" = "Negatives", "2" = "Diffusers", "3" = "Jumpers"),
         Category_full = recode(Category_full_max, "1" = "Negatives", "2" = "Diffusers", "3" = "Jumpers")) %>% 
  select(-Category_rare_max, -Category_full_max)
  

dim(slfPA_uptodate) #21,601
head(slfPA_uptodate)

# Save this file
write.csv(slfPA_uptodate, file.path(here(), "exported-data", "slfPA_uptodate.csv"), row.names=F)
```


Calculate statistics
```{r calculate proportions of data}
positive <- slfPA_uptodate %>% filter(Category_full %in% c("Diffusers", "Jumpers")) 
dim(positive)[1]/dim(slfPA_uptodate)[1] #17.93%
```

\newpage




# 5. Generate a random dispersal distribution of distances of jumpers to high-risk areas

The idea is to generate a distribution of distances to landscape features under the null hypothesis that SLF disperse randomly in the landscape. If a high number of random distributions are generated, we obtain the distribution of random dispersal distances to transports. The comparison of the random distribution and the observed average value gives the probability that the observed value is random.

Here, we generate 9,999 random datasets of distances to transport infrastructure within the convex hull of the positive SLF surveys. If the average value of the observed data is comprised within the simulated random values, it means that the pattern could be found by chance, and the distance between observed points and transport infrastructures is not significant.

To simplify calculations, we calculate all possible coordinates. We could remove those that were already calculated with slfPA, but here the calculation is relatively quick (and I could not make the code work because the coordinates seem to be slightly off between the slfPA and Coordinates_PA layers, for some reason)

Then the analysis consists in: (1) generating a random dataset of the same number of points as in the observed data, (2) for each random point, calculating the distance to transport infrastructures, (3) taking the average distance to each transport infrastructure for the dataset, (4) redoing the same cycle again for a total of 9,999 simulated random datasets.


## Create random coordinates
```{r dataset with points only for PA, echo=FALSE, warning =FALSE, message = FALSE}

# Load the SLF data
slfPA <- read.csv(file.path(here(), "exported-data", "slfPA.csv"))

# Define the boundaries
maxlat <- max(slfPA$latitude_rounded)
minlat <- min(slfPA$latitude_rounded)
maxlong <- max(slfPA$longitude_rounded)  
minlong <- min(slfPA$longitude_rounded)

# Generate the coordinates
Seqlat <- seq(from = minlat, to = maxlat, by = 1/111)
Seqlong <- seq(from = minlong, to = maxlong, by = 1/85)
Coordinates <- expand.grid(latitude_rounded = Seqlat, longitude_rounded = Seqlong)
Coordinates <- st_as_sf(x = Coordinates, coords = c("longitude_rounded", "latitude_rounded"), crs = "EPSG:4269", remove = F)
Coordinates <- st_transform(Coordinates, crs = "ESRI:102010")

# Plot this and check if this is correct
ggplot(data = US, fill = "white") +
  geom_sf() +
  geom_sf(data = Coordinates, alpha = 0.5, col = "blue") +
  geom_sf(data = slfPA, col = "red") +
  labs(x = "Longitude", y = "Latitude") +
  coord_sf(xlim = c(1000000, 2000000), ylim = c(-200000, 600000), expand = FALSE)

#Keep only those in PA
Coordinates_PA <- st_intersection(Coordinates, Pennsylvania)

ggplot(data = US, fill = "white") +
  geom_sf() +
  geom_sf(data = Coordinates_PA, col = "blue") +
  geom_sf(data = slfPA, col = "red") +
  labs(x = "Longitude", y = "Latitude") +
  coord_sf(xlim = c(1000000, 2000000), ylim = c(-200000, 600000), expand = FALSE)

Coordinates_PA %<>% select(latitude_rounded, longitude_rounded, geometry)


# Keep only the coordinates that are in the minimum convex polygon
chull <- st_read(file.path(here(), "figures", "GIS", "chull.shp"), quiet = T)

Coordinates_chull <- st_intersection(Coordinates, hull)

ggplot(data = US, fill = "white") +
  geom_sf() +
  geom_sf(data = Coordinates_chull, alpha = 0.5) +
  geom_sf(data = slfEstab_layer, col = "blue") +
  labs(x = "Longitude", y = "Latitude") +
  coord_sf(xlim = c(1000000, 2000000), ylim = c(-200000, 600000), expand = FALSE)


#Keep only those that are on land!
Coordinates_land <- st_intersection(Coordinates_chull, US)

random_coord <- ggplot(data = states, fill = "white") +
  geom_sf() +
  # geom_sf(data = Coordinates_land, alpha = 0.5) +
  geom_sf(data = hull, alpha = 0.5, fill = "white") + 
  geom_point(data = slfEstab_layer,
             aes(x = longitude_rounded, y = latitude_rounded), col = "black") +
  labs(x = "Longitude", y = "Latitude") +
  coord_sf(xlim = c(-82, -72), ylim = c(37, 43)) +
  theme_classic()


# Save file
Coordinates_land %<>% select(latitude, longitude, geometry)
st_geometry(Coordinates_land) <- NULL 
st_write(Coordinates_land, file.path(here(), "exported-data", "simulated_coordinates_PA.csv"))

```


## Calculate distances
Calculate distances to high risk for those points (copy paste code from above), done locally.
Should be done in a few hours locally

```{r calculate distances of PA points to POI}

Coordinates_land <- st_read(file.path(here(), "exported-data", "Distances_random distrib", "coordinates_set.shp"), quiet = T)

# Create rows for distances
Coordinates_PA %<>% add_column(DistToMail = NA,
                      DistToWood = NA,
                      DistToWineries = NA,
                      DistToPeople = NA,
                      DistToGarages = NA,
                      DistToBoats = NA,
                      DistToOthers = NA,
                      DistToAirports = NA,
                      DistToPorts = NA)

# Transform into a sf layer
Coordinates_PA <- st_as_sf(x = Coordinates_PA, coords = c("longitude_rounded", "latitude_rounded"), crs = 4269, remove = F)
Coordinates_PA <- st_transform(Coordinates_PA, crs = "ESRI:102010")

#Calculate their distance to transport infrastructures
for (j in 1:length(Coordinates_PA$DistToMail)){ 
  
  # Calculate distance to the closest mail-related point
  dist_mail <- st_distance(x = Coordinates_PA[j,], y = mail_buffer)
  Coordinates_PA$DistToMail[j] <- min(dist_mail)
  
  # Calculate distance to the closest wood-related point
  dist_wood <- st_distance(x = Coordinates_PA[j,], y = wood_buffer)
  Coordinates_PA$DistToWood[j] <- min(dist_wood)
  
  # Calculate distance to the closest winery
  dist_wine <- st_distance(x = Coordinates_PA[j,], y = wineries_buffer)
  Coordinates_PA$DistToWineries[j] <- min(dist_wine)
  
  # Calculate distance to the closest location for people
  dist_people <- st_distance(x = Coordinates_PA[j,], y = people_buffer)
  Coordinates_PA$DistToPeople[j] <- min(dist_people)
  
  # Calculate distance to the closest garage
  dist_garage <- st_distance(x = Coordinates_PA[j,], y = garages_buffer)
  Coordinates_PA$DistToGarages[j] <- min(dist_garage)

  # Calculate distance to the closest boat launch
  dist_boats <- st_distance(x = Coordinates_PA[j,], y = boats_buffer)
  Coordinates_PA$DistToBoats[j] <- min(dist_boats)
  
  # Calculate distance to the category others
  dist_others <- st_distance(x = Coordinates_PA[j,], y = others_buffer)
  Coordinates_PA$DistToOthers[j] <- min(dist_others)
  
  # Calculate distance to the closest airport
  dist_airport <- st_distance(x = Coordinates_PA[j,], y = airport_buffer)
  Coordinates_PA$DistToAirport[j] <- min(dist_airport)
  
  # Calculate distance to the closest port
  dist_port <- st_distance(x = Coordinates_PA[j,], y = port_buffer)
  Coordinates_PA$DistToPort[j] <- min(dist_port)
  
  if (j %% 1000 == 0){
    print(j)
    distances_random_PA <- Coordinates_PA
    st_geometry(distances_random_PA) <- NULL
    write.csv(distances_random_PA, file.path(here(), "exported-data", "Distances_random distrib", "grid_distances_poiPA.csv"), row.names = F)  
    }
}


# Verify that there is no 0
Coordinates_PA %>% filter(DistToMail == Inf | is.na(DistToMail))
Coordinates_PA %>% filter(DistToWood == Inf | is.na(DistToWood))


# Save file
st_geometry(Coordinates_PA) <- NULL
write.csv(Coordinates_PA, file.path(here(), "exported-data", "Distances_random distrib", "grid_distances_poiPA.csv"), row.names = F)
```



## Select random datasets, full dataset

```{r select random datasets}

slfPA_cat <- read.csv(file.path(here(), "exported-data", "pointsPA_chull.csv"))

Coordinates_PA <- read.csv(file.path(here(), "exported-data", "Distances_random distrib", "grid_distances_poiPA.csv"))

# Size of the dataset to be sampled
jumpers_full = dim(slfPA_cat %>% filter(Category_full == "Jumpers"))[1]
jumpers_rare = dim(slfPA_cat %>% filter(Category_rare == "Jumpers"))[1]
diffusers_full = dim(slfPA_cat %>% filter(Category_full == "Diffusers"))[1]
diffusers_rare = dim(slfPA_cat %>% filter(Category_rare == "Diffusers"))[1]
negatives = dim(slfPA_cat %>% filter(Category_full == "Negatives"))[1] #There is no difference in the negatives between the full and rarefied datasets!

for (i in 1:9999){
  #Generate a set of coordinates
  Random_jumpers_full <- Coordinates_PA[sample(nrow(Coordinates_PA), size = jumpers_full, replace = F),] %>%
    add_column(Category_full = "Jumpers")
  Random_diffusers_full <- Coordinates_PA[sample(nrow(Coordinates_PA), size = diffusers_full, replace = F),] %>%
    add_column(Category_full = "Diffusers")
  Random_negatives <- Coordinates_PA[sample(nrow(Coordinates_PA), size = negatives, replace = F),] %>%
    add_column(Category_full = "Negatives")
  
  Random_coordinates <- rbind(Random_jumpers_full, Random_diffusers_full,
                              Random_negatives)
  
  #Calculate the mean and median distance per simulation
  Random_means <- Random_coordinates %>% group_by(Category_full) %>% 
    summarise_at(vars(starts_with("DistTo")), list(mean = mean, median = median, sd = sd)) %>% 
    add_column(Simulation = i)
  
  #Save the table with the simulation number
  if (i == 1){
    Simulations <- Random_means
  } else {
    Simulations <- bind_rows(Simulations, Random_means)
  }
  
  if (i %% 100 == 0){ print(i)}
} 

write.csv(Simulations, "./exported-data/Distances_random distrib/SimulatedMeans_fullhighriskPA.csv", row.names = F)
```


## Select random datasets, rarefied dataset

```{r select random datasets rarefied}
for (i in 1:9999){
  #Generate a set of coordinates
  Random_jumpers_rare <- Coordinates_PA[sample(nrow(Coordinates_PA), size = jumpers_rare, replace = F),] %>%
    add_column(Category_rare = "Jumpers")
  Random_diffusers_rare <- Coordinates_PA[sample(nrow(Coordinates_PA), size = diffusers_rare, replace = F),] %>%
    add_column(Category_rare = "Diffusers")
  Random_negatives <- Coordinates_PA[sample(nrow(Coordinates_PA), size = negatives, replace = F),] %>%
    add_column(Category_rare = "Negatives")
  
  Random_coordinates <- rbind(Random_jumpers_rare, Random_diffusers_rare,
                              Random_negatives)
  
  #Calculate the mean distance per simulation
  Random_means <- Random_coordinates %>% group_by(Category_rare) %>% 
    summarise_at(vars(starts_with("DistTo")), list(mean = mean, median = median)) %>% 
    add_column(Simulation = i)
  
  #Save the table with the simulation number
  if (i == 1){
    Simulations <- Random_means
  } else {
    Simulations <- bind_rows(Simulations, Random_means)
  }
  
  if (i %% 100 == 0){ print(i)}
} 

write.csv(Simulations, "./exported-data/Distances_random distrib/SimulatedMeans_rarehighriskPA.csv", row.names = F)
```


```{r map of random points for jumpers, fig.width= 6, fig.height=7, eval = FALSE}
# Check on the first randomly generated dataset that the points location corresponds to the expectation in each disk portion (Figure 2).

states <- st_as_sf(maps::map("state", plot = FALSE, fill = TRUE))

ggplot(data = states) +
    geom_sf(fill = "white") +
    geom_point(data = Random_coordinates,
            aes(x = longitude_rounded, y = latitude_rounded), shape = 19, size = 2) +
    labs(x = "Longitude", y = "Latitude")+
    theme(legend.position="top") +
  coord_sf(xlim = c(-81, -73), ylim = c(38, 42.5), expand = FALSE)

```


#6. Visualize results
## Plot the null distribution against observed values

Full dataset
```{r plot null distribution to high risk, fig.height=2, fig.width = 6, fig.cap="Comparison of the distance of jumpers to high-risk areas to a random distribution."}

# Simulated means
MeanDistances <- read.csv(file.path(here(), "exported-data", "Distances_random distrib", "SimulatedMeans_fullhighriskPA.csv"))
names(MeanDistances)
head(MeanDistances)

#Modify dataset to get distribution of distances with a column for the type of distance, and one column for the type of dataset (Full or Rarefied)
randomPA_long <- MeanDistances %>%
  pivot_longer(cols = starts_with("DistTo"), names_to = "DistanceType", values_to = "DistanceValue")

head(randomPA_long)
unique(randomPA_long$DistanceType)

## FOR MATT SLF 101 MEAN FULL
randomPA_long_mean <- randomPA_long %>% filter(grepl("_mean", DistanceType))
head(randomPA_long_mean)
randomPA_long_mean$DistanceType <- as.factor(randomPA_long_mean$DistanceType)
levels(randomPA_long_mean$DistanceType)
levels(randomPA_long_mean$DistanceType) <- gsub(pattern = "_mean", replacement = "", x = levels(randomPA_long_mean$DistanceType))


head(randomPA_long_mean)
summary_sim_full <- randomPA_long_mean %>% group_by(Category_full, DistanceType) %>% summarise(mean_sim = mean(DistanceValue), sd_sim = sd(DistanceValue))
head(summary_sim_full)



slfPA_obsmeans <- slfPA_cat_long %>% 
  group_by(Category_full, DistanceType) %>% 
  summarise(MeanDistance = mean(DistanceValue)#,
            # MedianDistance = median(DistanceValue)
            )
slfPA_obsmeans %<>% filter(!DistanceType %in% c("DistToRoad", "DistToAirport", "DistToRail", "DistToPort"))



summary_full <- merge(summary_sim_full, slfPA_obsmeans)
summary_full %<>% rename(mean_obs = MeanDistance) %>% 
  mutate(effect_size = (mean_obs - mean_sim)/sd_sim)
write.csv(summary_full, "summary_simulations_full.csv")


Category = as.vector(unique(summary_full$DistanceType))
str(Category)
Properties = c("Boat launches and marinas", 
               "Bottling plants",
               "Colleges",
               "Distribution centers",
               "Truck garages and auto repairs",
               "Intermodal platforms",
               "Amazon fulfillment, Fedex and UPS",
               "Farmers markets",
               "Moving companies",
               "Popular destinations: amusement parks, auction centers, campgrounds, casinos, fairgrounds, flea markets, race tracks, stadiums, summer camps",
               "Truck stops",
               "Wineries",
               "Landscaping companies, lumber yards, sawmills")
Names <- data.frame(DistanceType = Category, Properties = Properties)


summary_full_cat <- merge(summary_full, Names)
write.csv(summary_full_cat, "summary_simulations_full.csv")


# GRAPH
ggplot(summary_full_cat, aes(y = effect_size, x = DistanceType)) + 
  geom_point(aes(col = Category_full)) +
  # scale_fill_brewer(palette = "Dark2") +
  # facet_wrap(~Category_full, scales = "free_y", ncol = 1) +
             # labeller = labeller(DistanceType = 
  #   c("DistToAirport" = "Airport",
  #     "DistToBoats" = "Boating",
  #     "DistToBottling" = "Bottling plants",
  #     "DistToColleges" = "Colleges",
  #     "DistToDistrib" = "Distribution centers",
  #     "DistToGarages" = "Garages",
  #     "DistToIntermodal" = "Intermodal platforms",
  #     "DistToMail" = "Mail carriers",
  #     "DistToMarket" = "Farmers market",
  #     "DistToMoving" = "Moving companies",
  #     "DistToPeople" = "Popular destinations",
  #     "DistToPort" = "Port",
  #     "DistToRail" =  "Railroad",
  #     "DistToRoad" = "Major road",
  #     "DistToTruckStop" = "Truck stops",
  #     "DistToWineries" = "Wineries",
  #     "DistToWood" = "Wood-related activities"))) +
  # # coord_cartesian(ylim=c(0, 30000)) +
  # xlab("Category") + ylab("Distance to the nearest... (km)") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
  


## FOR MATT SLF 101 MEDIANS FULL
randomPA_long_median <- randomPA_long %>% filter(grepl("_median", DistanceType))
head(randomPA_long_median)
randomPA_long_median$DistanceType <- as.factor(randomPA_long_median$DistanceType)
levels(randomPA_long_median$DistanceType)
levels(randomPA_long_median$DistanceType) <- gsub(pattern = "_median", replacement = "", x = levels(randomPA_long_median$DistanceType))


head(randomPA_long_median)
summary_sim_medianfull <- randomPA_long_median %>% group_by(Category_full, DistanceType) %>% summarise(median_sim = median(DistanceValue), sd_sim = sd(DistanceValue))
head(summary_sim_medianfull)



slfPA_obsmedians <- slfPA_cat_long %>% 
  group_by(Category_full, DistanceType) %>% 
  summarise(MedianDistance = median(DistanceValue)#,
            # MedianDistance = median(DistanceValue)
            )
slfPA_obsmedians %<>% filter(!DistanceType %in% c("DistToRoad", "DistToAirport", "DistToRail", "DistToPort"))



summary_medianfull <- merge(summary_sim_medianfull, slfPA_obsmedians)
summary_medianfull %<>% rename(median_obs = MedianDistance) %>% 
  mutate(effect_size = (median_obs - median_sim)/sd_sim)
write.csv(summary_medianfull, "summary_simulations_median_full.csv")


summary_medianfull_cat <- merge(summary_medianfull, Names)
write.csv(summary_medianfull_cat, "summary_simulations_median_full.csv")
```


## Calculate effect sizes
```{r }
# Simulated means
MeanDistances <- read.csv(file.path(here(), "exported-data", "Distances_random distrib", "SimulatedMeans_rarehighriskPA.csv"))
names(MeanDistances)
head(MeanDistances)

#Modify dataset to get distribution of distances with a column for the type of distance, and one column for the type of dataset (Full or Rarefied)
randomPA_long <- MeanDistances %>%
  pivot_longer(cols = starts_with("DistTo"), names_to = "DistanceType", values_to = "DistanceValue")

randomPA_long_mean <- randomPA_long %>% filter(grepl("_mean", DistanceType))
head(randomPA_long_mean)
randomPA_long_mean$DistanceType <- as.factor(randomPA_long_mean$DistanceType)
levels(randomPA_long_mean$DistanceType)
levels(randomPA_long_mean$DistanceType) <- gsub(pattern = "_mean", replacement = "", x = levels(randomPA_long_mean$DistanceType))


head(randomPA_long_mean)
summary_sim_rare <- randomPA_long_mean %>% group_by(Category_rare, DistanceType) %>% summarise(mean_sim = mean(DistanceValue), sd_sim = sd(DistanceValue))
head(summary_sim_rare)



slfPA_obsmeans <- slfPA_cat_long %>% 
  group_by(Category_rare, DistanceType) %>% 
  summarise(MeanDistance = mean(DistanceValue)#,
            # MedianDistance = median(DistanceValue)
            )
slfPA_obsmeans %<>% filter(!DistanceType %in% c("DistToRoad", "DistToAirport", "DistToRail", "DistToPort"))



summary_rare <- merge(summary_sim_rare, slfPA_obsmeans)
summary_rare %<>% rename(mean_obs = MeanDistance) %>% 
  mutate(effect_size = (mean_obs - mean_sim)/sd_sim)
write.csv(summary_rare, "summary_simulations_rare.csv")


summary_rare_cat <- merge(summary_rare, Names)
write.csv(summary_rare_cat, "summary_simulations_mean_rare.csv")


## FOR MATT SLF 101 MEDIANS FULL
randomPA_long_median <- randomPA_long %>% filter(grepl("_median", DistanceType))
head(randomPA_long_median)
randomPA_long_median$DistanceType <- as.factor(randomPA_long_median$DistanceType)
levels(randomPA_long_median$DistanceType)
levels(randomPA_long_median$DistanceType) <- gsub(pattern = "_median", replacement = "", x = levels(randomPA_long_median$DistanceType))


head(randomPA_long_median)
summary_sim_medianrare <- randomPA_long_median %>% group_by(Category_rare, DistanceType) %>% summarise(median_sim = median(DistanceValue), sd_sim = sd(DistanceValue))
head(summary_sim_medianrare)



slfPA_obsmedians <- slfPA_cat_long %>% 
  group_by(Category_rare, DistanceType) %>% 
  summarise(MedianDistance = median(DistanceValue)#,
            # MedianDistance = median(DistanceValue)
            )
slfPA_obsmedians %<>% filter(!DistanceType %in% c("DistToRoad", "DistToAirport", "DistToRail", "DistToPort"))



summary_medianrare <- merge(summary_sim_medianrare, slfPA_obsmedians)
summary_medianrare %<>% rename(median_obs = MedianDistance) %>% 
  mutate(effect_size = (median_obs - median_sim)/sd_sim)
write.csv(summary_medianrare, "summary_simulations_median_rare.csv")

summary_rare_mediancat <- merge(summary_medianrare, Names)
write.csv(summary_rare_mediancat, "summary_simulations_median_rare.csv")
```




## Plot simulated vs observed
```{r other}

randomPA_long_median <- randomPA_long %>% filter(grepl("_median", DistanceType))
randomPA_long_median$DistanceType <- as.factor(randomPA_long_median$DistanceType)
levels(randomPA_long_median$DistanceType)
levels(randomPA_long_median$DistanceType) <- gsub(pattern = "_median", replacement = "", x = levels(randomPA_long_median$DistanceType))





# Observed data
slfPA_cat <- read.csv(file.path(here(), "exported-data", "slfPA_cat.csv"))
slfPA_cat <- read.csv(file.path(here(), "exported-data", "pointsPA_chull.csv"))

slfPA_cat_long <- slfPA_cat %>%
  pivot_longer(cols = starts_with("DistTo"), names_to = "DistanceType", values_to = "DistanceValue")

slfPA_obsmeans <- slfPA_cat_long %>% 
  group_by(Category_full, DistanceType) %>% 
  summarise(MeanDistance = mean(DistanceValue)#,
            # MedianDistance = median(DistanceValue)
            )
slfPA_obsmeans %<>% filter(!DistanceType %in% c("DistToRoad", "DistToAirport", "DistToRail", "DistToPort"))

slfPA_obsmeans$Category_full <- factor(slfPA_obsmeans$Category_full, levels = c("Jumpers", "Diffusers", "Negatives"))
randomPA_long$Category_full <- factor(randomPA_long$Category_full, levels = c("Jumpers", "Diffusers", "Negatives"))

# Plot observed vs simulated means ####### ADD SIMULATED MEANS FOR AIRPORTS, PORTS, ROADS....
random_highrisk <- ggplot() +
  geom_histogram(data = randomPA_long_median %>% filter(Category_full == "Jumpers"), aes(x =  DistanceValue/1000, y = ..density.., fill = Category_full), binwidth = 0.1) +
  geom_vline(data = slfPA_obsmeans %>% filter(Category_full == "Jumpers"), mapping = aes(xintercept = MedianDistance/1000, col = Category_full), size = 1) +
  scale_fill_brewer(palette = "Dark2") +
  scale_color_brewer(palette = "Dark2") +
  xlab("Distance to the nearest... (km)") +
  ylab("Count (simulations)") +
  facet_wrap(~DistanceType, ncol = 4, scale = "free") +
  theme_classic() +
  guides(col = guide_legend("SLF category"), fill = guide_legend("SLF category"))

  # coord_cartesian(ylim = c(0,1)) +
  # facet_wrap(~DistanceType, ncol = 3, scale = "free") +
  # theme_classic()

ggsave(file.path(here(), "figures", "vignette_highrisk", "bootstrap_highrisk_chull_median_full.jpg"), random_highrisk, width = 10, height = 6)
```




## Plot rarefied dataset
```{r plot null distribution to high risk, fig.height=2, fig.width = 6, fig.cap="Comparison of the distance of jumpers to high-risk areas to a random distribution."}

# Simulated means
MeanDistances <- read.csv(file.path(here(), "exported-data", "Distances_random distrib", "SimulatedMeans_rarehighriskPA.csv"))

#Modify dataset to get distribution of distances with a column for the type of distance, and one column for the type of dataset (Full or Rarefied)
randomPA_long <- MeanDistances %>%
  pivot_longer(cols = starts_with("DistTo"), names_to = "DistanceType", values_to = "DistanceValue")

randomPA_long_median <- randomPA_long %>% filter(grepl("_median", DistanceType))
randomPA_long_median$DistanceType <- as.factor(randomPA_long_median$DistanceType)
levels(randomPA_long_median$DistanceType)
levels(randomPA_long_median$DistanceType) <- gsub(pattern = "_median", replacement = "", x = levels(randomPA_long_median$DistanceType))


# Observed data
slfPA_cat <- read.csv(file.path(here(), "exported-data", "slfPA_cat.csv"))
slfPA_cat <- read.csv(file.path(here(), "exported-data", "pointsPA_chull.csv"))
slfPA_cat_long <- slfPA_cat %>%
  pivot_longer(cols = starts_with("DistTo"), names_to = "DistanceType", values_to = "DistanceValue")
slfPA_obsmeans <- slfPA_cat_long %>% 
  group_by(Category_rare, DistanceType) %>% 
  summarise(MeanDistance = mean(DistanceValue),
            MedianDistance = median(DistanceValue))
slfPA_obsmeans %<>% filter(!DistanceType %in% c("DistToRoad", "DistToAirport", "DistToRail", "DistToPort"))

slfPA_obsmeans$Category_rare <- factor(slfPA_obsmeans$Category_rare, levels = c("Jumpers", "Diffusers", "Negatives"))
randomPA_long$Category_rare <- factor(randomPA_long$Category_rare, levels = c("Jumpers", "Diffusers", "Negatives"))

# Plot observed vs simulated means ####### ADD SIMULATED MEANS FOR AIRPORTS, PORTS, ROADS....
random_highrisk <- ggplot() +
  geom_histogram(data = randomPA_long_median %>% filter(Category_rare == "Jumpers"), aes(x =  DistanceValue/1000, y = ..density.., fill = Category_rare), binwidth = 0.1) +
  geom_vline(data = slfPA_obsmeans %>% filter(Category_rare == "Jumpers"), mapping = aes(xintercept = MedianDistance/1000, col = Category_rare), size = 1) +
  scale_fill_brewer(palette = "Dark2") +
  scale_color_brewer(palette = "Dark2") +
  xlab("Distance to the nearest... (km)") +
  ylab("Count (simulations)") +
  facet_wrap(~DistanceType, ncol = 4, scale = "free") +
  theme_classic() +
  guides(col = guide_legend("SLF category"), fill = guide_legend("SLF category"))

  # coord_cartesian(ylim = c(0,1)) +
  # facet_wrap(~DistanceType, ncol = 3, scale = "free") +
  # theme_classic()

ggsave(file.path(here(), "figures", "vignette_highrisk", "bootstrap_highrisk_chull_median_rare.jpg"), random_highrisk, width = 10, height = 6)


# Count how many simulations are lower than the observed data
# If it's below 5% it's not random
obsboats <- slfPA_obsmeans %>% filter(Category_rare == "Jumpers", DistanceType == "DistToBoats") %>% pull(MedianDistance) 
dim(randomPA_long_median %>% filter(Category_rare == "Jumpers", DistanceType == "DistToBoats", DistanceValue < obsboats))[1]/10000
#0.0032

obsbottle <- slfPA_obsmeans %>% filter(Category_rare == "Jumpers", DistanceType == "DistToBottling") %>% pull(MedianDistance) 
dim(randomPA_long_median %>% filter(Category_rare == "Jumpers", DistanceType == "DistToBottling", DistanceValue < obsbottle))[1]/10000
#0.008

obsintermodal <- slfPA_obsmeans %>% filter(Category_rare == "Jumpers", DistanceType == "DistToIntermodal") %>% pull(MedianDistance) 
dim(randomPA_long_median %>% filter(Category_rare == "Jumpers", DistanceType == "DistToIntermodal", DistanceValue < obsintermodal))[1]/10000
#0.1014


obstruck <- slfPA_obsmeans %>% filter(Category_rare == "Jumpers", DistanceType == "DistToTruckStop") %>% pull(MedianDistance) 
dim(randomPA_long_median %>% filter(Category_rare == "Jumpers", DistanceType == "DistToTruckStop", DistanceValue < obstruck))[1]/10000
#0.4375


obswineries <- slfPA_obsmeans %>% filter(Category_rare == "Jumpers", DistanceType == "DistToWineries") %>% pull(MedianDistance) 
dim(randomPA_long_median %>% filter(Category_rare == "Jumpers", DistanceType == "DistToWineries", DistanceValue < obswineries))[1]/10000
#0.0537

```


We can visualize the results on Figure 2. The histogram represents the distribution of distances to high-risk areas under the null hypothesis of random dispersal of jumpers. The black vertical lines indicate the significance limits. An observed value situated outside of these vertical lines leads to the rejection of the null hypothesis. The red line indicates the average distance between jumpers and high-risk areas observed in our dataset. The observed location of jumpers is significantly closer than random to high-risk areas.  

\newpage 
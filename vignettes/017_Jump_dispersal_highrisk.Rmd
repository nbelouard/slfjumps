---
title: "Making the most of invasion records, the case of the spotted lanternfly, part III: Extending knowledge to northeastern US"
author: 
- Nadege Belouard^[Temple University, nadege.belouard@temple.edu]
- Sebastiona De Bona^[Temple University, seba.debona@temple.edu]
- Jocelyn E. Behm^[Temple University, jebehm@temple.edu]
- Matthew R. Helmus^[Temple University, mrhelmus@temple.edu]
date: "5/3/2021"
output:
  pdf_document:
    toc: TRUE
    toc_depth: 2
  html_document:
    toc: TRUE
    toc_depth: 3
params:
  show_code: FALSE
  export_figures: TRUE
editor_options: 
  chunk_output_type: console
---

```{r setup for rendering, include = F}
# here we set the images to png, to reduce the size of the output
# we set some global paramters in the yaml to allow us to switch the chunks
# of code on and off when displaying
knitr::opts_chunk$set(dpi = 300, echo = params$show_code)
```

\newpage

# 1. Aim and setup
 
For this third vignette, we want to investigate the relationship between properties generating or receiving high levels of traffic and the actual presence of established populations of SLF. A list of areas considered at high risk of colonization by SLF in PA has been built by the iEcolab.  

We will calculate the distance of jumpers, diffusers and non-detections to high-risk areas, and test whether these distances are shorter than random using the same methods as in the previous vignette.

We first want to check whether SLF are preferentially established near high-risk locations, and whether there is a difference between jumpers, diffusers and non-detections. Because we have listed high-risk areas in PA only, we subset the jumpers, diffusers and non-detections datasets to points situated in PA only.

## Load packages
```{r load packages}
library(tidyverse)
library(sf)
library(spData)
library(dplyr)
library(reshape2)
library(here)
library(magrittr)

sf::sf_use_s2(FALSE)
```

## States map
```{r states names and centroid for global map, message = FALSE, warning = FALSE, echo = params$display}
# extracts a map of the States and recodes state labels to show the two-letter code rather than the full state name.

# obtaining simple feature objects for states and finding centroids for label positioning
states <- sf::st_as_sf(maps::map("state", plot = FALSE, fill = TRUE))
states <- cbind(states, st_coordinates(st_centroid(states)))
states <- st_transform(states, crs = "ESRI:102010")
st_crs(states)

# making table key for state 2-letter abbreviations
# the vectors state.abb and state.name contains strings of all
# US states and abbreviations
state_abbr <- tibble(state.name = str_to_lower(state.name), state.abb) %>% 
  left_join(tibble(ID = states$ID), ., by = c(ID = "state.name")) %>% 
  mutate(state.abb = replace_na(state.abb, ""))

# adding 2-letter codes to sf
states$code <- state_abbr$state.abb

# Visualize the data
ggplot(data = states) +
    geom_sf(data = states, fill = "white") +
    labs(x = "Longitude", y = "Latitude")
```



# 2. Calculate distances of SLF to high risk locations in PA

## Load the location of high risk data
```{r prepare dataset of high-risk areas for PA}
# Create a list with all files names
files <- list.files(path = file.path(here(), "figures", "GIS", "high-risk-areas-PA"), 
           pattern = NULL, all.files = FALSE,
           full.names = T, recursive = FALSE,
           ignore.case = FALSE, include.dirs = FALSE, no.. = FALSE)

# Add all the high risk locations in a single table
Highrisk <-  read.csv(files[1], sep=";")
file_name <- gsub("\\.csv", "", files[1])
Highrisk$category = gsub("C:/Users/labuser/Documents/Postdoc_SLF/SLF_jump_dispersal/slfjumps/figures/GIS/high-risk-areas-PA/jocelynb_pa_locations_webmap_table_", "", file_name) 

for (f in files[-1]) {
file <- read.csv(f, sep=";")
file_name = sub("\\.csv", "", f)
file$category = gsub("C:/Users/labuser/Documents/Postdoc_SLF/SLF_jump_dispersal/slfjumps/figures/GIS/high-risk-areas-PA/jocelynb_pa_locations_webmap_table_", "", file_name) 
Highrisk <- rbind(Highrisk, file)
} 

# dim(Highrisk)
unique(Highrisk$category)

write.csv(Highrisk, file.path(here(), "exported-data", "highrisk_locations.csv"), row.names = F)

```


Make categories
```{r calculate distances to high risk areas}

# Make more general categories
Highrisk <- read.csv(file.path(here(), "exported-data", "highrisk_locations.csv"))
unique(Highrisk$category)

Highrisk %<>% mutate(type = recode(category,
                                  "fedex" = "mail carriers", 
                                  "ups" = "mail carriers",
                                  "amazonFullfilment" = "mail carriers", 
                                  "landscape" = "wood", 
                                  "lumberYards" = "wood", 
                                  "sawmill" = "wood",
                                  "amusementParks" = "hobbies",
                                  "auctionCenter" = "hobbies",
                                  "campground" = "hobbies", 
                                  "casino" = "hobbies",
                                  "fairgrounds" = "hobbies",
                                  "fleamarket" = "hobbies",
                                  "racetracks" = "hobbies",
                                  "stadiums" = "hobbies", 
                                  "summerCamp" = "hobbies",
                                  "truckGarage" = "garages", 
                                  "autoRepair" = "garages",
                                  "boatLaunches" = "boat", 
                                  "marinas" = "boat",
                                  "movingCompanies" = "other", 
                                  "bottlingPlants" = "other", 
                                  "college" = "other", 
                                  "farmerMarket" = "other", 
                                  "truckStops" = "other",
                                  "distributionCenter" = "other", 
                                  "intermodal" = "other", 
                                  "armories" = "other"))
unique(Highrisk$type)

write.csv(Highrisk, file.path(here(), "exported-data", "highrisk_cat.csv"), row.names=F)

#Visualize data
Highrisk_layer <- st_as_sf(x = Highrisk, coords = c("Longitude", "Latitude"), crs = 4269)
Highrisk_proj <- st_transform(Highrisk_layer, crs = "ESRI:102010")

ggplot(data = states) +
    geom_sf(data = states, fill = "white") +
  geom_sf(data = Highrisk_proj, col = "blue") +
    labs(x = "Longitude", y = "Latitude")+
    theme(legend.position="top") +
  coord_sf(xlim = c(1000000, 2000000), ylim = c(-200000, 600000), expand = FALSE)
```


Create layers for landscape features
```{r make files per category}
# Subset this file in all categories & Apply buffers around points
unique(Highrisk_proj$type)

# Level of interest: high
# Mail carriers
mail <- Highrisk_proj %>% filter(type == "mail carriers")
mail_buffer <- st_buffer(mail, dist = 50)
dim(mail_buffer) #86

ggplot(data = states) +
    geom_sf(data = states, fill = "white") +
  geom_sf(data = mail_buffer, col = "blue", size = 2) +
    labs(x = "Longitude", y = "Latitude")+
  coord_sf(xlim = c(1000000, 2000000), ylim = c(-200000, 600000), expand = FALSE)


# Wood
wood <- Highrisk_proj %>% filter(type == "wood")
dim(wood)
wood_buffer <- st_buffer(wood, dist = 100)

ggplot(data = states) +
    geom_sf(data = states, fill = "white") +
  geom_sf(data = wood_buffer, col = "blue", size = 2) +
    labs(x = "Longitude", y = "Latitude") +
  coord_sf(xlim = c(1000000, 2000000), ylim = c(-200000, 600000), expand = FALSE)


# Wineries
wineries <- Highrisk_proj %>% filter(type == "winery")
wineries_buffer <- st_buffer(wineries, dist = 100)
dim(wineries)

ggplot(data = states) +
    geom_sf(data = states, fill = "white") +
  geom_sf(data = wineries_buffer, col = "blue", size = 2) +
    labs(x = "Longitude", y = "Latitude")+
  coord_sf(xlim = c(1000000, 2000000), ylim = c(-200000, 600000), expand = FALSE)



# People hobbies
people <- Highrisk_proj %>% filter(type == "hobbies")
people_buffer <- st_buffer(people, dist = 100)
dim(people)

ggplot(data = states) +
    geom_sf(data = states, fill = "white") +
  geom_sf(data = people_buffer, col = "blue", size = 2) +
    labs(x = "Longitude", y = "Latitude")+
  coord_sf(xlim = c(1000000, 2000000), ylim = c(-200000, 600000), expand = FALSE)


# Garages, Boat, Other
garages <- Highrisk_proj %>% filter(type == "garages")
garages_buffer <- st_buffer(garages, dist = 50)
dim(garages)

ggplot(data = states) +
    geom_sf(data = states, fill = "white") +
  geom_sf(data = garages_buffer, col = "blue", size = 2) +
    labs(x = "Longitude", y = "Latitude")+
  coord_sf(xlim = c(1000000, 2000000), ylim = c(-200000, 600000), expand = FALSE)



boats <- Highrisk_proj %>% filter(type ==  "boat")
boats_buffer <- st_buffer(boats, dist = 50)
dim(boats)

ggplot(data = states) +
    geom_sf(data = states, fill = "white") +
  geom_sf(data = boats_buffer, col = "blue", size = 2) +
    labs(x = "Longitude", y = "Latitude")+
  coord_sf(xlim = c(1000000, 2000000), ylim = c(-200000, 600000), expand = FALSE)


others <- Highrisk_proj %>% filter(type == "other")
others_buffer <- st_buffer(others, dist = 50)
dim(others_buffer)

ggplot(data = states) +
    geom_sf(data = states, fill = "white") +
  geom_sf(data = others_buffer, col = "blue", size = 2) +
    labs(x = "Longitude", y = "Latitude")+
  coord_sf(xlim = c(1000000, 2000000), ylim = c(-200000, 600000), expand = FALSE)



```    


Load data for ports and airports
```{r data for ports and airports}
airports <- st_read(file.path(here(), "figures", "GIS", "airports_primary_buffer.shp"), quiet = T)
st_crs(airports)
airports <- st_transform(airports, crs = "ESRI:102010")

ports <- st_read(file.path(here(), "figures", "GIS", "Ports_projected_buffer200.shp"), quiet = T)
st_crs(ports)
```


## Load SLF data

First load the dataset for the SLF data and clip the form of PA
```{r load datasets, message = FALSE, warning = FALSE}

# SLF data (in the folder SLF_datascience)
# First, load the dataset that contains the location of each survey
# Extract each point independently of the year it was sampled (for distance calculations)
grid_unique_chull <- read.csv(file.path(here(), "exported-data", "grid_chull_unique.csv"))
dim(grid_unique_chull) #32911
head(grid_unique_chull)

# Make it a shapefile to visualize the data
grid_chull_layer <- st_as_sf(x = grid_unique_chull, coords = c("longitude_rounded", "latitude_rounded"), crs = "EPSG:4269", remove = F) %>% st_transform(crs = "ESRI:102010")

# Visualize the data
ggplot(data = states) +
  geom_sf(data = states, fill = "white") +
  geom_sf(data = grid_chull_layer, col = "red") +
  coord_sf(xlim = c(1000000, 2000000), ylim = c(-200000, 600000), expand = FALSE) +
  labs(x = "Longitude", y = "Latitude")

```


## Clip the form of PA

Form of PA
```{r PA form}
US <- st_read(file.path(here(), "figures", "GIS", "gadm36_Cont_USA_county.shp"), crs = "EPSG:4326", quiet = T)
st_crs(US)
US <- st_transform(US, crs = "ESRI:102010")
Pennsylvania <- US %>% filter(NAME_1 == "Pennsylvania")

# Visualize Pennsylvania
ggplot(data = US, fill = "white") +
  geom_sf() +
  geom_sf(data = Pennsylvania, col = "blue") +
  labs(x = "Longitude", y = "Latitude") +
  coord_sf(xlim = c(1000000, 2000000), ylim = c(-200000, 600000), expand = FALSE)
```

In grid data
```{r clip grid data in PA}

slfPA_unique <- st_intersection(grid_chull_layer, Pennsylvania) 
dim(slfPA_unique) #20,286

# Visualize shapefiles
ggplot(data = states) +
    geom_sf(data = states, fill = "white") +
  geom_sf(data = grid_chull_layer, col = "red") +
  geom_sf(data = slfPA_unique, col = "blue") +
    labs(x = "Longitude", y = "Latitude")+
    theme(legend.position="top") +
  coord_sf(xlim = c(1000000, 2000000), ylim = c(-200000, 600000))
  
# This is PA clipped from the chull
st_geometry(slfPA_unique) <- NULL
write.csv(slfPA_unique, file.path(here(), "exported-data", "slfPA_unique.csv"), row.names=F)
```

In airports and ports
```{r clip airports and ports in PA}

airportsPA <- st_intersection(airports, Pennsylvania) 
st_write(airportsPA, file.path(here(), "figures", "GIS", "airportsPA.shp"))

# Visualize shapefiles
ggplot(data = states) +
    geom_sf(data = states, fill = "white") +
  geom_sf(data = airports) +
  geom_sf(data = airportsPA, col = "blue", size = 5) +
    labs(x = "Longitude", y = "Latitude")+
    theme(legend.position="top") +
  coord_sf(xlim = c(1000000, 2000000), ylim = c(-200000, 600000))


portsPA <- st_intersection(ports, Pennsylvania) 
st_write(portsPA, file.path(here(), "figures", "GIS", "portsPA.shp"))

# Visualize shapefiles
ggplot(data = states) +
    geom_sf(data = states, fill = "white") +
  geom_sf(data = ports) +
  geom_sf(data = portsPA, col = "blue", size = 5) +
    labs(x = "Longitude", y = "Latitude")+
    theme(legend.position="top") +
  coord_sf(xlim = c(1000000, 2000000), ylim = c(-200000, 600000))
```



## Calculate distances to landscape features
Calculate distances to points of interest in PA (a few hours overnight locally)
It includes:
- mail_buffer, people_buffer, others_buffer, wood_buffer, boats_buffer, garages_buffer, wineries_buffer
- airports, ports

```{r distance of SLF to points of interest, eval = FALSE}
unique(Highrisk$type)

# Create rows for distances
slfPA_unique %<>% add_column(DistToMail = NA,
                      DistToWood = NA,
                      DistToWineries = NA,
                      DistToHobbies = NA,
                      DistToGarages = NA,
                      DistToBoats = NA,
                      DistToOthers = NA,
                      DistToAirports = NA,
                      DistToPorts = NA)

#Calculate their distance to transport infrastructures
for (j in 1:length(slfPA_unique$DistToMail)){ 
  # Print the row being considered
  print(j)
  
  # Calculate distance to the closest mail-related point
  dist_mail <- st_distance(x = slfPA_unique[j,], y = mail_buffer)
  slfPA_unique$DistToMail[j] <- min(dist_mail)
  
  # Calculate distance to the closest wood-related point
  dist_wood <- st_distance(x = slfPA_unique[j,], y = wood_buffer)
  slfPA_unique$DistToWood[j] <- min(dist_wood)
  
  # Calculate distance to the closest winery
  dist_wine <- st_distance(x = slfPA_unique[j,], y = wineries_buffer)
  slfPA_unique$DistToWineries[j] <- min(dist_wine)
  
  # Calculate distance to the closest location for people
  dist_people <- st_distance(x = slfPA_unique[j,], y = people_buffer)
  slfPA_unique$DistToHobbies[j] <- min(dist_people)
  
  # Calculate distance to the closest garage
  dist_garage <- st_distance(x = slfPA_unique[j,], y = garages_buffer)
  slfPA_unique$DistToGarages[j] <- min(dist_garage)

  # Calculate distance to the closest boat launch
  dist_boats <- st_distance(x = slfPA_unique[j,], y = boats_buffer)
  slfPA_unique$DistToBoats[j] <- min(dist_boats)
  
  # Calculate distance to other infrastructures
  dist_others <- st_distance(x = slfPA_unique[j,], y = others_buffer)
  slfPA_unique$DistToOthers[j] <- min(dist_others)
  
  # Calculate distance to airports
  dist_airports <- st_distance(x = slfPA_unique[j,], y = airportsPA)
  slfPA_unique$DistToAirports[j] <- min(dist_airports)
  
  # Calculate distance to ports
  dist_ports <- st_distance(x = slfPA_unique[j,], y = portsPA)
  slfPA_unique$DistToPorts[j] <- min(dist_ports)
  
  if (j %% 1000 == 0){
    distances_structures <- slfPA_unique
    st_geometry(distances_structures) <- NULL
    write.csv(distances_structures, file.path(here(), "exported-data",  "distances_observed_highrisk.csv"), row.names = F)  
    }
}


# Save file
st_geometry(slfPA_unique) <- NULL
write.csv(slfPA_unique, file.path(here(), "exported-data", "distances_observed_highrisk.csv"), row.names = F)
```


Make sure all distances have been calculated
```{r check distances}
# Distance data
grid_poi <- read.csv(file.path(here(), "exported-data", "distances_observed_highrisk.csv"), h=T)

# Check if there are points without distance
grid_poi %>% filter(DistToMail == Inf | is.na(DistToMail))
grid_poi %>% filter(DistToWood == Inf | is.na(DistToWood))
grid_poi %>% filter(DistToWineries == Inf | is.na(DistToWineries))
grid_poi %>% filter(DistToHobbies == Inf | is.na(DistToHobbies))
grid_poi %>% filter(DistToGarages == Inf | is.na(DistToGarages))
grid_poi %>% filter(DistToBoats == Inf | is.na(DistToBoats))
grid_poi %>% filter(DistToOthers == Inf | is.na(DistToOthers))
grid_poi %>% filter(DistToAirports == Inf | is.na(DistToAirports))
grid_poi %>% filter(DistToPorts == Inf | is.na(DistToPorts))


# #Calculate the distance for points where they are missing
# grid_transports <- st_as_sf(x = grid_data_chull, 
#                             coords = c("longitude_rounded", "latitude_rounded"), 
#                             crs = "EPSG:4269", remove = F)
# grid_transports <- st_transform(grid_transports, crs = "ESRI:102010")
#     
# # calculate the distance for missing points
# for (j in 1:length(grid_transports$DistToRail)){ 
#   if (grid_transports$DistToRail[j] > 50000){
#     print(j)
# 
#     # Calculate distance to the closest railroad
#     dist_rail <- st_distance(x = grid_transports[j,], y = rail_chull)
#     grid_transports$DistToRail[j] <- min(dist_rail)
#   }
#   
#   if (grid_transports$DistToRoad[j] > 50000){
#     print(j)
# 
#     # Calculate distance to the closest road
#     dist_road <- st_distance(x = grid_transports[j,], y = road_chull)
#     grid_transports$DistToRoad[j] <- min(dist_road)
#   }
# }
# 
# # Check result
# grid_transports %>% filter(DistToRail == Inf | is.na(DistToRail))
# grid_transports %>% filter(DistToRoad == Inf | is.na(DistToRoad))
# 
# # Save file
# st_geometry(grid_transports) <- NULL
# write.csv(grid_transports, file.path(here(), "exported-data", "distances_observed_transports_complete.csv"), row.names = F)
```

\newpage


# 3. Create datasets for jumpers/diffusers/negatives

We need to associate each point to the fact that it's a jumper, or diffuser, or negative point, by merging the table with observed distances and the table with status.

Form of PA
```{r PA form}
US <- st_read(file.path(here(), "figures", "GIS", "gadm36_Cont_USA_county.shp"), crs = "EPSG:4326", quiet = T)
st_crs(US)
US <- st_transform(US, crs = "ESRI:102010")
Pennsylvania <- US %>% filter(NAME_1 == "Pennsylvania")

# Visualize Pennsylvania
ggplot(data = US, fill = "white") +
  geom_sf() +
  geom_sf(data = Pennsylvania, col = "blue") +
  labs(x = "Longitude", y = "Latitude") +
  coord_sf(xlim = c(1000000, 2000000), ylim = c(-200000, 600000), expand = FALSE)
```

1. Prepare the tables
```{r choose which dataset you are testing}

grid_poi <- read.csv(file.path(here(), "exported-data", "distances_observed_highrisk.csv"))
head(grid_poi)
dim(grid_poi) #20,286


# Create SLF PA dataset with all points, including, including column slf established
grid_data_chull <- read.csv(file.path(here(), "exported-data", "grid_data_chull.csv"))
dim(grid_data_chull) #45,964
head(grid_data_chull)

grid_data_chull <- st_as_sf(x = grid_data_chull, coords = c("longitude_rounded", "latitude_rounded"), crs = "EPSG:4269", remove = F) %>% st_transform(crs = "ESRI:102010")

slfPA <- st_intersection(grid_data_chull, Pennsylvania)
head(slfPA)
dim(slfPA) #29850
st_geometry(slfPA) <- NULL
write.csv(slfPA, file.path(here(), "exported-data", "slfPA.csv"), row.names=F)

# Merge it with the distance data
slfPA %<>% left_join(grid_poi) %>% select(-c(GID_0, NAME_0, GID_1, NAME_1, NL_NAME_1, GID_2, NAME_2, VARNAME_2, NL_NAME_2, TYPE_2, CC_2, HASC_2, ENGTYPE_2))
head(slfPA)
dim(slfPA)

# Verify if all points got a distance
slfPA %>% filter(DistToMail == Inf | is.na(DistToMail))
slfPA %>% filter(DistToWood == Inf | is.na(DistToWood))
slfPA %>% filter(DistToHobbies == Inf | is.na(DistToHobbies))

# Create the categories diffusers (if established) or negatives (if not)
slfPA %<>% mutate(Category_out = ifelse(slf_established == TRUE, "Diffusers", "Negatives"))

# Next load the dataset that contains all jumpers identified by sets of parameters
jumpers <- read.csv(file.path(here(), "exported-data", "jumps_full_rarefied.csv"))
jumpers %<>% add_column(Category = "Jumpers")
jumpers <- st_as_sf(x = jumpers, coords = c("longitude_rounded", "latitude_rounded"), crs = "EPSG:4269", remove = F) %>% st_transform(crs = "ESRI:102010")
jumpersPA <- st_intersection(jumpers, Pennsylvania)
st_geometry(jumpersPA) <- NULL
```

2. Create full dataset (multiple jumps per location)
```{r create full dataset}
# Put jumpers into the grid data and complete with the rest
# Verify if all jumps are added or only PA!
slfPA_full <- slfPA %>% 
  left_join(jumpersPA %>% select(latitude_rounded, longitude_rounded, bio_year, Category, Rarefied)) %>% 
  mutate(Category = ifelse(is.na(Category), Category_out, Category)) %>% 
  select(-Category_out)

# Verify we still have the right number of jumps
dim(slfPA_full %>% filter(Category == "Jumpers"))[1] == dim(jumpersPA)[1]

slfPA_full %<>% rename(Category_full = Category)
```

3. Create rarefied dataset (one jump per location)
```{r create rarefied dataset}
# Put jumpers into the grid data and complete with the rest
slfPA_rarefied <- slfPA %>% 
  left_join(jumpersPA %>% filter(Rarefied == TRUE) %>% 
                                    select(latitude_rounded, longitude_rounded, bio_year, Category, Rarefied)) %>% 
  mutate(Category = ifelse(is.na(Category), Category_out, Category)) %>% 
  select(-Category_out)

# Verify we still have the right number of jumps
dim(slfPA_rarefied %>% filter(Category == "Jumpers"))[1] == dim(jumpersPA %>% filter(Rarefied == TRUE))[1]

slfPA_rarefied %<>% rename(Category_rarefied = Category) 
```


4. Reassemble in one long dataset with one column for the type of dataset
```{r reassemble full and rarefied dataset}
slfPA_cat <- merge(slfPA_full, slfPA_rarefied)

write.csv(slfPA_cat, file.path(here(), "exported-data", "slfPA_observed.csv"), row.names = F)
```

\newpage



# 4. Create dataset "as of today" or "most up to date"

Count each point only once as positive or negative (summarise data for each point)
```{r generate a grid of unique points for situation as of 2020}

slfPA_cat <- read.csv(file.path(here(), "exported-data", "slfPA_observed.csv"))
dim(slfPA_cat) #29850

# Show the number of duplicates per point
slfPA_cat %>% group_by(latitude_rounded, longitude_rounded) %>% count() %>% 
  group_by(n) %>% count(n)

# Order the category level
unique(slfPA_cat$Category_rarefied)
slfPA_cat$Category_rarefied <- factor(slfPA_cat$Category_rarefied, levels = c("Negatives", "Diffusers", "Jumpers"))
slfPA_cat$Category_full <- factor(slfPA_cat$Category_full, levels = c("Negatives", "Diffusers", "Jumpers"))


# Translate factors into ordinal
slfPA_uptodate <- slfPA_cat %>% mutate(Category_rare_num = as.numeric(Category_rarefied),
                    Category_full_num = as.numeric(Category_full)) %>%  
  group_by(latitude_rounded, longitude_rounded, across(starts_with("Dist"))) %>% 
  summarize(Category_rare_max = max(Category_rare_num),
            Category_full_max = max(Category_full_num)) %>% 
  mutate(Category_rare = recode(Category_rare_max, "1" = "Negatives", "2" = "Diffusers", "3" = "Jumpers"),
         Category_full = recode(Category_full_max, "1" = "Negatives", "2" = "Diffusers", "3" = "Jumpers")) %>% 
  ungroup() %>% 
  select(-Category_rare_max, -Category_full_max, -DistToIntro)
  

dim(slfPA_uptodate) #20,286
head(slfPA_uptodate)

# Save this file
write.csv(slfPA_uptodate, file.path(here(), "exported-data", "slfPA_uptodate.csv"), row.names=F)
```


Calculate statistics
```{r calculate proportions of data}
positive <- slfPA_uptodate %>% filter(Category_full %in% c("Diffusers", "Jumpers")) 
dim(positive)[1]/dim(slfPA_uptodate)[1] #18.91%
```

\newpage




# 5. Generate a random dispersal distribution of distances of jumpers to high-risk areas

The idea is to generate a distribution of distances to landscape features under the null hypothesis that SLF disperse randomly in the landscape. If a high number of random distributions are generated, we obtain the distribution of random dispersal distances to transports. The comparison of the random distribution and the observed average value gives the probability that the observed value is random.

Here, we generate 9,999 random datasets of distances to transport infrastructure within the convex hull of the positive SLF surveys. If the average value of the observed data is comprised within the simulated random values, it means that the pattern could be found by chance, and the distance between observed points and transport infrastructures is not significant.

To simplify calculations, we calculate all possible coordinates. We could remove those that were already calculated with slfPA, but here the calculation is relatively quick (and I could not make the code work because the coordinates seem to be slightly off between the slfPA and Coordinates_PA layers, for some reason)

Then the analysis consists in: (1) generating a random dataset of the same number of points as in the observed data, (2) for each random point, calculating the distance to transport infrastructures, (3) taking the average distance to each transport infrastructure for the dataset, (4) redoing the same cycle again for a total of 9,999 simulated random datasets.


## Create random coordinates
Form of PA
```{r PA form}
US <- st_read(file.path(here(), "figures", "GIS", "gadm36_Cont_USA_county.shp"), crs = "EPSG:4326", quiet = T)
st_crs(US)
US <- st_transform(US, crs = "ESRI:102010")
Pennsylvania <- US %>% filter(NAME_1 == "Pennsylvania")

# Visualize Pennsylvania
ggplot(data = US, fill = "white") +
  geom_sf() +
  geom_sf(data = Pennsylvania, col = "blue") +
  labs(x = "Longitude", y = "Latitude") +
  coord_sf(xlim = c(1000000, 2000000), ylim = c(-200000, 600000), expand = FALSE)
```

Create coordinates
```{r dataset with points only for PA, echo=FALSE, warning =FALSE, message = FALSE}

# Load the SLF data
slfPA <- read.csv(file.path(here(), "exported-data", "slfPA_uptodate.csv"))
slfPA <- st_as_sf(x = slfPA, coords = c("longitude_rounded", "latitude_rounded"), crs = "EPSG:4269", remove = F) %>% 
  st_transform(crs = "ESRI:102010")

# (1) Calculate all potential coordinates
maxlat <- max(slfPA$latitude_rounded)
minlat <- min(slfPA$latitude_rounded)
maxlong <- max(slfPA$longitude_rounded)  
minlong <- min(slfPA$longitude_rounded)

Seqlat <- seq(from = minlat, to = maxlat, by = 1/111)
Seqlong <- seq(from = minlong, to = maxlong, by = 1/85)
Coordinates <- expand.grid(latitude = Seqlat, longitude = Seqlong)
Coordinates %<>% mutate(latitude_rounded = round(latitude, 5),
         longitude_rounded = round(longitude, 5))

Coordinates <- st_as_sf(x = Coordinates, coords = c("longitude_rounded", "latitude_rounded"), crs = "EPSG:4269", remove = F) %>% 
  st_transform(crs = "ESRI:102010")

# Plot this and check if this is correct
ggplot(data = US, fill = "white") +
  geom_sf() +
  geom_sf(data = Coordinates, alpha = 0.5, col = "blue") +
  geom_sf(data = slfPA, col = "red") +
  labs(x = "Longitude", y = "Latitude") +
  coord_sf(xlim = c(1000000, 2000000), ylim = c(-200000, 600000), expand = FALSE)


# (2) Keep only those in PA
Coordinates_PA <- st_intersection(Coordinates, Pennsylvania)

ggplot(data = US, fill = "white") +
  geom_sf() +
  geom_sf(data = Coordinates_PA, col = "blue") +
  geom_sf(data = slfPA, col = "red") +
  labs(x = "Longitude", y = "Latitude") +
  coord_sf(xlim = c(1000000, 2000000), ylim = c(-200000, 600000), expand = FALSE)

Coordinates_PA %<>% select(latitude_rounded, longitude_rounded, geometry)


# (3) Keep only the coordinates that are in the minimum convex polygon
chull <- st_read(file.path(here(), "figures", "GIS", "chull.shp"), quiet = T) %>% 
  st_transform(crs = "ESRI:102010")

Coordinates_chull <- st_intersection(Coordinates, chull)

ggplot(data = US, fill = "white") +
  geom_sf() +
  geom_sf(data = Coordinates_chull, alpha = 0.5) +
  geom_sf(data = slfPA, col = "blue") +
  labs(x = "Longitude", y = "Latitude") +
  coord_sf(xlim = c(1000000, 2000000), ylim = c(-200000, 600000), expand = FALSE)

# We don't subset those that are on land since there is no shoreline in PA!

# Save file
Coordinates_chull %<>% select(latitude_rounded, longitude_rounded, geometry)
st_geometry(Coordinates_chull) <- NULL 
write.csv(Coordinates_chull, file.path(here(), "exported-data", "simulated_coordinates_PA.csv"), row.names = F)

```


## Calculate distances
Calculate distances to high risk for those points (copy paste code from above). Should be done in a few hours locally. 
Can reduce the time by removing points for which we already have data (observed data)

```{r select points with missing distances}
Coordinates_simulated <- read.csv(file.path(here(), "exported-data",  "simulated_coordinates_PA.csv"))
dim(Coordinates_simulated)
head(Coordinates_simulated)
anyDuplicated(Coordinates_simulated)
``` 


```{r calculate distances of PA points to POI}

# Create rows for distances
Coordinates_simulated %<>% add_column(DistToMail = NA,
                      DistToWood = NA,
                      DistToWineries = NA,
                      DistToPeople = NA,
                      DistToGarages = NA,
                      DistToBoats = NA,
                      # DistToOthers = NA,
                      DistToAirports = NA,
                      DistToPorts = NA)

# Transform into a sf layer
Coordinates_simulated <- st_as_sf(x = Coordinates_simulated, coords = c("longitude_rounded", "latitude_rounded"), crs = 4269, remove = F)
Coordinates_simulated <- st_transform(Coordinates_simulated, crs = "ESRI:102010")

#Calculate their distance to transport infrastructures
for (j in 1:length(Coordinates_simulated$DistToMail)){ 
  print(j)
  # Calculate distance to the closest mail-related point
  dist_mail <- st_distance(x = Coordinates_simulated[j,], y = mail_buffer)
  Coordinates_simulated$DistToMail[j] <- min(dist_mail)
  
  # Calculate distance to the closest wood-related point
  dist_wood <- st_distance(x = Coordinates_simulated[j,], y = wood_buffer)
  Coordinates_simulated$DistToWood[j] <- min(dist_wood)
  
  # Calculate distance to the closest winery
  dist_wine <- st_distance(x = Coordinates_simulated[j,], y = wineries_buffer)
  Coordinates_simulated$DistToWineries[j] <- min(dist_wine)
  
  # Calculate distance to the closest location for people
  dist_people <- st_distance(x = Coordinates_simulated[j,], y = people_buffer)
  Coordinates_simulated$DistToPeople[j] <- min(dist_people)
  
  # Calculate distance to the closest garage
  dist_garage <- st_distance(x = Coordinates_simulated[j,], y = garages_buffer)
  Coordinates_simulated$DistToGarages[j] <- min(dist_garage)

  # Calculate distance to the closest boat launch
  dist_boats <- st_distance(x = Coordinates_simulated[j,], y = boats_buffer)
  Coordinates_simulated$DistToBoats[j] <- min(dist_boats)
  
  # Calculate distance to the category others
  # dist_others <- st_distance(x = Coordinates_simulated[j,], y = others_buffer)
  # Coordinates_simulated$DistToOthers[j] <- min(dist_others)
  
  # Calculate distance to the closest airport
  dist_airport <- st_distance(x = Coordinates_simulated[j,], y = airportsPA)
  Coordinates_simulated$DistToAirport[j] <- min(dist_airport)
  
  # Calculate distance to the closest port
  dist_port <- st_distance(x = Coordinates_simulated[j,], y = portsPA)
  Coordinates_simulated$DistToPort[j] <- min(dist_port)
  
  if (j %% 500 == 0){
    print(j)
    distances_random_PA <- Coordinates_simulated
    st_geometry(distances_random_PA) <- NULL
    write.csv(distances_random_PA, file.path(here(), "exported-data",  "distances_simulated_highrisk.csv"), row.names = F)  
    }
}


# Verify that there is no 0
Coordinates_simulated %>% filter(DistToMail == Inf | is.na(DistToMail))
Coordinates_simulated %>% filter(DistToWood == Inf | is.na(DistToWood))
st_geometry(Coordinates_simulated) <- NULL

# Save file
write.csv(Coordinates_simulated, file.path(here(), "exported-data",  "distances_simulated_highrisk.csv"), row.names = F)
```


If we don't want to recalculate all the distances, we can simply use data from previous calculations
```{r use data from previous versions}
head(Coordinates_simulated)
dim(Coordinates_simulated)

#load previous data with distances for all PA
Coordinates_simulated_old <- read.csv(file.path(here(), "tables", "Distances_random distrib", "grid_distances_poiPA.csv"))
dim(Coordinates_simulated_old)
Coordinates_simulated_old <- st_as_sf(x = Coordinates_simulated_old, coords = c("longitude_rounded", "latitude_rounded"), 
                                      crs = "EPSG:4269", remove = F) %>% 
  st_transform(crs = "ESRI:102010")

# Keep only the coordinates that are in the minimum convex polygon
chull <- st_read(file.path(here(), "figures", "GIS", "chull.shp"), quiet = T) %>% 
  st_transform(crs = "ESRI:102010")

Coordinates_simulated_old <- st_intersection(Coordinates_simulated_old, chull)
dim(Coordinates_simulated_old)
anyDuplicated(Coordinates_simulated_old)

ggplot(data = states, fill = "white") +
  geom_sf() +
  geom_sf(data = Coordinates_simulated_old, col = "yellow") +
  geom_sf(data = Coordinates_simulated, col = "blue", alpha = 0.5) +
  labs(x = "Longitude", y = "Latitude") +
  coord_sf(xlim = c(1400000, 1450000), ylim = c(300000, 350000), expand = FALSE)

st_geometry(Coordinates_simulated_old) <- NULL
Coordinates_simulated_old %<>% select(latitude_rounded, longitude_rounded, DistToMail, DistToWood, DistToWineries, DistToPeople, DistToGarages, DistToBoats) 
write.csv(Coordinates_simulated_old, file.path(here(), "exported-data", "distances_simulated_highrisk.csv"), row.names = F)


## Do the same thing with airports and ports
#load previous data with distances for all chull
Coordinates_simulated_airportsports <- read.csv(file.path(here(), "tables", "Distances_random distrib", "Random_transport.csv"))
dim(Coordinates_simulated_airportsports)
Coordinates_simulated_airportsports <- st_as_sf(x = Coordinates_simulated_airportsports, coords = c("longitude", "latitude"), crs = "EPSG:4269", remove = F) %>% 
  st_transform(crs = "ESRI:102010")
anyDuplicated(Coordinates_simulated_airportsports)

# Keep only the coordinates that are in Pennsylvania
Coordinates_simulated_airportsports <- st_intersection(Coordinates_simulated_airportsports, Pennsylvania)
dim(Coordinates_simulated_airportsports)

ggplot(data = states, fill = "white") +
  geom_sf() +
  geom_sf(data = Coordinates_simulated_airportsports, col = "yellow") +
  geom_sf(data = chull, col = "blue", alpha = 0.5) +
  labs(x = "Longitude", y = "Latitude")  +
  coord_sf(xlim = c(1000000, 2000000), ylim = c(-200000, 600000), expand = FALSE)


st_geometry(Coordinates_simulated_airportsports) <- NULL
Coordinates_simulated_airportsports %<>% select(latitude, longitude, DistToRail, DistToRoad, DistToAirport, DistToPort) %>% 
  rename(latitude_rounded = latitude,
         longitude_rounded = longitude)
write.csv(Coordinates_simulated_airportsports, file.path(here(), "exported-data", "distances_simulated_transports.csv"), row.names = F)

```


## Select random dataset, full dataset

From slfPA uptodate (the dataset we use in the end)
```{r select random datasets}

# Load 
slfPA_uptodate <- read.csv(file.path(here(), "exported-data", "slfPA_uptodate.csv"))

Simulated_highrisk <- read.csv(file.path(here(), "exported-data", "distances_simulated_highrisk.csv"))
Simulated_transports <- read.csv(file.path(here(), "exported-data", "distances_simulated_transports.csv"))
head(Simulated_highrisk)
head(Simulated_transports)
# Coordinates don't match but here all we care about is distances!
Coordinates_simulated <- cbind(Simulated_highrisk %>% select(-latitude_rounded, -longitude_rounded), 
                               Simulated_transports%>% select(-latitude_rounded, -longitude_rounded))
head(Coordinates_simulated)

# Size of the dataset to be sampled
jumpers_full = dim(slfPA_uptodate %>% filter(Category_full == "Jumpers"))[1]

for (i in 1:9999){
  #Generate a set of coordinates
  Random_coordinates <- Coordinates_simulated[sample(nrow(Coordinates_simulated), size = jumpers_full, replace = F),] %>%
    add_column(Category_full = "Jumpers")
  
  #Calculate the mean and median distance per simulation
  Random_means <- Random_coordinates %>% group_by(Category_full) %>% 
    summarise_at(vars(starts_with("DistTo")), list(mean = mean, median = median)) %>% 
    add_column(Simulation = i)
  
  #Save the table with the simulation number
  if (i == 1){
    Simulations <- Random_means
  } else {
    Simulations <- bind_rows(Simulations, Random_means)
  }
  
  if (i %% 100 == 0){ print(i)}
} 

write.csv(Simulations, file.path(here(), "exported-data", "slf_simdatasets_full_highrisk.csv"), row.names = F)
```


## Select random datasets, rarefied dataset

```{r select random datasets rarefied}

slfPA_uptodate <- read.csv(file.path(here(), "exported-data", "slfPA_uptodate.csv"))

Simulated_highrisk <- read.csv(file.path(here(), "exported-data", "distances_simulated_highrisk.csv"))
Simulated_transports <- read.csv(file.path(here(), "exported-data", "distances_simulated_transports.csv"))
head(Simulated_highrisk)
head(Simulated_transports)
# Coordinates don't match but here all we care about is distances!
Coordinates_simulated <- cbind(Simulated_highrisk %>% select(-latitude_rounded, -longitude_rounded), 
                               Simulated_transports%>% select(-latitude_rounded, -longitude_rounded))
head(Coordinates_simulated)

# Size of the dataset to be sampled
jumpers_rare = dim(slfPA_uptodate %>% filter(Category_rare == "Jumpers"))[1]


for (i in 1:9999){
  #Generate a set of coordinates
  Random_coordinates <- Coordinates_simulated[sample(nrow(Coordinates_simulated), size = jumpers_rare, replace = F),] %>%
    add_column(Category_rare = "Jumpers")
  
  #Calculate the mean distance per simulation
  Random_means <- Random_coordinates %>% group_by(Category_rare) %>% 
    summarise_at(vars(starts_with("DistTo")), list(mean = mean, median = median)) %>% 
    add_column(Simulation = i)
  
  #Save the table with the simulation number
  if (i == 1){
    Simulations <- Random_means
  } else {
    Simulations <- bind_rows(Simulations, Random_means)
  }
  
  if (i %% 100 == 0){ print(i)}
} 

write.csv(Simulations, file.path(here(), "exported-data", "slf_simdatasets_rarefied_highrisk.csv"), row.names = F)
```


#6. Visualize results

Load observed data
```{r load observed data full}

# Convert the observed data to long format, get the average values
slfPA_uptodate <- read.csv(file.path(here(), "exported-data", "slfPA_uptodate.csv"))
dim(slfPA_uptodate)
head(slfPA_uptodate)
slfPA_uptodate %<>% select(-DistToOthers)


# Also load data for roads and railroads
slf_uptodate <- read.csv(file.path(here(), "exported-data", "slf_obs_uptodate.csv"))
dim(slf_uptodate)
head(slf_uptodate)


# Keep only the coordinates that are in Pennsylvania
slf_uptodate <- st_as_sf(x = slf_uptodate, coords = c("longitude_rounded", "latitude_rounded"), crs = "EPSG:4269", remove = F) %>% 
  st_transform(crs = "ESRI:102010")
slf_uptodate <- st_intersection(slf_uptodate, Pennsylvania)
dim(slf_uptodate)
head(slf_uptodate)
slf_uptodate %<>% select(latitude_rounded, longitude_rounded, DistToRail, DistToRoad, Category_rare, Category_full)
st_geometry(slf_uptodate) <- NULL


# Convert to long format
slfPA_uptodate_long <- slfPA_uptodate %>%
  pivot_longer(cols = starts_with("DistTo"), names_to = "DistanceType", values_to = "DistanceValue")
slf_uptodate_long <- slf_uptodate %>%
  pivot_longer(cols = starts_with("DistTo"), names_to = "DistanceType", values_to = "DistanceValue")

# Calculate mean and median for full dataset
slf_obsmeans_full <- slfPA_uptodate_long %>%
  filter(Category_full == "Jumpers") %>% 
  group_by(Category_full, DistanceType) %>% 
  summarise(MeanDistance = mean(DistanceValue),
            MedianDistance = median(DistanceValue))

slftrans_obsmeans_full <- slf_uptodate_long %>%
  filter(Category_full == "Jumpers") %>% 
  group_by(Category_full, DistanceType) %>% 
  summarise(MeanDistance = mean(DistanceValue),
            MedianDistance = median(DistanceValue))

slf_obsmeans_full <- rbind(slf_obsmeans_full, slftrans_obsmeans_full)


# Calculate mean and median for rarefied dataset
slf_obsmeans_rarefied <- slfPA_uptodate_long %>%
  filter(Category_rare == "Jumpers") %>% 
  group_by(Category_rare, DistanceType) %>% 
  summarise(MeanDistance = mean(DistanceValue),
            MedianDistance = median(DistanceValue))

slftrans_obsmeans_rarefied <- slf_uptodate_long %>%
  filter(Category_rare == "Jumpers") %>% 
  group_by(Category_rare, DistanceType) %>% 
  summarise(MeanDistance = mean(DistanceValue),
            MedianDistance = median(DistanceValue))

slf_obsmeans_rarefied <- rbind(slf_obsmeans_rarefied, slftrans_obsmeans_rarefied)
```

## Full dataset
Select mean and median for full dataset
```{r plot null distribution to high risk, fig.height=2, fig.width = 6, fig.cap="Comparison of the distance of jumpers to high-risk areas to a random distribution."}

# Load the simulated values
Simulations_full <- read.csv(file.path(here(), "exported-data", "slf_simdatasets_full_highrisk.csv"))
names(Simulations_full)
head(Simulations_full)

# Convert simulations to long format
Simulations_full_long <- Simulations_full %>% pivot_longer(cols = starts_with("DistTo"), names_to = "DistanceType", values_to = "DistanceValue")

# Select only the means
Simulations_full_long_mean <- Simulations_full_long %>%
  filter(grepl("_mean", DistanceType))
Simulations_full_long_mean$DistanceType <- gsub(x = Simulations_full_long_mean$DistanceType, pattern = "_mean", replacement = "")
Simulations_full_long_mean %<>% mutate(DistanceType = recode(DistanceType, 
                                      "DistToAirport" = "DistToAirports",
                                      "DistToPort" = "DistToPorts",
                                      "DistToPeople" = "DistToHobbies"))

# Select only the medians
Simulations_full_long_median <- Simulations_full_long %>%
  filter(grepl("_median", DistanceType))
Simulations_full_long_median$DistanceType <- gsub(x = Simulations_full_long_median$DistanceType, pattern = "_median", replacement = "")
Simulations_full_long_median %<>% mutate(DistanceType = recode(DistanceType, 
                                      "DistToAirport" = "DistToAirports",
                                      "DistToPort" = "DistToPorts",
                                      "DistToPeople" = "DistToHobbies"))
```

Visualize results
```{r mean and median for observed values}

#Plot means
random_transport <- ggplot() +
  geom_histogram(data = Simulations_full_long_mean, 
                 aes(x =  DistanceValue/1000, y = ..density..), binwidth = 0.1) +
  geom_vline(data = slf_obsmeans_full,
             mapping = aes(xintercept = MeanDistance/1000), size = 1) +
  scale_fill_brewer(palette = "Dark2") +
  scale_color_brewer(palette = "Dark2") +
  xlab("Distance to the nearest... (km)") +
  ylab("Count (simulations)") +
  facet_wrap(~DistanceType, ncol = 5, scale = "free") +
  theme_classic()

random_transport

ggsave(file.path(here(), "figures", "jump_highrisk", "bootstrap_fullmeans.jpg"), random_transport, width = 10, height = 3)



# Plot medians
random_transport <- ggplot() +
  geom_histogram(data = Simulations_full_long_median, 
                 aes(x =  DistanceValue/1000, y = ..density..), binwidth = 0.1) +
  geom_vline(data = slf_obsmeans_full,
             mapping = aes(xintercept = MedianDistance/1000), size = 1) +
  scale_fill_brewer(palette = "Dark2") +
  scale_color_brewer(palette = "Dark2") +
  xlab("Distance to the nearest... (km)") +
  ylab("Count (simulations)") +
  facet_wrap(~DistanceType, ncol = 5, scale = "free") +
  theme_classic()

random_transport

ggsave(file.path(here(), "figures", "jump_highrisk", "bootstrap_fullmedians.jpg"), random_transport, width = 10, height = 3)
``` 

Count simulations
```{r count simulations}
# COUNT HOW MANY SIMULATIONS ARE SMALLER THAN THE OBS VALUE
obswineries <- slf_obsmeans_full %>% filter(DistanceType == "DistToWineries") %>% pull(MedianDistance) 
dim(Simulations_full_long_median %>% filter(DistanceType == "DistToWineries", DistanceValue < obswineries))[1]
p = 9/9999

obsports <- slf_obsmeans_full %>% filter(DistanceType == "DistToPorts") %>% pull(MedianDistance) 
dim(Simulations_full_long_median %>% filter(DistanceType == "DistToPorts", DistanceValue < obsports))[1]
p = 9978/9999

# All the others are zero
# MAJOR difference with previous results: airports have a much higher difference than before!

```




## Rarefied dataset

Calculate mean and median for simulated data
```{r count simulations rarefied}

# Load the simulated values
Simulations_rare <- read.csv(file.path(here(), "exported-data", "slf_simdatasets_rarefied_highrisk.csv"))

# Convert simulations to long format
Simulations_rare_long <- Simulations_rare %>% pivot_longer(cols = starts_with("DistTo"), names_to = "DistanceType", values_to = "DistanceValue")
Simulations_rare_long %<>% rename(Category_rarefied = Category_rare) 


# Select only the means
Simulations_rare_long_mean <- Simulations_rare_long %>%
  filter(grepl("_mean", DistanceType))
Simulations_rare_long_mean$DistanceType <- gsub(x = Simulations_rare_long_mean$DistanceType, 
                                                pattern = "_mean", replacement = "")
Simulations_rare_long_mean %<>% mutate(DistanceType = recode(DistanceType, 
                                      "DistToAirport" = "DistToAirports",
                                      "DistToPort" = "DistToPorts",
                                      "DistToPeople" = "DistToHobbies"))

# Select only the medians
Simulations_rare_long_median <- Simulations_rare_long %>%
  filter(grepl("_median", DistanceType))
Simulations_rare_long_median$DistanceType <- gsub(x = Simulations_rare_long_median$DistanceType, 
                                                pattern = "_median", replacement = "")
Simulations_rare_long_median %<>% mutate(DistanceType = recode(DistanceType, 
                                      "DistToAirport" = "DistToAirports",
                                      "DistToPort" = "DistToPorts",
                                      "DistToPeople" = "DistToHobbies"))
```

Visualize results
```{r visualize rare}

#Plot means
random_transport <- ggplot() +
  geom_histogram(data = Simulations_rare_long_mean, 
                 aes(x =  DistanceValue/1000, y = ..density..), binwidth = 0.1) +
  geom_vline(data = slf_obsmeans_rarefied,
             mapping = aes(xintercept = MeanDistance/1000), size = 1) +
  scale_fill_brewer(palette = "Dark2") +
  scale_color_brewer(palette = "Dark2") +
  xlab("Distance to the nearest... (km)") +
  ylab("Count (simulations)") +
  facet_wrap(~DistanceType, ncol = 5, scale = "free") +
  theme_classic()

random_transport

ggsave(file.path(here(), "figures", "jump_highrisk", "bootstrap_raremeans.jpg"), random_transport, width = 10, height = 3)


# Plot median
random_transport <- ggplot() +
  geom_histogram(data = Simulations_rare_long_median, 
                 aes(x =  DistanceValue/1000, y = ..density..), binwidth = 0.1) +
  geom_vline(data = slf_obsmeans_rarefied,
             mapping = aes(xintercept = MedianDistance/1000), size = 1) +
  scale_fill_brewer(palette = "Dark2") +
  scale_color_brewer(palette = "Dark2") +
  xlab("Distance to the nearest... (km)") +
  ylab("Count (simulations)") +
  facet_wrap(~DistanceType, ncol = 5, scale = "free") +
  theme_classic() 

ggsave(file.path(here(), "figures", "jump_highrisk", "bootstrap_raremedians.jpg"), random_transport, width = 10, height = 3)
```

Count simulations
```{r count simulations rare}

# Count how many simulations are lower than the observed data
# If it's below 5% it's not random

# MEDIANS
obsairports <- slf_obsmeans_rarefied %>% filter(DistanceType == "DistToAirports") %>% pull(MeanDistance) 
dim(Simulations_rare_long_mean %>% filter(DistanceType == "DistToAirports", DistanceValue < obsairports))[1]
p = 282/9999

obshobbies <- slf_obsmeans_rarefied %>% filter(DistanceType == "DistToHobbies") %>% pull(MeanDistance) 
dim(Simulations_rare_long_mean %>% filter(DistanceType == "DistToHobbies", DistanceValue < obshobbies))[1]
p = 34/9999

obsmail <- slf_obsmeans_rarefied %>% filter(DistanceType == "DistToMail") %>% pull(MeanDistance) 
dim(Simulations_rare_long_mean %>% filter(DistanceType == "DistToMail", DistanceValue < obsmail))[1]
p = 7/9999

obsports <- slf_obsmeans_rarefied %>% filter(DistanceType == "DistToPorts") %>% pull(MedianDistance) 
dim(Simulations_rare_long_median %>% filter(DistanceType == "DistToPorts", DistanceValue < obsports))[1]
p = 9062/9999

obsroad <- slf_obsmeans_rarefied %>% filter(DistanceType == "DistToRoad") %>% pull(MeanDistance) 
dim(Simulations_rare_long_mean %>% filter(DistanceType == "DistToRoad", DistanceValue < obsroad))[1]
p = 6/9999

obswineries <- slf_obsmeans_rarefied %>% filter(DistanceType == "DistToWineries") %>% pull(MeanDistance) 
dim(Simulations_rare_long_mean %>% filter(DistanceType == "DistToWineries", DistanceValue < obswineries))[1]
p = 141/9999

obswood <- slf_obsmeans_rarefied %>% filter(DistanceType == "DistToWood") %>% pull(MeanDistance) 
dim(Simulations_rare_long_mean %>% filter(DistanceType == "DistToWood", DistanceValue < obswood))[1]
p = 3/9999

# all the others are 0!




# MEANS
obsairports <- slf_obsmeans_rarefied %>% filter(DistanceType == "DistToAirports") %>% pull(MedianDistance) 
dim(Simulations_rare_long_median %>% filter(DistanceType == "DistToAirports", DistanceValue < obsairports))[1]
p = 1093/9999

obsboats <- slf_obsmeans_rarefied %>% filter(DistanceType == "DistToBoats") %>% pull(MedianDistance) 
dim(Simulations_rare_long_median %>% filter(DistanceType == "DistToBoats", DistanceValue < obsboats))[1]
p = 0/9999

obsgarages <- slf_obsmeans_rarefied %>% filter(DistanceType == "DistToGarages") %>% pull(MedianDistance) 
dim(Simulations_rare_long_median %>% filter(DistanceType == "DistToGarages", DistanceValue < obsgarages))[1]
p = 7/9999

obshobbies <- slf_obsmeans_rarefied %>% filter(DistanceType == "DistToHobbies") %>% pull(MedianDistance) 
dim(Simulations_rare_long_median %>% filter(DistanceType == "DistToHobbies", DistanceValue < obshobbies))[1]
p = 6/9999

obsmail <- slf_obsmeans_rarefied %>% filter(DistanceType == "DistToMail") %>% pull(MedianDistance) 
dim(Simulations_rare_long_median %>% filter(DistanceType == "DistToMail", DistanceValue < obsmail))[1]
p = 5/9999

obsports <- slf_obsmeans_rarefied %>% filter(DistanceType == "DistToPorts") %>% pull(MedianDistance) 
dim(Simulations_rare_long_median %>% filter(DistanceType == "DistToPorts", DistanceValue < obsports))[1]
p = 9062/9999

obsroad <- slf_obsmeans_rarefied %>% filter(DistanceType == "DistToRoad") %>% pull(MedianDistance) 
dim(Simulations_rare_long_median %>% filter(DistanceType == "DistToRoad", DistanceValue < obsroad))[1]
p = 9/9999

obswineries <- slf_obsmeans_rarefied %>% filter(DistanceType == "DistToWineries") %>% pull(MedianDistance) 
dim(Simulations_rare_long_median %>% filter(DistanceType == "DistToWineries", DistanceValue < obswineries))[1]
p = 629/9999

obswood <- slf_obsmeans_rarefied %>% filter(DistanceType == "DistToWood") %>% pull(MedianDistance) 
dim(Simulations_rare_long_median %>% filter(DistanceType == "DistToWood", DistanceValue < obswood))[1]
p = 9/9999


```



# 7. Calculate effect sizes

# Observed means and medians
```{r load observed data full}

# Convert the observed data to long format, get the average values
slfPA_uptodate <- read.csv(file.path(here(), "exported-data", "slfPA_uptodate.csv"))
dim(slfPA_uptodate)
head(slfPA_uptodate)
slfPA_uptodate %<>% select(-DistToOthers)


# Also load data for roads and railroads
slf_uptodate <- read.csv(file.path(here(), "exported-data", "slf_obs_uptodate.csv"))
dim(slf_uptodate)
head(slf_uptodate)


# Keep only the coordinates that are in Pennsylvania
slf_uptodate <- st_as_sf(x = slf_uptodate, coords = c("longitude_rounded", "latitude_rounded"), crs = "EPSG:4269", remove = F) %>% 
  st_transform(crs = "ESRI:102010")
slf_uptodate <- st_intersection(slf_uptodate, Pennsylvania)
dim(slf_uptodate)
head(slf_uptodate)
slf_uptodate %<>% select(latitude_rounded, longitude_rounded, DistToRail, DistToRoad, Category_rare, Category_full)
st_geometry(slf_uptodate) <- NULL


# Convert to long format
slfPA_uptodate_long <- slfPA_uptodate %>%
  pivot_longer(cols = starts_with("DistTo"), names_to = "DistanceType", values_to = "DistanceValue")
slf_uptodate_long <- slf_uptodate %>%
  pivot_longer(cols = starts_with("DistTo"), names_to = "DistanceType", values_to = "DistanceValue")

# Calculate mean and median for full dataset
slf_obsmeans_full <- slfPA_uptodate_long %>%
  filter(Category_full == "Jumpers") %>% 
  group_by(Category_full, DistanceType) %>% 
  summarise(MeanDistance = mean(DistanceValue),
            MedianDistance = median(DistanceValue))

slftrans_obsmeans_full <- slf_uptodate_long %>%
  filter(Category_full == "Jumpers") %>% 
  group_by(Category_full, DistanceType) %>% 
  summarise(MeanDistance = mean(DistanceValue),
            MedianDistance = median(DistanceValue))

slf_obsmeans_full <- rbind(slf_obsmeans_full, slftrans_obsmeans_full)


# Calculate mean and median for rarefied dataset
slf_obsmeans_rarefied <- slfPA_uptodate_long %>%
  filter(Category_rare == "Jumpers") %>% 
  group_by(Category_rare, DistanceType) %>% 
  summarise(MeanDistance = mean(DistanceValue),
            MedianDistance = median(DistanceValue))

slftrans_obsmeans_rarefied <- slf_uptodate_long %>%
  filter(Category_rare == "Jumpers") %>% 
  group_by(Category_rare, DistanceType) %>% 
  summarise(MeanDistance = mean(DistanceValue),
            MedianDistance = median(DistanceValue))

slf_obsmeans_rarefied <- rbind(slf_obsmeans_rarefied, slftrans_obsmeans_rarefied)
```


# Simulated means and medians
## Full dataset

Select mean and median for full dataset
```{r select means and median for full dataset"}

# Load the simulated values
Simulations_full <- read.csv(file.path(here(), "exported-data", "slf_simdatasets_full_highrisk.csv"))
names(Simulations_full)
head(Simulations_full)

# Convert simulations to long format
Simulations_full_long <- Simulations_full %>% pivot_longer(cols = starts_with("DistTo"), names_to = "DistanceType", values_to = "DistanceValue")

# Select only the means
Simulations_full_long_mean <- Simulations_full_long %>%
  filter(grepl("_mean", DistanceType))
Simulations_full_long_mean$DistanceType <- gsub(x = Simulations_full_long_mean$DistanceType, pattern = "_mean", replacement = "")
Simulations_full_long_mean %<>% mutate(DistanceType = recode(DistanceType, 
                                      "DistToAirport" = "DistToAirports",
                                      "DistToPort" = "DistToPorts",
                                      "DistToPeople" = "DistToHobbies"))


# Select only the medians
Simulations_full_long_median <- Simulations_full_long %>%
  filter(grepl("_median", DistanceType))
Simulations_full_long_median$DistanceType <- gsub(x = Simulations_full_long_median$DistanceType, pattern = "_median", replacement = "")
Simulations_full_long_median %<>% mutate(DistanceType = recode(DistanceType, 
                                      "DistToAirport" = "DistToAirports",
                                      "DistToPort" = "DistToPorts",
                                      "DistToPeople" = "DistToHobbies"))


# Summarise simulated data
summary_sim_full_means <- Simulations_full_long_mean %>% 
  group_by(DistanceType) %>% 
  summarise(mean_sim = mean(DistanceValue), sd_mean = sd(DistanceValue))

summary_sim_full_medians <- Simulations_full_long_median %>% 
  group_by(DistanceType) %>% 
  summarise(median_sim = median(DistanceValue), sd_median = sd(DistanceValue))
```


Calculate effect size
```{r effect sizes full dataset}

slf_obsmeans_full #mean and median for observed data
summary_sim_full_means #mean for simulated data
summary_sim_full_medians #median for simulated data

# merge all datasets
summary_full <- merge(summary_sim_full_means, summary_sim_full_medians, by = "DistanceType") %>% 
  merge(slf_obsmeans_full, by = "DistanceType")


# calculate effect sizes
summary_full %<>% rename(mean_obs = MeanDistance,
                         median_obs = MedianDistance) %>% 
  mutate(effect_size_mean = (mean_sim - mean_obs)/sd_mean,
         effect_size_median = (median_sim - median_obs)/sd_median) %>% 
  arrange(effect_size_mean)
write.csv(summary_full, file.path(here(), "exported-data", "summary_simulations_full.csv"))

summary_full %<>% mutate(DistanceType = recode(DistanceType, 
                                              "DistToMail" = "Mail carriers",
                                              "DistToRail" = "Railroads",
                                              "DistToGarages" = "Garages",
                                              "DistToHobbies" = "Popular destinations",
                                              "DistToAirports" = "Airports",
                                              "DistToWood" = "Wood-related activities",
                                              "DistToRoad" = "Major roads",
                                              "DistToBoats" = "Boat launches and marinas",
                                              "DistToWineries" = "Wineries",
                                              "DistToPorts" = "Ports"))


write.csv(summary_full, file.path(here(), "exported-data", "effect_sizes_highrisk_full.csv"), row.names = F)
```


Visualize results
```{r plot}

effectsize_full <- ggplot(summary_full) + 
  geom_bar(aes(y = effect_size_mean, x = reorder(DistanceType, -effect_size_mean)), stat = "identity", fill = "pink") +
  geom_bar(aes(y = effect_size_median, x = reorder(DistanceType, -effect_size_mean)), stat = "identity", alpha = 0.5, fill = "lightblue") +
  ylab("Effect size") + xlab("Type of destination") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

ggsave(file.path(here(), "figures", "jump_highrisk", "effect_size_fullds.jpg"), effectsize_full, height = 5, width = 6)
```



## Rarefied dataset

Select mean and median for rare dataset
```{r mean and median for rare dataset}

# Load the simulated values
Simulations_rare <- read.csv(file.path(here(), "exported-data", "slf_simdatasets_rarefied_highrisk.csv"))
names(Simulations_rare)
head(Simulations_rare)

# Convert simulations to long format
Simulations_rare_long <- Simulations_rare %>% pivot_longer(cols = starts_with("DistTo"), names_to = "DistanceType", values_to = "DistanceValue")

# Select only the means
Simulations_rare_long_mean <- Simulations_rare_long %>%
  filter(grepl("_mean", DistanceType))
Simulations_rare_long_mean$DistanceType <- gsub(x = Simulations_rare_long_mean$DistanceType, pattern = "_mean", replacement = "")
Simulations_rare_long_mean %<>% mutate(DistanceType = recode(DistanceType, 
                                      "DistToAirport" = "DistToAirports",
                                      "DistToPort" = "DistToPorts",
                                      "DistToPeople" = "DistToHobbies"))


# Select only the medians
Simulations_rare_long_median <- Simulations_rare_long %>%
  filter(grepl("_median", DistanceType))
Simulations_rare_long_median$DistanceType <- gsub(x = Simulations_rare_long_median$DistanceType, pattern = "_median", replacement = "")
Simulations_rare_long_median %<>% mutate(DistanceType = recode(DistanceType, 
                                      "DistToAirport" = "DistToAirports",
                                      "DistToPort" = "DistToPorts",
                                      "DistToPeople" = "DistToHobbies"))


# Summarise simulated data
summary_sim_rare_means <- Simulations_rare_long_mean %>% 
  group_by(DistanceType) %>% 
  summarise(mean_sim = mean(DistanceValue), sd_mean = sd(DistanceValue))

summary_sim_rare_medians <- Simulations_rare_long_median %>% 
  group_by(DistanceType) %>% 
  summarise(median_sim = median(DistanceValue), sd_median = sd(DistanceValue))
```

Calculate effect size
```{r effect sizes rare dataset}

slf_obsmeans_rarefied #mean and median for observed data
summary_sim_rare_means #mean for simulated data
summary_sim_rare_medians #median for simulated data

# merge all datasets
summary_rare <- merge(summary_sim_rare_means, summary_sim_rare_medians, by = "DistanceType") %>% 
  merge(slf_obsmeans_rarefied, by = "DistanceType")


# calculate effect sizes
summary_rare %<>% rename(mean_obs = MeanDistance,
                         median_obs = MedianDistance) %>% 
  mutate(effect_size_mean = (mean_sim - mean_obs)/sd_mean,
         effect_size_median = (median_sim - median_obs)/sd_median) %>% 
  arrange(effect_size_mean)
write.csv(summary_rare, file.path(here(), "exported-data", "summary_simulations_rare.csv"))

summary_rare %<>% mutate(DistanceType = recode(DistanceType, 
                                              "DistToMail" = "Mail carriers",
                                              "DistToRail" = "Railroads",
                                              "DistToGarages" = "Garages",
                                              "DistToHobbies" = "Popular destinations",
                                              "DistToAirports" = "Airports",
                                              "DistToWood" = "Wood-related activities",
                                              "DistToRoad" = "Major roads",
                                              "DistToBoats" = "Boat launches and marinas",
                                              "DistToWineries" = "Wineries",
                                              "DistToPorts" = "Ports"))

write.csv(summary_rare, file.path(here(), "exported-data", "effect_sizes_highrisk_rarefied.csv"), row.names = F)
```


Visualize results
```{r plot}

effectsize_rare <- ggplot(summary_rare) + 
  geom_bar(aes(y = effect_size_mean, x = reorder(DistanceType, -effect_size_mean)), stat = "identity", fill = "pink") +
  geom_bar(aes(y = effect_size_median, x = reorder(DistanceType, -effect_size_mean)), stat = "identity", alpha = 0.5, fill = "lightblue") +
  ylab("Effect size") + xlab("Type of destination") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

ggsave(file.path(here(), "figures", "jump_highrisk", "effect_size_rareds.jpg"), effectsize_rare, height = 5, width = 6)
```


We can visualize the results on Figure 2. The histogram represents the distribution of distances to high-risk areas under the null hypothesis of random dispersal of jumpers. The black vertical lines indicate the significance limits. An observed value situated outside of these vertical lines leads to the rejection of the null hypothesis. The red line indicates the average distance between jumpers and high-risk areas observed in our dataset. The observed location of jumpers is significantly closer than random to high-risk areas.  

In this vignette, we found that:  

(1) Jumpers are situated significantly close to high-risk areas. These high-risk areas could be preferential locations for the progression of the invasion.
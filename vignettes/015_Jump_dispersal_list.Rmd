---
title: "Making the most of invasion records, the case of the spotted lanternfly, part I: isolating jump dispersal and diffusive spread"
author: 
- Nadege Belouard^[Temple University, nadege.belouard@temple.edu]
- Sebastiona De Bona^[Temple University, seba.debona@temple.edu]
- Jocelyn E. Behm^[Temple University, jebehm@temple.edu]
- Matthew R. Helmus^[Temple University, mrhelmus@temple.edu]
date: "1/6/2021"
output:
  pdf_document:
    toc: TRUE
    toc_depth: 2
  html_document:
    toc: TRUE
    toc_depth: 3
params:
  display: FALSE
  run: TRUE
  loadfiles: FALSE
  savefiles: TRUE
editor_options: 
  chunk_output_type: console
---

# Aim and setup

The dispersal of a species can be autonomous or vectored, and in the case of the spotted lanternfly, it is strongly suspected that human transportation dramatically increases the spread of the species. While most dispersal events occur over short distances and likely result in a continuous invasive range, anthropogenic dispersal promotes the occurrence of dispersal "jumps", and the establishment of satellite populations away from the core of the invasion. Distinguishing diffusive spread and jump dispersal is important to understand the process of invasion, its evolution, but also to take efficient management measures.

The spotted lanternfly, *Lycorma delicatula* (hereafter SLF) is an insect from China that is an invasive pest in the US. Since the initial detection of SLF in Berks County, PA, in 2014, large-scale surveys were conducted to trace the progression of the invasion, resulting in a large amount of detection and non-detection data. A unique dataset summarizing SLF presence and absence in the US was constructed using the package `lycordata`, and constitutes an opportunity to study the spread of the SLF. 

The aim of this first vignette is to differentiate diffusive spread from jump dispersal using a simple and conservative method. We calculated the distance between each detection point and the introduction site. We defined a distance that SLF are unlikely to disperse autonomously - here, 15 kilometers. Then, we looked for gaps larger than 15 kilometers in the distribution of the distance to the introduction site. Every detection of SLF found after such a gap was considered to be a jump event, i.e. an event of anthropogenic dispersal potentially leading to the establishment of a new population, if it is situated at least 15 kilometers away from any previous jump. The threshold of the diffusive spread was considered to be the last positive survey before this gap. Considering that the expansion of the invasion is heterogeneous in space, we divided the invasion into 12 disk portions with the introduction site as the origin, to increase the accuracy of the calculations while keeping the analyses reasonably simple. The optimization of the parameters (gap size, number of disk portions) is described in a companion vignette.

For the sake of homogeneity with other analyses presented, only established populations (detection involving more than one individual, as defined in `lycordata`) will be used in analyses. This section is designed for the optimization of the parameters leading to the list of jump dispersal. The final list of jumps is generated, as well as intermediate files that can be used to refine analyses.


```{r setup for rendering, include = F, messages = F, warning = F}
# First we set parameters and install all the necessary packages
knitr::opts_chunk$set(dpi = 300, warning = FALSE, message = FALSE, echo = FALSE)

# attaching necessary packages
library(tidyverse)
library(magrittr)
library(sf)
library(maps)
library(DescTools)
library(reshape2)
library(geosphere)
library(ggplot2)
library(knitr)
library(here)
library(slfjumps)
library(purrr)
library(leaflet)
library(randomcoloR)

sf::sf_use_s2(FALSE)
```

## States map
```{r states names and centroid for global map, message = FALSE, warning = FALSE, echo = params$display}
# extracts a map of the States and recodes state labels to show the two-letter code rather than the full state name.

# obtaining simple feature objects for states and finding centroids for label positioning
states <- sf::st_as_sf(maps::map("state", plot = FALSE, fill = TRUE))
# sf::st_as_sf(maps::map("county", plot = TRUE, fill = FALSE))
states <- cbind(states, st_coordinates(st_centroid(states)))

# making table key for state 2-letter abbreviations
# the vectors state.abb and state.name contains strings of all
# US states and abbreviations
state_abbr <- tibble(state.name = str_to_lower(state.name), state.abb) %>%
  left_join(tibble(ID = states$ID), ., by = c(ID = "state.name")) %>%
  mutate(state.abb = replace_na(state.abb, ""))

# adding 2-letter codes to sf
states$code <- state_abbr$state.abb

ggplot() +
    geom_sf(data = states, alpha = 0) +
  geom_text(data = states, aes(X, Y, label = code), size = 2) 

```

\newpage


```{r packages and data, message = FALSE, warning = FALSE, echo = params$display}
#load the dataset from lycordata
# This is the last dataset without all the 2021 data
load("./data/tinyslf_v6.rda")
slf_tiny <- tinyslf
dim(slf_tiny) # 327347 rows

# Remove one point in Maryland that is not correct
# This point should already be removed in this version of the dataset
# slf_tiny %>% filter((between(longitude, -77, -76) & between(latitude, 38.8, 39.2) & slf_established == T))
# dim(slf_tiny) #327346 rows

#what years are included?
table(slf_tiny$bio_year)
#we only want 2014 to 2020
slf_tiny %<>% filter(bio_year %in% c(2014:2020))
dim(slf_tiny) #324091 rows  
unique(slf_tiny$state)
```


# 1. Data initialization

## Data reshaping

In `lycordata`, each survey appears in a row, and states whether SLF were present (one individual found) and/or established (more than one individual or an egg mass found). Multiple surveys were conducted at the same location during the same year, resulting in a complex and redundant dataset of `r dim(slf_tiny)[1]` rows (surveys).

We reshape the table to summarize the information by rounding the geographical coordinates to cells of 1 km^2 (1 km * 1 km), so that one line represents the detection status at a given location for a given year. The code is borrowed from Seba De Bona's `lycordata` vignette to homogenize our data.

Note: when several surveys indicate that SLF are "present" the same year at the same location, we could be tempted to categorize them in the "established" category. However, the category "present" often refers to dead individuals, although this information is not explicitly available. We use a conservative approach and kept the same categories while summarizing the data.

```{r rounding coordinates, echo = params$display, warnings = FALSE, message = FALSE, eval = params$run}
# specifying the width of the mesh, in km
size_of_grid <- 1

# rounding coordinates and summarizing surveys by location and year
# we round the latitude and longitude to 5 decimal places to avoid problems of memory limitations
#this is ok because the size of the grid is 0.009 (1/111) at the smallest (3 decimal places)
grid_data <- slf_tiny %>%
  mutate(latitude_grid = RoundTo(latitude, multiple = size_of_grid/111),
         longitude_grid = RoundTo(longitude, multiple = size_of_grid/85)) %>%
  mutate(latitude_rounded = round(latitude_grid, 5),
         longitude_rounded = round(longitude_grid, 5)) %>% 
  group_by(bio_year, latitude_rounded, longitude_rounded) %>%
  summarise(slf_present = any(slf_present),
            slf_established = any(slf_established)) %>% 
  ungroup()

knitr::kable(head(grid_data))
dim(grid_data) #58331 rows
```


Let's look at the points on a map
```{r map all points}
# Map surveys
map <- ggplot(data = states) +
    geom_point(data = grid_data,
             aes(x = longitude_rounded, y = latitude_rounded, col = slf_established)) + 
  geom_sf(data = states, alpha = 0) +
  geom_text(data = states, aes(X, Y, label = code), size = 2)

map 

ggsave(file.path(here(), "figures", "jump_list", "1.map_allpoints.jpg"), map,
       height = 5, width = 5)

# Number of positive surveys over all surveys (including negative surveys in CA!)
dim(grid_data %>% filter(slf_established == T))[1] #now 7044
# Percentage of positive surveys
dim(grid_data %>% filter(slf_established == T))[1]/dim(grid_data)[1] #12.08% 
```



The area surveyed is huge, but mostly made of negative surveys in the western USA
we don't need to keep these areas because they are not useful to model anything!
```{r remove far western points}
# What is the area of interest (with established populations)
grid_data %>% filter(slf_established == T) %>% 
  summarise(range(latitude_rounded),
            range(longitude_rounded))
# The westernmost longitude with established populations is -80.6

# We can eliminate points at longitudes west of -90 to eliminate some useless points
grid_data %<>% filter(longitude_rounded > -90)
dim(grid_data) #58144
dim(grid_data %>% filter(slf_established == T))[1] #the number of positive points did not change (7044)

# Map it
map <- ggplot(data = states) +
  geom_point(data = grid_data,
             aes(x = longitude_rounded, y = latitude_rounded, col = slf_established),
             shape = 19) +
  geom_sf(data = states, alpha = 0) +
  geom_text(data = states, aes(X, Y, label = code), size = 2)

map

ggsave(file.path(here(), "figures", "jump_list", "2.map_eastpoints.jpg"),
       map, width = 5, height = 5)


# Map only positive points
map_pos <- ggplot(data = states) +
  geom_point(data = grid_data %>% filter(slf_established == T),
             aes(x = longitude_rounded, y = latitude_rounded),
             shape = 19) +
  geom_sf(data = states, alpha = 0) +
  geom_text(data = states, aes(X, Y, label = code), size = 2) +
  coord_sf(xlim = c(-88, -70), ylim = c(35, 45), expand = FALSE)
  
map_pos

ggsave(file.path(here(), "figures", "jump_list", "3.map_pospoints.jpg"),
       map_pos, width = 5, height = 5)

# Number of positive surveys did not change
dim(grid_data %>% filter(slf_established == T))[1] #now 7044
# Percentage of positive surveys
dim(grid_data %>% filter(slf_established == T))[1]/dim(grid_data)[1] #now 12.11%
```

The table now has `r dim(grid_data)[1]` rows.


## Distances and status calculation

```{r calculate distances to the introduction point, echo = params$display, eval = params$run}

#Coordinates of the introduction site, extracted from Barringer et al. 2015
centroid <- data.frame(longitude_rounded = -75.675340, latitude_rounded = 40.415240)

#Compute distances to the introduction point, in km
grid_data <- grid_data %>% 
  mutate(DistToIntro = distm(grid_data[,c(3,2)], centroid, fun=distGeo)/1000)


# We could create a new column containing a more detailed status (established vs present vs undetected)
# but since we only care about established populations, we can just keep the column slf_established!

# # Compute a single column with the SLF status at each point: undetected, present, or established
# grid_data <- grid_data %>%
#   mutate(Status = NA)
# 
# for (i in 1:length(grid_data$bio_year)) {
#   if (grid_data$slf_established[i]) {
#     grid_data$Status[i] = "Established"
#   } else if (grid_data$slf_present[i]) {
#     grid_data$Status[i] = "Present"
#   } else {
#     grid_data$Status[i] = "Undetected"
#   }
# }
# 
# # Put levels of Status in correct order for plots
# grid_data$Status <- factor(grid_data$Status, levels = c("Undetected", "Present", "Established"))

# Save the grid file = points with their distance to the introduction site
write.csv(grid_data, file.path(here(), "exported-data", "grid_data.csv"), row.names = F)
dim(grid_data)[1]
# The number of rows must be 58,144 with 2014-2020 data


```

We first calculate the distance between each survey point and the introduction point (-75.675340, 40.415240, from Barringer et al., 2015). This distance will be the basis of all subsequent analyses. The summary of this distance is (in kilometers):  
`r summary(grid_data$DistToIntro)`
We also create a variable summarizing the status of the survey for each point: SLF `r levels(grid_data$Status)`.

\newpage






# 2. Differentiating diffusive spread and jump dispersal

A custom program searches for each year the distance at which the gap occurs, and returns both the survey before this threshold (the limit of diffusive spread) and a list of surveys found after this threshold (jump events).   
* Note that here, we consider that populations do not go extinct, so that the limit of the diffusive spread cannot be lower in year y than in year y+1. This is because fewer and fewer surveys are conducted near the introduction site over time, leading to the appearance of a false first gap near the introduction site (see Figure 3, surveys are shifted on the right in 2019 and 2020).  
* If a jump event is identified closer than 10 miles to a jump from the previous year, it is removed from the list, as SLF likely spread from the jump of the previous year.
* The function runs independently for each disk portion, generating false-positive and false-negative (see troubleshooting with disk rotation).

We divide the invasion records into sectors to increase the accuracy of subsequent calculations.


## Parameters optimization

```{r load dataset}
grid_data <- read.csv(file.path(here(), "exported-data", "grid_data.csv"), h=T)
dim(grid_data)
```

Now we need to figure out the best set of parameters to find the accurate number of jumps, i.e. the highest number of jumps detected by the algorithm. To do so, we run several analysis with extreme sets of parameters to get closer to a plateau in the number of jumps that are found, before a finer optimization is done.

The parameter that increases most the number of jumps is the number of sectors, so we begin with this parameter and a fixed (high) number of rotations, and look for a plateau.

```{r first approximation of the number of sectors}

# First iteration:
i = 12
initial_number_rotations = 10
Results_prev = -1
centroid <- c(long = -75.675340, lat = 40.415240)

# Attribute the geographical sector of each point for each rotation
slfdata <- attribute_sectors(grid_data, nb_sectors = i, centroid = centroid, rotation = initial_number_rotations)
slfdata_long <- slfdata %>% 
  pivot_longer(cols = starts_with("rotation"), names_to = "rotation_nb", values_to = "sectors_nb", 
               names_prefix = "rotation", names_transform = list(rotation_nb = as.integer))

# Find the jumps
Results <- find_jumps(slfdata_long, gap_size = 15, bio_year = c(2014:2020))



# Run the loop to find the approximate number of sectors recommended
while (dim(Results$Jump)[1] > Results_prev){
  Results_prev <- dim(Results$Jump)[1]
  j = i
  i = i + 8

  slfdata <- attribute_sectors(grid_data, nb_sectors = i, centroid = centroid, rotation = initial_number_rotations)
  slfdata_long <- slfdata %>% 
    pivot_longer(cols = starts_with("rotation"), names_to = "rotation_nb", values_to = "sectors_nb", 
                 names_prefix = "rotation", names_transform = list(rotation_nb = as.integer)) 
  
  Results <- find_jumps(slfdata_long, gap_size = 15, bio_year = c(2014:2020))
  
  rm(slfdata)
  rm(slfdata_long)
}

print(paste0("The right number of sectors is likely between ", j, " and ", i))

rm(Results)
```

The right number of sectors is close to 20-28.

```{r first approximation of the number of rotations}
# Then, look at the number of rotations needed
# Run this on the highest number of sectors found above (i).
r = 2
Results_prev = -1

slfdata <- attribute_sectors(grid_data, nb_sectors = i, centroid = centroid, rotation = r)
slfdata_long <- slfdata %>% 
  pivot_longer(cols = starts_with("rotation"), names_to = "rotation_nb", values_to = "sectors_nb", 
               names_prefix = "rotation", names_transform = list(rotation_nb = as.integer)) 
Results <- find_jumps(slfdata_long, gap_size = 15, bio_year = c(2014:2020))


while (dim(Results$Jump)[1] > Results_prev){
  Results_prev <- dim(Results$Jump)[1]
  k = r
  r = r + 5

  slfdata <- attribute_sectors(grid_data, nb_sectors = i, centroid = centroid, rotation = r)
  slfdata_long <- slfdata %>% 
  pivot_longer(cols = starts_with("rotation"), names_to = "rotation_nb", 
               values_to = "sectors_nb", names_prefix = "rotation", names_transform = list(rotation_nb = as.integer)) 
  Results <- find_jumps(slfdata_long, gap_size = 15, bio_year = c(2014:2020))
  
  rm(slfdata)
  rm(slfdata_long)

}

print(paste0("The number of rotations is likely between ", k, " and ", r))
```

The number of rotations is close to 2-7.
Use these approximations to run the optimization program!

```{r run optimization}
# Here we are going to run the number of sectors of every multiple of 4 between 20 and 32
# and the number of rotations between 1 and 10

sectors = c(20, 24, 28)
rotations = c(1, 2, 3, 4, 5, 6, 7)
centroid <- c(-75.675340, 40.415240)
  
optim_list <- data.frame(s = NULL,
                         r = NULL,
                         DistToIntro = NULL,
                         bio_year = NULL,
                         latitude_rounded = NULL,
                         longitude_rounded = NULL,
                         slf_present = NULL,
                         slf_established = NULL,
                         # Status = NULL,
                         theta = NULL,
                         DistToSLF = NULL)


for (s in sectors){
  for (r in rotations){
    i = 1
    print(paste0("Sectors: ", s, ", rotations: ", r))
    slfdata <- attribute_sectors(grid_data, nb_sectors = s, centroid = centroid, rotation = r)
    slfdata_long <- slfdata %>% 
      pivot_longer(cols = starts_with("rotation"), names_to = "rotation_nb", 
                   values_to = "sectors_nb", names_prefix = "rotation", 
                   names_transform = list(rotation_nb = as.integer)) 
    Results <- find_jumps(slfdata_long, gap_size = 15, bio_year = c(2014:2020))
    Results$Jump %<>% add_column(r = r, s = s)
    optim_list <- rbind(optim_list, Results$Jump)
    i = i + 1
    
    rm(slfdata)
    rm(slfdata_long)
  }
}

# List of jumps
head(optim_list)
write.csv(optim_list, file.path(here(), "tables", "optim_list.csv"))

# List of the number of jumps found per combination
optim_sum <- optim_list %>% group_by(s, r) %>% summarise(jumps = n())
head(optim_sum)
max(optim_sum$jumps)

write.csv(optim_sum, file.path(here(), "tables", "optim_jumps.csv"))

```


Plot the optimization

```{r plot optimization}

# Plot it
optim_plot <- ggplot(data = optim_sum, 
                     aes(x = as.factor(r), y = jumps)) +
  geom_bar(lwd = .05, stat = "identity") +
  xlab("Number of rotations") +
  ylab("Number of jumps") +
  facet_wrap(~as.factor(s), nrow = 1) +
  theme_classic() +
  ggtitle("Number of sectors") +
  theme(legend.position = "bottom", text = element_text(size = 12), axis.text.x = element_text(size = 5, angle = 90), plot.title = element_text(size = 12, hjust = 0.5))

optim_plot

ggsave(file.path(here(), "figures", "jump_list", "4.optim_plot.jpg"), optim_plot)
```


## Best dataset
The best result (highest number of jumps in the least amount of time) is given by the dataset with XX rotations and XX sectors.
Rerun it to save results.

```{r run best dataset and save results}

# Right combination:
optim_sum %>% filter(s == 20, r == 3)

slfdata <- attribute_sectors(grid_data, nb_sectors = 20, centroid = centroid, rotation = 3)

slfdata_long <- slfdata %>% 
  pivot_longer(cols = starts_with("rotation"), names_to = "rotation_nb", 
               values_to = "sectors_nb", names_prefix = "rotation",
               names_transform = list(rotation_nb = as.integer)) 

Results <- find_jumps(slfdata_long, gap_size = 15, bio_year = c(2014:2020))

dim(Results$Jump)
head(Results$Dist)

write.csv(Results$Dist, file.path(here(), "exported-data", "thresholds.csv"), row.names = F)
write.csv(Results$Jump, file.path(here(), "exported-data", "jumps.csv"), row.names = F)
```


The jump dispersal events found by the function can be visualized on a map. Jump locations are colored according to their year of appearance, among all the established populations in grey. The introduction site is signaled by a blue cross. We note that most jump events occur in northern Virginia or western Pennsylvania. In Winchester (VA), a diffusive spread appears around jump events, indicating that a secondary invasion began in this area. A similar pattern is found around Harrisburg (PA), although the diffusive spread has now reached Harrisburg too.

```{r map all jumps per params set}

# All jumps one set
map_jumps <- ggplot(data = states) +
  geom_point(data = grid_data %>% filter(slf_established == T, bio_year %in% c(2014:2020)),
             aes(x = longitude_rounded, y = latitude_rounded), size = 1, shape = 19, col = "grey") +
  geom_point(data = centroid,
             aes(x = longitude_rounded, y = latitude_rounded), col = "blue", shape = 4, size = 5) +
  geom_point(data = jumps, 
             aes(x = longitude_rounded, y = latitude_rounded, fill = as.factor(bio_year)), shape = 21, size = 2) +
  scale_fill_manual(values = c("#009E73", "#0072B2", "firebrick3", "gold2", "black")) +
  geom_text(data = states,
            aes(X, Y, label = code), size = 4) +
  labs(x = "Longitude", y = "Latitude")+
  geom_sf(data = states, alpha = 0) + 
  coord_sf(xlim = c(-82, -72), ylim = c(38, 43), expand = FALSE) + 
  theme(legend.position="top") +
  guides(fill = guide_legend("Year"))

map_jumps

ggsave(file.path(here(), "figures", "jump_list", "map_jumps.jpg"), map_jumps , width = 8, height = 8)


```



\newpage


# 3. Rarefy outbreaks

```{r load datasets}

jumps <- read.csv(file.path(here(), "exported-data", "jumps.csv"), h=T)
grid_data <- read.csv(file.path(here(), "exported-data", "grid_data.csv"), h=T)
centroid <- data.frame(longitude_rounded = -75.675340, latitude_rounded = 40.415240)

```


Find points with important outbreaks

```{r run group_jumps function to define groups}

# The function needs to run separately for each set of parameters since the list of jumps is not the same!
Jump_groups <- group_jumps(jumps, gap_size = 15)

#Check how many points there are per group
Jump_groups %>% group_by(bio_year, Group) %>% summarise(Nb = n()) %>% arrange(-Nb) %>% filter(Nb > 1)


#Check on a map if it worked!
# Zoom on groups to see if groups make sense
pal <- colorFactor(rep(rainbow(5), 8), 
                   domain = unique(Jump_groups$Group))

leaflet(data = Jump_groups) %>% 
      addTiles() %>% 
      addCircleMarkers(lng = ~longitude_rounded, 
                       lat = ~latitude_rounded, 
                       color = ~pal(Group), 
                       label = ~Group)  

write.csv(Jump_groups, file.path(here(), "exported-data", "jump_groups.csv"), row.names = F)
```


Most jump events occurred in Harrisburg, PA, and Winchester, VA, in 2018. They might be true independent jumps, i.e. SLF hitchhiked multiple times to these locations the same year. Alternatively, they might be the result of SLF quickly spreading from a single jump event. Finally, they can be a mix between these two hypotheses. For the rest of the analyses, we will test the two most contrasted hypotheses in parallel in order to test whether results vary. For the first hypothesis (all points are independent introductions), the dataset consists of all jump points. For the second hypothesis (only one introduction in Harrisburg and Winchester), the dataset consists of each "grouped jumps" summarized each by their most central point.


```{r run rarify_groups function on all datasets}
Jumps_unique <- rarefy_groups(Jump_groups) %>% add_column(Rarefied = TRUE)
dim(Jumps_unique)

# Map it
map_rarefied <- ggplot(data = states) +
  geom_sf(data = states, fill = "white") +
  coord_sf(xlim = c(-82, -72), ylim = c(38, 43), expand = FALSE) +
  geom_point(data = Jump_groups,
             aes(x = longitude_rounded, y = latitude_rounded, col = as.factor(Group)), shape = 19, size = 3, show.legend = F) +
  geom_point(data = Jumps_unique, aes(x = longitude_rounded, y = latitude_rounded)) +
  labs(x = "Longitude", y = "Latitude")+
  theme(legend.position="right")

map_rarefied

ggsave(file.path(here(), "figures", "jump_list", "6.map_rarefication.jpg"),
       map_rarefied, width = 5, height = 5)
```


## Assemble datasets
Assemble the datasets to have only one big dataset with all jumpers: different sets of parameters, full, and reduced.

```{r assemble all datasets into one}

Jumps_unique %<>% select("longitude_rounded", "latitude_rounded", "bio_year", "Rarefied")

Jumps_full_rarefied <- merge(jumps, Jumps_unique, by = c("latitude_rounded", "longitude_rounded", "bio_year"), all = T)

dim(jumps)[1] == dim(Jumps_full_rarefied)[1]
dim(Jumps_full_rarefied %>% filter(Rarefied == TRUE))[1] == dim(Jumps_unique)[1]

write.csv(Jumps_full_rarefied, file.path(here(), "exported-data", "jumps_full_rarefied.csv"), row.names = F)
```


# 4. Analysis

```{r load datasets for analysis}
Jumps <- read.csv(file.path(here(), "exported-data", "jumps_full_rarefied.csv"))
Jumps %<>% mutate(Rarefied = ifelse(is.na(Rarefied), "Full", "Rarefied"))
dim(Jumps)
Jumps$Rarefied <- factor(Jumps$Rarefied, levels = c( "Full", "Rarefied"))

Thresholds <- read.csv(file.path(here(), "./exported-data/thresholds.csv"))
grid_data <- read.csv(file.path(here(), "exported-data", "grid_data.csv"), h=T)
centroid <- data.frame(longitude_rounded = -75.675340, latitude_rounded = 40.415240)
jump_groups <- read.csv(file.path(here(), "exported-data", "jump_groups.csv"))
```


## Map of jumps

```{r jumps map}

map_rarefied <- ggplot(data = grid_data) +
  geom_sf(data = states, fill = "white") +
  geom_point(data = grid_data %>% filter(slf_established == T), 
             aes(x = longitude_rounded, y = latitude_rounded), col = "lightgrey") +
  geom_sf(data = states, alpha = 0) +
  geom_point(data = centroid, aes(x = longitude_rounded, y = latitude_rounded), col = "black", shape = 4, size = 5) +
  geom_point(data = Jumps,
             aes(x = longitude_rounded, y = latitude_rounded, 
                 col = as.factor(bio_year), stroke = Rarefied, group = Rarefied, shape = Rarefied), size = 4) +
  
  scale_discrete_manual(aesthetics = "stroke", values = c('Rarefied' = 1, 'Full' = 2)) +
  scale_discrete_manual(aesthetics = "shape", values = c('Rarefied' = 19, 'Full' = 21)) +
  scale_color_manual(values = c("gold2", "firebrick3", "#0072B2", "#009E73")) +
  scale_fill_manual(values = c("gold2", "firebrick3", "#0072B2", "#009E73")) +
  scale_alpha_manual(values = c(0.5, 1)) +
  
  coord_sf(xlim = c(-81, -73), ylim = c(38, 43)) +
  labs(x = "Longitude", y = "Latitude") +
  theme(legend.position="right", text = element_text(size = 10),
        panel.background = element_rect(fill = "white"),
        legend.key = element_rect(fill = "white")) +
   guides(colour = guide_legend("Biological year"), alpha = guide_legend("Dataset"), fill = guide_legend("Biological year"))

map_rarefied

ggsave(file.path(here(), "figures", "jump_list", "7.map_jumpsv2.jpg"), map_rarefied, height = 8, width = 8)
```


## Number of jumps per year

```{r number of jumps per year}

# Bar plot of the number of jumps per year
Jumps_year <- Jumps %>% group_by(bio_year, Rarefied) %>% summarise(n = n())
 
jumps_plot <- ggplot() +
  geom_bar(data = Jumps_year, 
           aes(x = bio_year, y = n, fill = as.factor(bio_year), group = Rarefied, col = as.factor(bio_year), 
               alpha = Rarefied), stat = "identity", lwd = .25) +
  scale_fill_manual(values = c("gold2", "firebrick3", "#0072B2", "#009E73")) +
  # scale_alpha_manual(values = c(0, 1)) +
  scale_color_manual(values = c("gold2", "firebrick3", "#0072B2", "#009E73")) +
  xlab("Biological year") +
  ylab("Number of jumps") +
  theme_classic() +
  guides(alpha = "none", fill = "none", col = "none") +
  theme(text = element_text(size = 10), legend.position = "top")

jumps_plot

ggsave(file.path(here(), "figures", "jump_list", "8.jumps_number.jpg"), jumps_plot, height = 2.5, width = 4)
```


## Mean distance to the invasion front per year

```{r mean dist to threshold per year}

Thresholds %<>% filter(rotation_nb == 1) %>% 
  select(longitude_rounded, latitude_rounded, bio_year, sectors_nb) %>% 
  rename(sectors = sectors_nb,
         latitude_threshold = latitude_rounded,
         longitude_threshold = longitude_rounded)

# Attribute sectors to each jump: find sector for rotation 0
Jumps_sectors <- attribute_sectors(Jumps, nb_sectors = 20, centroid = c(-75.675340, 40.415240), rotation = 1) 
Jumps_sectors %<>% select(-c(theta.1, sectors)) %>% rename(sectors = rotation1)

# Find corresponding threshold per bio year 
JumpLength <- left_join(Jumps_sectors, Thresholds)
JumpLength %<>% rowwise() %>%  
  mutate(DistToThreshold = as.vector(distm(c(longitude_rounded, latitude_rounded), 
                                           c(longitude_threshold, latitude_threshold), fun = distGeo))/1000)

write.csv(JumpLength, file.path(here(), "exported-data", "DistToThreshold.csv"))

JumpLength_tot <- rbind(JumpLength %>% filter(Rarefied == "Rarefied"),
                        JumpLength %>% mutate(Rarefied = "Full"))
dim(JumpLength_tot)[1] == 37+135



# Jitter
JumpLength_tot$Rarefied <- factor(JumpLength_tot$Rarefied, levels = c("Rarefied", "Full"))

MeanDist_jump <- ggplot() +
  geom_jitter(data = JumpLength_tot,
             aes(x = as.factor(bio_year), y = DistToThreshold, 
                 col = as.factor(bio_year),
                 fill = as.factor(bio_year),
                 alpha = Rarefied), size = 4) +
  scale_alpha_manual(values = c(1,0.4)) +
  scale_discrete_manual(aesthetics = "shape", values = c('Rarefied' = 19, 'Full' = 21)) +
  scale_color_manual(values = c("gold2", "firebrick3", "#0072B2", "#009E73")) +
  scale_fill_manual(values = c("gold2", "firebrick3", "#0072B2", "#009E73")) +
  labs(x = "Dataset", y = "Distance to the invasion front (km)") +
  theme_classic() +
  theme(legend.position="top", text = element_text(size = 10), plot.tag.position = c(0.01, 1)) +
  guides(fill = "none", shape = "none", col = "none", alpha = "none")
MeanDist_jump

ggsave(file.path(here(), "figures", "jump_list", "9.jumps_number_jitter.jpg"), MeanDist_jump, height = 2.5, width = 4)


# Boxplot
JumpLength_tot$Rarefied <- factor(JumpLength_tot$Rarefied, levels = c("Rarefied", "Full"))
MeanDist_jump <- ggplot() +
  geom_boxplot(data = JumpLength_tot,
             aes(x = as.factor(bio_year), y = DistToThreshold, 
                 col = as.factor(bio_year),
                 fill = as.factor(bio_year),
                 alpha = Rarefied)) +
  scale_alpha_manual(values = c(0.7,0)) +
  scale_discrete_manual(aesthetics = "shape", values = c('Rarefied' = 19, 'Full' = 21)) +
  scale_color_manual(values = c("gold2", "firebrick3", "#0072B2", "#009E73")) +
  scale_fill_manual(values = c("gold2", "firebrick3", "#0072B2", "#009E73")) +
  labs(x = "Dataset", y = "Distance to the invasion front (km)") +
  theme_classic() +
  theme(legend.position="top", text = element_text(size = 10), plot.tag.position = c(0.01, 1)) + 
  guides(fill = "none", shape = "none", col = "none", alpha = "none")
MeanDist_jump


ggsave(file.path(here(), "figures", "jump_list", "10.jumps_number_boxplot.jpg"), MeanDist_jump, height = 2.5, width = 4)

# Summary
JumpLength_tot %>% group_by(Rarefied, bio_year) %>% summarise(mean(DistToThreshold))
summary(aov(DistToThreshold ~ bio_year, data = JumpLength)) #p = 0.37

summary(aov(DistToThreshold ~ bio_year, data = JumpLength %>% filter(Rarefied == "Rarefied"))) # p = 0.34

```



## Evolution of the radius of diffusive spread and jumps over time

We can now look at how the radius of the invasion increases over time, when differentiating diffusive spread and jump dispersal (Figure 6). In the westernmost disk portions, jump dispersal is responsible for the very high increase in the invasion radius. In the other disk portions, the spread seems to be mostly linked to diffusive dispersal. 

```{r figure total spread/diffusive spread for slf established, fig.cap = "Evolution of the radius of the invasion over time, when diffusive spread and jump dispersal are separated", echo=FALSE, message = FALSE}

#Data on total spread
Thresholds %<>% filter(rotation_nb == 1)


data <- grid_data %>% 
  filter(slf_established == T & bio_year %in% c(2014:2020)) 
threshold_data <- attribute_sectors(data, nb_sectors = 20, centroid = c(long = -75.675340, lat = 40.415240), rotation = 4)

threshold_data %<>% group_by(bio_year, rotation1) %>% 
  summarise(MaxDist = max(DistToIntro)) %>% 
  rename(sectors_nb = rotation1)


spread <- ggplot() +
    geom_bar(data = threshold_data, aes(x = bio_year, y = MaxDist), stat="identity", fill = "white", col = "black") +
  geom_bar(data = Thresholds, aes(x = bio_year , y = DistToIntro), fill = "grey", col = "black", stat = "identity") +
  ylab("Invasion radius (km)")+
  xlab("Year") +
    facet_wrap(~sectors_nb) +
  theme_classic()

spread

ggsave(file.path(here(), "figures", "jump_list", "11.spread_radius.jpg"), spread, height = 10, width = 8)
```


## Map of outbreaks

```{r jumps map}
hist(jump_groups %>% count(Group) %>% filter(n > 1) %>% pull(n), breaks = 100)

outbreaks <- jump_groups %>% count(Group) %>% filter(n > 1)

pal <- colorFactor(rep(rainbow(5), 8), 
                   domain = unique(jump_groups$Group))

leaflet(data = jump_groups) %>% 
      addTiles() %>% 
      addCircleMarkers(lng = ~longitude_rounded, 
                       lat = ~latitude_rounded, 
                       color = ~pal(Group), 
                       label = ~Group)  

```

Outbreaks always seem to be situated at places with BOTH roads and railroads (or railyards).

\newpage

# 5. Conclusion

The spread of the spotted lanternfly in the US is likely due both to diffusive spread and human-assisted jump dispersal. 75 jump occurrences have been identified, and most of them are situated in Winchester (north VA) and western Pennsylvania (especially Harrisburg). 

Jump events are likely be caused by SLF hitchhiking on human transports, and establishing near transport infrastructures: railroads, roads, and airports. In the next vignette, we will test the significance of the proximity between jump events and transport infrastructures by a comparison with a random distribution. We will also compare these distances to those of diffusers (SLF spread through diffusive spread) and of points where SLF were not detected, to check for a potential bias in survey locations.
---
title: "Making the most of invasion records, the case of the spotted lanternfly, part I: isolating jump dispersal and diffusive spread"
author: 
- Nadege Belouard^[Temple University, nadege.belouard@temple.edu]
- Sebastiona De Bona^[Temple University, seba.debona@temple.edu]
- Jocelyn E. Behm^[Temple University, jebehm@temple.edu]
- Matthew R. Helmus^[Temple University, mrhelmus@temple.edu]
date: "1/6/2021"
output:
  pdf_document:
    toc: TRUE
    toc_depth: 2
  html_document:
    toc: TRUE
    toc_depth: 3
params:
  display: FALSE
  run: TRUE
  loadfiles: FALSE
  savefiles: TRUE
editor_options: 
  chunk_output_type: console
---

# Aim and setup

The dispersal of a species can be autonomous or vectored, and in the case of the spotted lanternfly, it is strongly suspected that human transportation dramatically increases the spread of the species. While most dispersal events occur over short distances and likely result in a continuous invasive range, anthropogenic dispersal promotes the occurrence of dispersal "jumps", and the establishment of satellite populations away from the core of the invasion. Distinguishing diffusive spread and jump dispersal is important to understand the process of invasion, its evolution, but also to take efficient management measures.

The spotted lanternfly, *Lycorma delicatula* (hereafter SLF) is an insect from China that is an invasive pest in the US. Since the initial detection of SLF in Berks County, PA, in 2014, large-scale surveys were conducted to trace the progression of the invasion, resulting in a large amount of detection and non-detection data. A unique dataset summarizing SLF presence and absence in the US was constructed using the package `lycordata`, and constitutes an opportunity to study the spread of the SLF. 

The aim of this first vignette is to differentiate diffusive spread from jump dispersal using a simple and conservative method. We calculated the distance between each detection point and the introduction site. We defined a distance that SLF are unlikely to disperse autonomously - here, 15 kilometers. Then, we looked for gaps larger than 15 kilometers in the distribution of the distance to the introduction site. Every detection of SLF found after such a gap was considered to be a jump event, i.e. an event of anthropogenic dispersal potentially leading to the establishment of a new population, if it is situated at least 15 kilometers away from any previous jump. The threshold of the diffusive spread was considered to be the last positive survey before this gap. Considering that the expansion of the invasion is heterogeneous in space, we divided the invasion into 12 disk portions with the introduction site as the origin, to increase the accuracy of the calculations while keeping the analyses reasonably simple. The optimization of the parameters (gap size, number of disk portions) is described in a companion vignette.

For the sake of homogeneity with other analyses presented, only established populations (detection involving more than one individual, as defined in `lycordata`) will be used in analyses. This section is designed for the optimization of the parameters leading to the list of jump dispersal. The final list of jumps is generated, as well as intermediate files that can be used to refine analyses.


```{r setup for rendering, include = F, messages = F, warning = F}
# First we set parameters and install all the necessary packages
knitr::opts_chunk$set(dpi = 300, warning = FALSE, message = FALSE, echo = FALSE)

# attaching necessary packages
library(tidyverse)
library(magrittr)
library(sf)
library(spData)
library(maps)
library(DescTools)
library(ggmap)
library(reshape2)
library(geosphere)
library(ggplot2)
library(gridExtra)
library(lycordata)
library(knitr)
library(here)

sf::sf_use_s2(FALSE)
```


```{r states names and centroid for global map, message = FALSE, warning = FALSE, echo = params$display}
# extracts a map of the States and recodes state labels to show the two-letter code rather than the full state name.

# obtaining simple feature objects for states and finding centroids for label positioning
states <- sf::st_as_sf(maps::map("state", plot = FALSE, fill = TRUE))
# sf::st_as_sf(maps::map("county", plot = TRUE, fill = FALSE))
states <- cbind(states, st_coordinates(st_centroid(states)))

# making table key for state 2-letter abbreviations
# the vectors state.abb and state.name contains strings of all
# US states and abbreviations
state_abbr <- tibble(state.name = str_to_lower(state.name), state.abb) %>%
  left_join(tibble(ID = states$ID), ., by = c(ID = "state.name")) %>%
  mutate(state.abb = replace_na(state.abb, ""))

# adding 2-letter codes to sf
states$code <- state_abbr$state.abb

ggplot() +
    geom_sf(data = states, alpha = 0) +
  geom_text(data = states, aes(X, Y, label = code), size = 2) 

```

\newpage


```{r packages and data, message = FALSE, warning = FALSE, echo = params$display}
#load the dataset from lycordata
load("../packages/lycordata-main/exported_data/tinyslf.rda")
slf_tiny <- tinyslf
dim(slf_tiny) #327347 rows
head(slf_tiny)

# Remove one point in Maryland that is not correct
slf_tiny %<>% filter(!(between(longitude, -77, -76) & between(latitude, 38.8, 39.2) & slf_established == T))
dim(slf_tiny) #327346 rows

#what years are included?
unique(slf_tiny$bio_year)
#we only want 2014 to 2020
slf_tiny %<>% filter(bio_year %in% c(2014:2020))
dim(slf_tiny) #324090 rows

```


# 1. Data initialization

## Data reshaping

In `lycordata`, each survey appears in a row, and states whether SLF were present (one individual found) and/or established (more than one individual or an egg mass found). Multiple surveys were conducted at the same location during the same year, resulting in a complex and redundant dataset of `r dim(slf_tiny)[1]` rows (surveys).

We reshape the table to summarize the information by rounding the geographical coordinates to cells of 1 km^2 (1 km * 1 km), so that one line represents the detection status at a given location for a given year. The code is borrowed from Seba De Bona's `lycordata` vignette to homogenize our data.

Note: when several surveys indicate that SLF are "present" the same year at the same location, we could be tempted to categorize them in the "established" category. However, the category "present" often refers to dead individuals, although this information is not explicitly available. We use a conservative approach and kept the same categories while summarizing the data.

```{r rounding coordinates, echo = params$display, warnings = FALSE, message = FALSE, eval = params$run}
# specifying the width of the mesh, in km
size_of_grid <- 1

# rounding coordinates and summarizing surveys by location and year
# we round the latitude and longitude to 5 decimal places to avoid problems of memory limitations
#this is ok because the size of the grid is 0.009 (1/111) at the smallest (3 decimal places)
grid_data <- slf_tiny %>%
  mutate(latitude_grid = RoundTo(latitude, multiple = size_of_grid/111),
         longitude_grid = RoundTo(longitude, multiple = size_of_grid/85)) %>%
  mutate(latitude_rounded = round(latitude_grid, 5),
         longitude_rounded = round(longitude_grid, 5)) %>% 
  group_by(bio_year, latitude_rounded, longitude_rounded) %>%
  summarise(slf_present = any(slf_present),
            slf_established = any(slf_established)) %>% 
  ungroup()

knitr::kable(head(grid_data))
dim(grid_data) #58330
```


Let's look at the points on a map
```{r map all points}
# Map surveys
ggplot(data = states) +
  geom_point(data = grid_data,
             aes(x = longitude_rounded, y = latitude_rounded, col = slf_established),
             shape = 19) +
  geom_sf(data = states, alpha = 0) +
  geom_text(data = states, aes(X, Y, label = code), size = 2)

# Number of positive surveys over all surveys (including negative surveys in CA!)
dim(grid_data %>% filter(slf_established == T))[1] #7049
# Percentage of positive surveys
dim(grid_data %>% filter(slf_established == T))[1]/dim(grid_data)[1] #12.08%




# The area surveyed is huge, but mostly made of negative surveys in the western USA
# we don't need to keep these areas because they are not useful to model anything!
# What is the area of interest (with established populations)
grid_data %>% filter(slf_established == T) %>% 
  summarise(range(latitude_rounded),
            range(longitude_rounded))
# The westernmost longitude with established populations is -80.6
# We can eliminate points at longitudes west of -90 to eliminate some useless points
grid_data %<>% filter(longitude_rounded > -90)
dim(grid_data) #58143
dim(grid_data %>% filter(slf_established == T))[1] #the number of positive points did not change (7049)

# Map it
ggplot(data = states) +
  geom_point(data = grid_data,
             aes(x = longitude_rounded, y = latitude_rounded, col = slf_established),
             shape = 19) +
  geom_sf(data = states, alpha = 0) +
  geom_text(data = states, aes(X, Y, label = code), size = 2)


# Number of positive surveys did not change
dim(grid_data %>% filter(slf_established == T))[1] #7049
# Percentage of positive surveys
dim(grid_data %>% filter(slf_established == T))[1]/dim(grid_data)[1] #12.12%
```

The table now has `r dim(grid_data)[1]` rows.


## Distances and status calculation

```{r calculate distances to the introduction point, echo = params$display, eval = params$run}

#Coordinates of the introduction site, extracted from Barringer et al. 2015
centroid <- c(-75.675340, 40.415240)

#Compute distances to the introduction point, in km
grid_data <- grid_data %>% 
  mutate(DistToIntro = distm(grid_data[,c(3,2)], centroid, fun=distGeo)/1000)

# Compute a single column with the SLF status at each point: undetected, present, or established
grid_data <- grid_data %>%
  mutate(Status = NA)

for (i in 1:length(grid_data$bio_year)) {
  if (grid_data$slf_established[i]) {
    grid_data$Status[i] = "Established"
  } else if (grid_data$slf_present[i]) {
    grid_data$Status[i] = "Present"
  } else {
    grid_data$Status[i] = "Undetected"
  }
}

# Put levels of Status in correct order for plots
grid_data$Status <- factor(grid_data$Status, levels = c("Undetected", "Present", "Established"))

# Save the grid file = points with their distance to the introduction site
write.csv(grid_data, file.path(here(), "exported-data", "grid_data.csv"), row.names = F)
dim(grid_data)[1]
# The number of rows must be 58,143 with 2014-2020 data


```

We first calculate the distance between each survey point and the introduction point (-75.675340, 40.415240, from Barringer et al., 2015). This distance will be the basis of all subsequent analyses. The summary of this distance is (in kilometers):  
`r summary(grid_data$DistToIntro)`
We also create a variable summarizing the status of the survey for each point: SLF `r levels(grid_data$Status)`.


## Space division

We divide the invasion records into sectors to increase the accuracy of subsequent calculations.

```{r function: distinguish sectors, echo = TRUE, warnings = FALSE, eval = params$run}

# Determine sectors based on trigonometry
# A full circle is 2*pi
# The sector of each point is determined by atan2(y,x), the angle of the point relative to the horizontal line of the introduction site

attribute_sectors <- function(dataset, nb_sectors = 8, centroid = c(-75.675340, 40.415240), rotation = 1) {
  
  # Calculate theta, the angle between the point and the horizontal line, for each point
  x = NULL
  y = NULL
  # dataset$theta <- NA
  # grid_data <- dataset
  grid_data <- dataset %>% add_column(theta = NA,
                                      sectors = nb_sectors)

  for (i in 1:length(grid_data$longitude_rounded)) {
    x = grid_data$longitude_rounded[i] - centroid[1]
    y = grid_data$latitude_rounded[i] - centroid[2]
    grid_data$theta[i] = base::atan2(y,x) + pi
  }

  # Create a variables for the angle
  angle_sector = 2*pi/nb_sectors
  p = angle_sector/rotation
  # Attribute the right disk portion number
  for (i in 0:(rotation-1)){
    grid_data <- grid_data %>% mutate(thetanew = theta - p*i,
                                      thetanew = ifelse(thetanew < 0, thetanew + 2*pi, thetanew),
                                      sector = ceiling((thetanew)/angle_sector)) %>%
      dplyr::select(-thetanew) %>% 
      rename(!!quo_name(paste("rotation", i+1, sep = "")) := sector)
  }
  
  return(grid_data)
}

```




\newpage






# 3. Differentiating diffusive spread and jump dispersal

## Exploration of histograms of distances to the introduction site (DEPRECATED)

Let's have a look at the distribution of the distances of positive vs negative surveys to the introduction point (Figure 3).

```{r histogram of spread distances per year, fig.cap = "Distribution of the distance between SLF populations and the introduction site, per disk portion and per year", fig.height = 20, fig.width = 20, warning = F, echo = params$display}

# histogram_distances <- ggplot(data = grid_data_rotate0 %>% filter(bio_year %in% c(2015:2020), Status != "Present"), aes(x = DistToIntro)) + 
#   geom_histogram(aes(x = DistToIntro, fill = Status),
#                  breaks = seq(0,500,10)) +
#   facet_wrap(~portion_name + bio_year, ncol = 6) +
#   scale_fill_manual(values = c("grey", "red")) +
#   xlab("Distance from the introduction site (km)") +
#   ylab("Number of survey locations") +
#   theme(plot.title = element_text(hjust = 0.5, size = 12), legend.position = "top")
# 
# histogram_distances
```



```{save map disk portions, eval = params$savefiles, echo = params$display}
# ggsave("../figures/vignette_quadrants/histogram_distances.jpg", histogram_distances, width = 20, height = 20)
```

The fact that surveys with SLF undetected are always recorded further than detection events indicates that we can be be fairly confident that the spread of the SLF is accurately monitored. It is also the case in 2020 in the NNE and ESE portions. We can also see that the distribution of established populations is sometimes discontinuous, with gaps where populations were not detected. Detections that appear after the first gap are likely the result of jump dispersal, i.e. human-vectored transportation of SLF in new locations (secondary introductions). 

We can further understand the yearly spread of the SLF by distinguishing diffusive spread (the continuous progress of the invasion) and jump dispersal (long-distance, human-vectored dispersal). We calculated, for each year, the limit of diffusive spread by finding the first 10-mile gap in the distribution of the distances to the introduction site. 



## Function differentiating diffusive spread and jump dispersal

A custom program searches for each year the distance at which the gap occurs, and returns both the survey before this threshold (the limit of diffusive spread) and a list of surveys found after this threshold (jump events).   
* Note that here, we consider that populations do not go extinct, so that the limit of the diffusive spread cannot be lower in year y than in year y+1. This is because fewer and fewer surveys are conducted near the introduction site over time, leading to the appearance of a false first gap near the introduction site (see Figure 3, surveys are shifted on the right in 2019 and 2020).  
* If a jump event is identified closer than 10 miles to a jump from the previous year, it is removed from the list, as SLF likely spread from the jump of the previous year.
* The function runs independently for each disk portion, generating false-positive and false-negative (see troubleshooting with disk rotation).


```{r program that runs this function for each quadrant and year, echo = TRUE}

threshold_jump_multiple_num <- function(dataset, 
                                    bio_year = c(2014:2021),
                                    gap_size = 15) {
  
  ##############################################################################################
  ## PHASE 1: IDENTIFY THE THREHOLDS OF DIFFUSIVE SPREAD, AND GET A LIST OF POTENTIAL JUMPS  ###
  ##############################################################################################
  #Initialize variables for the results
  Dist = NULL
  Jumps_alls = NULL
  Jumps_allr = NULL
  rotation = unique(sort(dataset$rotation_nb)) #Look for the number of rotations in the dataset
  sector = unique(sort(dataset$sectors_nb)) #Look for the number of portions in the dataset
  
  for (rot in rotation){
    dataset_rot <- dataset %>% filter(rotation_nb == rot) #Create a dataset for this rotation
    
    for (s in sector){
      dataset_n = NULL
      jumpers_sector = data.frame(DistToIntro = 0)
      for (y in bio_year){
        
        #Select the dataset. We assume that no population is going extinct over the years and cumulate datasets.
        dataset_n <- rbind(dataset_n, dataset_rot %>% filter(sectors_nb == s & bio_year == y & Status == "Established"))
        if (dim(dataset_n)[1] == 0){ #If there is no point in the sector up to that year, go to the next sector
          next
        }
   
        # Initialize values
        i = 1
        distancei = 1
        j = 2
        distancej = 2
      
        # Order the variable by increasing order
        distance_sorted <- sort(dataset_n$DistToIntro) 
    
        # Loop until it finds the threshold or until the variable is finished
        while ( (distancei + gap_size > distancej) & (j <= length(distance_sorted)) ) { 
          distancei = distance_sorted[i]
          distancej = distance_sorted[j]
          i = i+1
          j = j+1
          }
    
        if (distancei + gap_size > distancej) { # there is no jump
          threshold = distance_sorted[i]
          } else { #a jump was found, take the previous iteration
          threshold = distance_sorted[i-1]
          # print(paste0("Jump in ", rot, ", portion ", p, " and year ", y, " after distance ", distancei))
          # We make sure that the gap is not due to an absence of surveys!
          dataset_total <- dataset_rot %>% filter(sectors_nb == s & bio_year %in% c(2014:y) & between(DistToIntro, threshold, threshold+15))
          dim(dataset_total)[1]
          if (dim(dataset_total)[1] == 0) { 
            print(paste0("Error: there is no survey in the gap in rotation ", rot, ", sector ", s, " and year ", y, " after distance ", threshold)) 
            }
          }
        
        
        
        #Find the threshold survey in the initial table (that is not ordered)
        rowNumber = which(grepl(threshold, dataset_n$DistToIntro))
        if (length(rowNumber > 1)) { rowNumber = tail(rowNumber, n = 1)} #If the threshold has been the same for several years, take the last year
    
        #Store results in objects
        threshold_survey = dataset_n[rowNumber,]
        
        # Make sure the threshold is associated to the correct year, even if the threshold is the same as the year before
        threshold_survey$bio_year <- y
        jump_survey = dataset_n %>% filter(DistToIntro > threshold & bio_year == y)
        
         # Add results at each iteration
        Dist = rbind(Dist, threshold_survey)
        jumpers_sector = dplyr::bind_rows(jumpers_sector, jump_survey)
        }
      Jumps_alls = rbind(Jumps_alls, jumpers_sector[-1,])
    }
    
    Jumps_allr = rbind(Jumps_allr, Jumps_alls)
  } 
  
  # Reduce the jump list and the dataset to unique points (without repetitions due to rotations)
  Jumps_all = Jumps_allr %>% dplyr::select(-c(sectors_nb, rotation_nb, sectors)) %>% unique()
  dataset_unique = dataset %>% dplyr::select(-c(sectors_nb, rotation_nb, sectors)) %>% unique()
  
  
  
  ###########################################################################
  ## PHASE 2: KEEP ONLY JUMPS AT LEAST <gap size> AWAY FROM OTHER POINTS  ###
  ###########################################################################
  # Are surveys in the list of jump surveys real new jumps or just diffusion of secondary introductions from the previous year?
  # i.e. are new jumpers less than 10 miles from a previous jump?
  # we run that second part on the whole dataset, not within disk portions, to avoid the occurrence of false-positive)

  dataset_nprev = NULL
  Jumps = NULL

  for (y in bio_year[1:length(bio_year)]){
    jumps_year <- Jumps_all %>% dplyr::filter(bio_year == y)  #select jumps for this year
    dataset_all <- dataset_unique %>% dplyr::filter(bio_year %in% c(bio_year[1]:y) & Status == "Established") #all points up to this year
    doubles <- bind_rows(jumps_year, dataset_all) #aggregate the datasets
    dataset_diffusers <- doubles %>% group_by_all() %>% filter(n() == 1) #remove duplicate points = keep only diffusers
  
    jumps_year <- jumps_year %>% add_column(DistToSLF = NA) #Create a column for the dist to the nearest other point
    
    if (dim(dataset_all)[1] == 0 | dim(jumps_year)[1] == 0){
      next
    } else {
      # Create shapefiles with the two sets of points
      dataset_diffusers_layer <- st_as_sf(x = dataset_diffusers, coords = c("longitude_rounded", "latitude_rounded"), crs = 4326, remove = F)
      dataset_diffusers_proj <- st_transform(dataset_diffusers_layer, crs = 4326)
      jumps_year_layer <- st_as_sf(x = jumps_year, coords = c("longitude_rounded", "latitude_rounded"), crs = 4326, remove = F)
      jumps_year_proj <- st_transform(jumps_year_layer, crs = 4326)
  
      #Calculate their pairwise distances
      for (jump in 1:length(jumps_year_proj$DistToSLF)){
        pairwise_dist <- st_distance(x = jumps_year_proj[jump,], y = dataset_diffusers_proj)
        jumps_year_proj$DistToSLF[jump] <- min(pairwise_dist)
    }
  
      #Select those at least 10 miles away from the others
      st_geometry(jumps_year_proj) <- NULL
      not_a_jump = jumps_year_proj %>% filter(DistToSLF < gap_size*1000) 
      newjumpers = jumps_year_proj %>% filter(DistToSLF > gap_size*1000)
    }
    
    
    
  #################################################################
  ## PHASE 3: REITERATE PHASE 2 WITH POINTS DISCARDED AS JUMPS  ###
  #################################################################
    # Last precaution: re-iterate the analysis with points that were finally not jumps
    # until the dataset stabilises to a list of real jumps away from any other point
      
    if (dim(newjumpers)[1] != 0){ #if jumpers are identified this year, let's check if they are true
      
      while (dim(not_a_jump)[1] != 0){ # until we don't deny any more jumper
        # Create shapefiles with the two sets of points
        notajump_layer <- st_as_sf(x = not_a_jump, coords = c("longitude_rounded", "latitude_rounded"), crs = 4326, remove = F)
        notajump_proj <- st_transform(notajump_layer, crs = 4326)
        newjumpers_layer <- st_as_sf(x = newjumpers, coords = c("longitude_rounded", "latitude_rounded"), crs = 4326, remove = F)
        newjumpers_proj <- st_transform(newjumpers_layer, crs = 4326)
    
          #Calculate their pairwise distances
        for (jump in 1:length(newjumpers_proj$DistToSLF)){
          pairwise_dist <- st_distance(x = newjumpers_proj[jump,], y = notajump_proj)
          newjumpers_proj$DistToSLF[jump] <- min(pairwise_dist)
        }
  
        #Select those at least 10 miles away from the others
        st_geometry(newjumpers_proj) <- NULL
        not_a_jump = newjumpers_proj %>% filter(DistToSLF < gap_size*1000)
        newjumpers = newjumpers_proj %>% filter(DistToSLF > gap_size*1000)
      } 
    }
    
    Jumps = bind_rows(Jumps, newjumpers) #add the final list of jumpers for each year
  }

  results <- list("Dist" = Dist, "Jump" = Jumps)
  
  return(results)
} 

``` 


## Run the program (with one particular set of parameters)

```{r run the program, echo = params$display, eval = params$run}

grid_data <- read.csv(file.path(here(), "exported-data", "grid_data2021.csv"), h=T)
centroid <- c(long = -75.675340, lat = 40.415240)

# Test if we still get the right number of jumps with the parameters from the last dataset
test <- grid_data %>% filter(bio_year %in% c(2014:2020))

# Attribute sectors
slfdata <- attribute_sectors(grid_data, nb_sectors = 20, centroid = centroid, rotation = 4)
dim(grid_data)

# Convert table to long format
slfdata_long <- slfdata %>% 
  pivot_longer(cols = starts_with("rotation"), names_to = "rotation_nb", values_to = "sectors_nb", 
               names_prefix = "rotation", names_transform = list(rotation_nb = as.integer)) 

# Find jumps
Results <- threshold_jump_multiple_num(slfdata_long, gap_size = 15, bio_year = c(2014:2020))
dim(Results$Jump)



ggplot(data = states) +
  geom_point(data = grid_data %>% filter(Status == "Established", bio_year %in% c(2014:2020)),
             aes(x = longitude_rounded, y = latitude_rounded), size = 1, shape = 19, col = "grey") +
  geom_point(data = grid_data,
             aes(x = centroid[1], y = centroid[2]), col = "blue", shape = 4, size = 5) +
   geom_point(data = Results$Jump, 
             aes(x = longitude_rounded, y = latitude_rounded, fill = as.factor(bio_year)), shape = 21, size = 2) +
  geom_text(data = states,
            aes(X, Y, label = code), size = 4) +
  labs(x = "Longitude", y = "Latitude")+
  geom_sf(data = states, alpha = 0) + 
  coord_sf(xlim = c(-82, -72), ylim = c(38, 43)) + 
  theme(legend.position="top")


```


## Parameters approximation

Now we need to figure out the best set of parameters to find the accurate number of jumps, i.e. the highest number of jumps detected by the algorithm. To do so, we run several analysis with extreme sets of parameters to get closer to a plateau in the number of jumps that are found, before a finer optimization is done.

```{r individual loops for sectors and rotations}

# The parameter that increases most the number of jumps is the number of sectors, so we begin with this parameter and a fixed (high) number of rotations, and look for a plateau.
grid_data <- read.csv(file.path(here(), "exported-data", "grid_data2021.csv"), h=T)

# First iteration:
i = 20
initial_number_rotations = 10
Results_prev = c(-1)
centroid <- c(long = -75.675340, lat = 40.415240)

slfdata <- attribute_sectors(grid_data, nb_sectors = i, centroid = centroid, rotation = initial_number_rotations)
slfdata_long <- slfdata %>% 
  pivot_longer(cols = starts_with("rotation"), names_to = "rotation_nb", values_to = "sectors_nb", 
               names_prefix = "rotation", names_transform = list(rotation_nb = as.integer)) 


# Run the loop to find the number of sectors recommended
while (dim(Results$Jump)[1] > Results_prev){
  Results_prev <- dim(Results$Jump)[1]
  j = i
  i = i + 4

  slfdata <- attribute_sectors(grid_data, nb_sectors = i, centroid = centroid, rotation = initial_number_rotations)
  slfdata_long <- slfdata %>% 
    pivot_longer(cols = starts_with("rotation"), names_to = "rotation_nb", values_to = "sectors_nb", 
                 names_prefix = "rotation", names_transform = list(rotation_nb = as.integer)) 
  
  Results <- threshold_jump_multiple_num(slfdata_long, gap_size = 15, bio_year = c(2014:2021))
}

nb_sectors_recommended = j



# Then, look at the number of rotations needed
r = 2
Results_prev = c(-1)

slfdata <- attribute_sectors(grid_data, nb_sectors = 20, centroid = centroid, rotation = r)
slfdata_long <- slfdata %>% 
  pivot_longer(cols = starts_with("rotation"), names_to = "rotation_nb", values_to = "sectors_nb", 
               names_prefix = "rotation", names_transform = list(rotation_nb = as.integer)) 
Results <- threshold_jump_multiple_num(slfdata_long, gap_size = 15, bio_year = c(2014:2021))


while (dim(Results$Jump)[1] > Results_prev){
  Results_prev <- dim(Results$Jump)[1]
  k = r
  r = r + 5

  slfdata <- attribute_sectors(grid_data, nb_sectors = i, centroid = centroid, rotation = r)
  slfdata_long <- slfdata %>% 
  pivot_longer(cols = starts_with("rotation"), names_to = "rotation_nb", 
               values_to = "sectors_nb", names_prefix = "rotation", names_transform = list(rotation_nb = as.integer)) 
Results <- threshold_jump_multiple_num(slfdata_long, gap_size = 15, bio_year = c(2014:2021))
}

nb_rotations_recommended = k


# Confirm the number of sectors with this number of rotations
if (initial_number_rotations < nb_rotations_recommended){
  print("Check the number of sectors again!")
} else {
    print(paste0("You're good to go with ", nb_sectors_recommended, " sectors and ", nb_rotations_recommended, " rotations"))
}



# Run the program
nb_sectors_recommended = 20
nb_rotations_recommended = 7
slfdata <- attribute_sectors(grid_data, 
                             nb_sectors = nb_sectors_recommended, 
                             centroid = centroid, 
                             rotation = nb_rotations_recommended)
slfdata_long <- slfdata %>% 
  pivot_longer(cols = starts_with("rotation"), names_to = "rotation_nb", 
               values_to = "sectors_nb", names_prefix = "rotation", 
               names_transform = list(rotation_nb = as.integer)) 
Results <- threshold_jump_multiple_num(slfdata_long, gap_size = 15, bio_year = c(2014:2021))
dim(Results$Jump)

table(Results$Jump$bio_year)
# write.csv(Results$Dist, "./exported-data/jumps_thresholds.csv", row.names = F)
```


## Optimization
```{r create figure of optimization}

sectors = c(20)
rotations = c(7)
centroid <- c(-75.675340, 40.415240)
optim <- data.frame(sectors = 0,
          rotations = 0,
          jumps = 0)
  
optim_list <- data.frame(s = NULL,
                         r = NULL,
                         DistToIntro = NULL,
                         bio_year = NULL,
                         latitude_rounded = NULL,
                         longitude_rounded = NULL,
                         slf_present = NULL,
                         slf_established = NULL,
                         Status = NULL,
                         theta = NULL,
                         DistToSLF = NULL)


dim(grid_data %>% filter(Status == "Established", bio_year %in% c(2014:2020)))


for (s in sectors){
  for (r in rotations){
    i = 1
    print(paste0("Sectors: ", s, ", rotations: ", r))
    slfdata <- attribute_sectors(grid_data, nb_sectors = s, centroid = centroid, rotation = r)
    slfdata_long <- slfdata %>% 
      pivot_longer(cols = starts_with("rotation"), names_to = "rotation_nb", 
                   values_to = "sectors_nb", names_prefix = "rotation", 
                   names_transform = list(rotation_nb = as.integer)) 
    Results <- threshold_jump_multiple_num(slfdata_long, gap_size = 15, bio_year = c(2014:2020))
    optim[i,] <- c(s, r, dim(Results$Jump)[1])
    Results$Jump %<>% add_column(r = r, s = s)
    optim_list <- rbind(optim_list, Results$Jump)
    i = i + 1
  }
}


# Number of jumps per set of parameters
write.csv(optim, file.path(here(), "exported-data", "jumps_optimization2021.csv"), row.names = F)
# optim <- read.csv("./exported-data/jumps_optimization2021.csv")

# List of jumps per set of parameters
write.csv(optim_list, file.path(here(), "exported-data", "jumps_optimization_list2021.csv"), row.names = F)
# optim_list <- read.csv("./exported-data/jumps_optimization_list2021.csv")
sum(optim$jumps) == dim(optim_list)[1]



optim
optim_sum <- optim_list %>% group_by(s, r) %>% summarise(jumps = n())

# Plot it
optim_plot <- ggplot(data = optim_sum, 
                     aes(x = as.factor(r), y = jumps)) +
  geom_bar(lwd = .05, stat = "identity") +
  scale_fill_manual(values = c("#009E73", "#0072B2", "firebrick3", "gold2", "black")) +
  # scale_alpha_manual(values = c(0.5, 1)) +
  xlab("Number of rotations") +
  ylab("Number of jumps") +
  facet_wrap(~as.factor(s), nrow = 1) +
  # guides(fill = guide_legend("Biological year", reverse = T), alpha = guide_legend("Dataset")) +
  theme_classic() +
  ggtitle("Number of sectors") +
  theme(legend.position = "bottom", text = element_text(size = 12), axis.text.x = element_text(size = 5, angle = 90), plot.title = element_text(size = 12, hjust = 0.5))

optim_plot





# Run attribute_groups and rarefy on this
rarefied_list <- data.frame(bio_year = NULL,
                          latitude_rounded = NULL,
                          longitude_rounded = NULL,
                          s = NULL,
                          r = NULL,
                          Rarefied = NULL)
sectors = c(4,8,12,16,20,24,28,40,60,80,100)
rotations = c(1,2,3,4,5,10,15,20)

for (sec in sectors){
  for (rot in rotations){
    print(paste0("Sectors: ", sec, ", rotations: ", rot))
    dataset <- optim_list %>% filter(s == sec, r == rot)
    Jump_groups <- group_jumps(dataset, gap_size = 15)
    Jumps_unique <- rarefy_groups(Jump_groups) %>% add_column(Rarefied = TRUE)
    Jumps_unique %<>% select("latitude_rounded", "longitude_rounded", "bio_year", "s", "r", "Rarefied")
    rarefied_list <- rbind(rarefied_list, Jumps_unique)
  }
}


rarefied_list %>% group_by(s, r) %>% summarise(jumps = n())

full_list <- merge(optim_list, rarefied_list, by = c("latitude_rounded", "longitude_rounded", "bio_year", "r", "s"),
                             all = T)
full_list %<>% mutate(Rarefied = ifelse(is.na(Rarefied), "Full", "Rarefied"))

full_list$Rarefied <- factor(full_list$Rarefied, levels = c( "Full", "Rarefied"))

Optim_year <- full_list %>% group_by(sectors = s, rotations = r, bio_year = bio_year, Rarefied = Rarefied) %>% summarise(jumps = n())



#2nd version with colors
optim_plot <- ggplot(data = Optim_year, 
                     aes(x = as.factor(rotations), y = jumps, 
                         fill = forcats::fct_rev(as.factor(bio_year)),
                         #alpha = Rarefied)
                         ) +
  geom_bar(lwd = .05, stat = "identity") +
  scale_fill_manual(values = c("#009E73", "#0072B2", "firebrick3", "gold2", "black")) +
  scale_alpha_manual(values = c(0.5, 1)) +
  xlab("Number of rotations") +
  ylab("Number of jumps") +
  facet_wrap(~as.factor(sectors), nrow = 1) +
  guides(fill = guide_legend("Biological year", reverse = T), alpha = guide_legend("Dataset")) +
  theme_classic() +
  ggtitle("Number of sectors") +
  theme(legend.position = "bottom", text = element_text(size = 12), axis.text.x = element_text(size = 5, angle = 90), plot.title = element_text(size = 12, hjust = 0.5))

optim_plot

ggsave("./figures/vignette_quadrants/optim_plot3.jpg", optim_plot, height = 6, width = 12)


## Count how long it takes for min number of sectors x rotations
# system.time({slfdata <- attribute_sectors(grid_data, nb_sectors = 20, centroid = centroid, rotation = 5)
#   slfdata_long <- slfdata %>%
#   pivot_longer(cols = starts_with("rotation"), names_to = "rotation_nb", values_to = "sectors_nb", names_prefix = "rotation", names_transform = list(rotation_nb = as.integer))
#   Results <- threshold_jump_multiple_num(slfdata_long, gap_size = 15, bio_year = c(2014:2020))})
# # 38.77 sec
# dim(Results$Jump)[1] #139
# 
# # Compare to higher number of sectors but no rotation
# system.time({slfdata <- attribute_sectors(grid_data, nb_sectors = 80, centroid = centroid, rotation = 1)
#   slfdata_long <- slfdata %>%
#   pivot_longer(cols = starts_with("rotation"), names_to = "rotation_nb", values_to = "sectors_nb", names_prefix = "rotation", names_transform = list(rotation_nb = as.integer))
#   Results <- threshold_jump_multiple_num(slfdata_long, gap_size = 15, bio_year = c(2014:2020))})
# # 72 sec
# dim(Results$Jump)[1] #139
```

\newpage


# 4. Results 


## Threshold locations found per year (with rotations)

Make a single object for all thresholds and save it.

```{r bind threshold datasets}

write.csv(Results$Dist, "./exported-data/thresholds.csv", row.names = F)
```

Maps of thresholds for each parameter sets.

```{r map of threshold points per year per rotation, fig.cap = "Map of SLF jumps", fig.height=6, fig.width=10}
centroid <- c(-75.675340, 40.415240)


map_thresholds <- ggplot(data = states) +
  geom_point(data = grid_data %>% filter(Status == "Established"),
             aes(x = longitude_rounded, y = latitude_rounded), size = 1, shape = 19, col = "grey") +
  geom_point(data = grid_data,
             aes(x = centroid[1], y = centroid[2]), col = "blue", shape = 4, size = 5) +
   geom_point(data = Results$Dist, 
             aes(x = longitude_rounded, y = latitude_rounded), col = "black", shape = 21, size = 2) +
  facet_wrap(~bio_year, ncol = 4) +
  geom_text(data = states,
            aes(X, Y, label = code), size = 4) +
  labs(x = "Longitude", y = "Latitude")+
  geom_sf(data = states, alpha = 0) + 
coord_sf(xlim = c(-82, -72), ylim = c(38, 43), expand = FALSE) +
  theme(legend.position="top")

ggsave("./figures/vignette_quadrants/map_thresholds.jpg", map_thresholds, width = 15, height = 10)



map_thresholds2016 <- ggplot(data = states) +
  geom_point(data = grid_data %>% filter(Status == "Established", bio_year %in% c(2014:2016)),
             aes(x = longitude_rounded, y = latitude_rounded), size = 1, shape = 19, col = "grey") +
  geom_point(data = grid_data,
             aes(x = centroid[1], y = centroid[2]), col = "blue", shape = 4, size = 5) +
   geom_point(data = Results$Dist %>% filter(bio_year == 2016), 
             aes(x = longitude_rounded, y = latitude_rounded), col = "black", shape = 21, size = 2) +
  # facet_wrap(~params, ncol = 4) +
  geom_text(data = states,
            aes(X, Y, label = code), size = 4) +
  labs(x = "Longitude", y = "Latitude")+
  geom_sf(data = states, alpha = 0) + 
  coord_sf(xlim = c(-77, -75), ylim = c(40, 41), expand = FALSE) + 
  theme(legend.position="top")

ggsave("../figures/vignette_quadrants/map_thresholds_2016.jpg", map_thresholds2016 , width = 15, height = 10)


map_thresholds2017 <- ggplot(data = states) +
  geom_point(data = grid_data %>% filter(Status == "Established", bio_year %in% c(2014:2017)),
             aes(x = longitude_rounded, y = latitude_rounded), size = 1, shape = 19, col = "grey") +
  geom_point(data = grid_data,
             aes(x = centroid[1], y = centroid[2]), col = "blue", shape = 4, size = 5) +
   geom_point(data = Results$Dist %>%  filter(bio_year == 2017), 
             aes(x = longitude_rounded, y = latitude_rounded), col = "black", shape = 21, size = 2) +
  # facet_wrap(~params, ncol = 4) +
  geom_text(data = states,
            aes(X, Y, label = code), size = 4) +
  labs(x = "Longitude", y = "Latitude")+
  geom_sf(data = states, alpha = 0) + 
  coord_sf(xlim = c(-77, -74), ylim = c(39.5, 41), expand = FALSE) +  
  theme(legend.position="top")

ggsave("../figures/vignette_quadrants/map_thresholds_2017.jpg", map_thresholds2017 , width = 15, height = 10)


map_thresholds2018 <- ggplot(data = states) +
  geom_point(data = grid_data %>% filter(Status == "Established", bio_year %in% c(2014:2018)),
             aes(x = longitude_rounded, y = latitude_rounded), size = 1, shape = 19, col = "grey") +
  geom_point(data = grid_data,
             aes(x = centroid[1], y = centroid[2]), col = "blue", shape = 4, size = 5) +
   geom_point(data = Results$Dist %>%  filter(bio_year == 2018), 
             aes(x = longitude_rounded, y = latitude_rounded), col = "black", shape = 21, size = 2) +
  facet_wrap(~params, ncol = 4) +
  geom_text(data = states,
            aes(X, Y, label = code), size = 4) +
  labs(x = "Longitude", y = "Latitude")+
  geom_sf(data = states, alpha = 0) + 
  coord_sf(xlim = c(-79, -74), ylim = c(39, 42), expand = FALSE) +
  theme(legend.position="top")

ggsave("../figures/vignette_quadrants/map_thresholds_2018.jpg", map_thresholds2018 , width = 15, height = 10)



map_thresholds2019 <- ggplot(data = states) +
  geom_point(data = grid_data %>% filter(Status == "Established", bio_year %in% c(2014:2019)),
             aes(x = longitude_rounded, y = latitude_rounded), size = 1, shape = 19, col = "grey") +
  geom_point(data = grid_data,
             aes(x = centroid[1], y = centroid[2]), col = "blue", shape = 4, size = 5) +
   geom_point(data = Results$Dist %>%  filter(bio_year == 2019), 
             aes(x = longitude_rounded, y = latitude_rounded), col = "black", shape = 21, size = 2) +
  facet_wrap(~params, ncol = 4) +
  geom_text(data = states,
            aes(X, Y, label = code), size = 4) +
  labs(x = "Longitude", y = "Latitude")+
  geom_sf(data = states, alpha = 0) + 
  coord_sf(xlim = c(-81, -73), ylim = c(39, 42), expand = FALSE) +
  theme(legend.position="top")

ggsave("../figures/vignette_quadrants/map_thresholds_2019.jpg", map_thresholds2019 , width = 15, height = 10)



map_thresholds2020 <- ggplot(data = states) +
  geom_point(data = grid_data %>% filter(Status == "Established", bio_year %in% c(2014:2020)),
             aes(x = longitude_rounded, y = latitude_rounded), size = 1, shape = 19, col = "grey") +
  geom_point(data = grid_data,
             aes(x = centroid[1], y = centroid[2]), col = "blue", shape = 4, size = 5) +
   geom_point(data = Results$Dist %>%  filter(bio_year == 2020), 
             aes(x = longitude_rounded, y = latitude_rounded), col = "black", shape = 21, size = 2) +
  facet_wrap(~params, ncol = 4) +
  geom_text(data = states,
            aes(X, Y, label = code), size = 4) +
  labs(x = "Longitude", y = "Latitude")+
  geom_sf(data = states, alpha = 0) + 
  coord_sf(xlim = c(-82, -72), ylim = c(38, 43), expand = FALSE) + 
  theme(legend.position="top")

ggsave("../figures/vignette_quadrants/map_thresholds_2020.jpg", map_thresholds2020 , width = 15, height = 10)

```

The gap size of 16 km is discarded because Harrisburg is not considered a jump. We keep gap sizes of 15 and 10 km.
DP16GS10 is discarded - aberrant thresholds are found near the introduction site due to a lack of data.
DP8GS10 is discarded - aberrant threhsolds are found far away from the invasion front
We keep DP8GS15, DP12GS10 and DP12GS15, DP16GS15.

## Jump locations

The 82 jump events found by the function can be visualized on a map (Figure 4). Jump locations are colored according to their year of appearance, among all the established populations in grey. The introduction site is signaled by a blue cross. We note that most jump events occur in northern Virginia or western Pennsylvania. In Winchester (VA), a diffusive spread appears around jump events, indicating that a secondary invasion began in this area. A similar pattern is found around Harrisburg (PA), although the diffusive spread has now reached Harrisburg too.

Make a single object for all jumps and save it.

```{r save file with jumps for all rotations and datasets}

write.csv(Results$Jump, "./exported-data/jumps.csv", row.names = F)
jumps <- read.csv("./exported-data/jumps.csv")
```

Maps of jumps for each parameter sets.

```{r map of jumps per params per year, fig.cap = "Map of SLF jumps", fig.height=6, fig.width=10}

map_jumps2016 <- ggplot(data = states) +
  geom_point(data = Jumps %>%  filter(bio_year %in% c(2014:2015)),
             aes(x = longitude_rounded, y = latitude_rounded), col = "yellow", shape = 19, size = 2) +
  geom_point(data = grid_data %>% filter(Status == "Established", bio_year %in% c(2014:2016)),
             aes(x = longitude_rounded, y = latitude_rounded), size = 1, shape = 19, col = "grey") +
  geom_point(data = grid_data,
             aes(x = centroid[1], y = centroid[2]), col = "blue", shape = 4, size = 5) +
   geom_point(data = Thresholds %>%  filter(bio_year == 2016), 
             aes(x = longitude_rounded, y = latitude_rounded), col = "black", shape = 21, size = 2) +
  geom_point(data = Jumps %>%  filter(bio_year == 2016), 
             aes(x = longitude_rounded, y = latitude_rounded), fill = "blue", shape = 21, size = 2) +
  facet_wrap(~params, ncol = 4) +
  geom_text(data = states,
            aes(X, Y, label = code), size = 4) +
  labs(x = "Longitude", y = "Latitude")+
  geom_sf(data = states, alpha = 0) + 
  coord_sf(xlim = c(-77, -75), ylim = c(40, 41), expand = FALSE) + 
  theme(legend.position="top")

ggsave("../figures/vignette_quadrants/map_jumps_2016.jpg", map_jumps2016 , width = 15, height = 10)


map_jumps2017 <- ggplot(data = states) +
  geom_point(data = Jumps %>%  filter(bio_year %in% c(2014:2016)),
             aes(x = longitude_rounded, y = latitude_rounded), col = "yellow", shape = 19, size = 2) +
  geom_point(data = grid_data %>% filter(Status == "Established", bio_year %in% c(2014:2017)),
             aes(x = longitude_rounded, y = latitude_rounded), size = 1, shape = 19, col = "grey") +
  geom_point(data = grid_data %>%  filter(bio_year == 2017),
             aes(x = centroid[1], y = centroid[2]), col = "blue", shape = 4, size = 5) +
   geom_point(data = Thresholds %>%  filter(bio_year == 2017), 
             aes(x = longitude_rounded, y = latitude_rounded), col = "black", shape = 21, size = 2) +
  geom_point(data = Jumps %>% filter(bio_year == 2017), 
             aes(x = longitude_rounded, y = latitude_rounded), fill = "blue", shape = 21, size = 2) +
  facet_wrap(~params, ncol = 4) +
  geom_text(data = states,
            aes(X, Y, label = code), size = 4) +
  labs(x = "Longitude", y = "Latitude")+
  geom_sf(data = states, alpha = 0) + 
  coord_sf(xlim = c(-77, -74), ylim = c(39.5, 41), expand = FALSE) + 
  theme(legend.position="top")

ggsave("../figures/vignette_quadrants/map_jumps_2017.jpg", map_jumps2017 , width = 15, height = 10)


map_jumps2018 <- ggplot(data = states) +
  geom_point(data = Jumps %>%  filter(bio_year %in% c(2014:2017)),
             aes(x = longitude_rounded, y = latitude_rounded), col = "yellow", shape = 19, size = 2) +
  geom_point(data = grid_data %>% filter(Status == "Established", bio_year %in% c(2014:2018)),
             aes(x = longitude_rounded, y = latitude_rounded), size = 1, shape = 19, col = "grey") +
  geom_point(data = grid_data,
             aes(x = centroid[1], y = centroid[2]), col = "blue", shape = 4, size = 5) +
   geom_point(data = Thresholds %>%  filter(bio_year == 2018), 
             aes(x = longitude_rounded, y = latitude_rounded), col = "black", shape = 21, size = 2) +
  geom_point(data = Jumps %>%  filter(bio_year == 2018), 
             aes(x = longitude_rounded, y = latitude_rounded), fill = "blue", shape = 21, size = 2) +
  facet_wrap(~params, ncol = 4) +
  geom_text(data = states,
            aes(X, Y, label = code), size = 4) +
  labs(x = "Longitude", y = "Latitude")+
  geom_sf(data = states, alpha = 0) + 
  coord_sf(xlim = c(-79, -74), ylim = c(39, 42), expand = FALSE) + 
  theme(legend.position="top")

ggsave("../figures/vignette_quadrants/map_jumps_2018.jpg", map_jumps2018 , width = 15, height = 10)



map_jumps2019 <- ggplot(data = states) +
  geom_point(data = Jumps %>%  filter(bio_year %in% c(2014:2018)),
             aes(x = longitude_rounded, y = latitude_rounded), col = "yellow", shape = 19, size = 2) +
  geom_point(data = grid_data %>% filter(Status == "Established", bio_year %in% c(2014:2019)),
             aes(x = longitude_rounded, y = latitude_rounded), size = 1, shape = 19, col = "grey") +
  geom_point(data = grid_data,
             aes(x = centroid[1], y = centroid[2]), col = "blue", shape = 4, size = 5) +
  geom_point(data = Thresholds %>%  filter(bio_year == 2019), 
             aes(x = longitude_rounded, y = latitude_rounded), col = "black", shape = 21, size = 2) +
  geom_point(data = Jumps %>%  filter(bio_year == 2019), 
             aes(x = longitude_rounded, y = latitude_rounded), fill = "blue", shape = 21, size = 2) +
  facet_wrap(~params, ncol = 4) +
  geom_text(data = states,
            aes(X, Y, label = code), size = 4) +
  labs(x = "Longitude", y = "Latitude")+
  geom_sf(data = states, alpha = 0) + 
  coord_sf(xlim = c(-81, -73), ylim = c(39, 42), expand = FALSE) + 
  theme(legend.position="top")

ggsave("../figures/vignette_quadrants/map_jumps_2019.jpg", map_jumps2019 , width = 15, height = 10)


map_jumps2020 <- ggplot(data = states) +
  geom_point(data = Jumps %>%  filter(bio_year %in% c(2014:2019)),
             aes(x = longitude_rounded, y = latitude_rounded), col = "yellow", shape = 19, size = 2) +
  geom_point(data = grid_data %>% filter(Status == "Established", bio_year %in% c(2014:2020)),
             aes(x = longitude_rounded, y = latitude_rounded), size = 1, shape = 19, col = "grey") +
  geom_point(data = grid_data,
             aes(x = centroid[1], y = centroid[2]), col = "blue", shape = 4, size = 5) +
  geom_point(data = Thresholds %>%  filter(bio_year == 2020), 
             aes(x = longitude_rounded, y = latitude_rounded), col = "black", shape = 21, size = 2) +
  geom_point(data = Jumps %>%  filter(bio_year == 2020), 
             aes(x = longitude_rounded, y = latitude_rounded), fill = "blue", shape = 21, size = 2) +
  facet_wrap(~params, ncol = 4) +
  geom_text(data = states,
            aes(X, Y, label = code), size = 4) +
  labs(x = "Longitude", y = "Latitude")+
  geom_sf(data = states, alpha = 0) + 
  coord_sf(xlim = c(-82, -72), ylim = c(38, 43), expand = FALSE) + 
  theme(legend.position="top")

ggsave("../figures/vignette_quadrants/map_jumps_2020.jpg", map_jumps2020 , width = 15, height = 10)

```


```{r map all jumps per params set}

# All jumps one set
map_jumps <- ggplot(data = states) +
  geom_point(data = grid_data %>% filter(Status == "Established", bio_year %in% c(2014:2020)),
             aes(x = longitude_rounded, y = latitude_rounded), size = 1, shape = 19, col = "grey") +
  geom_point(data = grid_data,
             aes(x = centroid[1], y = centroid[2]), col = "blue", shape = 4, size = 5) +
  geom_point(data = jumps, 
             aes(x = longitude_rounded, y = latitude_rounded, fill = as.factor(bio_year)), shape = 21, size = 2) +
  geom_text(data = states,
            aes(X, Y, label = code), size = 4) +
  labs(x = "Longitude", y = "Latitude")+
  geom_sf(data = states, alpha = 0) + 
  coord_sf(xlim = c(-82, -72), ylim = c(38, 43), expand = FALSE) + 
  theme(legend.position="top")

ggsave("./figures/vignette_quadrants/map_jumps.jpg", map_jumps , width = 8, height = 8)


```



## Remove duplicates for outbreaks

Find points with important outbreaks

```{r group_jumps aka function for finding outbreaks i.e. detecting groups of points and attributing them a group number}

group_jumps <- function(Jumps_oneparam, gap_size = 15) {
  
  Jumps_oneparam %<>% add_column(Group = NA,
                                ID = seq(1:length(Jumps_oneparam$DistToIntro)))
                                bio_year = unique(Jumps_oneparam$bio_year)

  Jumps_yearn <- Jumps_oneparam 

  # Calculate all pairwise distances between points
  jumps <- st_as_sf(x = Jumps_yearn, coords = c("longitude_rounded", "latitude_rounded"), crs = 4326, remove = F)
  jumps_proj <- st_transform(jumps, crs = 4326)
  pairwise_dist <- st_distance(x = jumps_proj)
  units(pairwise_dist) <- NULL
  pairwise_dist <- as.data.frame(pairwise_dist)
  
  # Create a table with, for each point, a list of all the neighbor points (which we are going to populate with a loop) 
  close_points <- tibble(Point = seq(1:length(Jumps_yearn$DistToIntro)), Neighbors = list(NULL))
  
  for (i in 1:dim(pairwise_dist)[1]) { #take each point of the table
    neighbors_i = c()     # Create an empty vector to contain the list of neighbors for this point
    
      for (j in 1:dim(pairwise_dist)[2]) { # go through all the distances
      if (pairwise_dist[i,j] < gap_size * 1000 & pairwise_dist[i,j] != 0) { # if a point is closer than a gap size to the other point, but not the same point
        neighbors_i <- c(neighbors_i, j) # Add it to the vector
        }
      }
    
    if (length(neighbors_i) > 0){ #if this point has neighbors,
    close_points$Neighbors[[i]] <- neighbors_i # Add the vector to a table with the sample at which it is attributed
    } 
  }


  # Merge the lists of neighbor points with common points to obtain groups
  # This is done by iterating the list of neighbors of each point created above
  point_list <- close_points$Point #put the names of all points in a vector
  group_name = 1
  
  while (length(point_list) > 1){ #while there are points in the list of points
    i = point_list[1] # take the first point of the list
    group_i = c(i, close_points$Neighbors[[i]]) # initiate the vector with the neighbors of the point i
    
    for (j in (i+1):length(close_points$Point)) { #consider the next point in the table
      if (is.null(close_points$Neighbors[[j]]) == F & #if the new point considered j has neighbors
          close_points$Point[[j]] %in% close_points$Neighbors[[i]]) { # and if this new point j is in the list of neighbors of i  
        for (k in close_points$Neighbors[[j]]){ #look at the list of neighbors of j
          if (!(k %in% group_i) & k != i){ # if the point in the list of neighbors of j is not in the list of neighbors of i
            group_i <- c(group_i, k) # add the point to the list of neighbors of i
          }
        }
      }
    }
    
    for (c in group_i){ #for each point in this final list
      Jumps_oneparam <- Jumps_oneparam %>% mutate(Group = replace(Group, ID == c, group_name)) # find their ID in the table and attribute them the group name
    }
    
    point_list <- point_list[!point_list %in% group_i] # now remove all these neighbor points from the list of points to find another group
    group_name = group_name + 1 # the next group will be called n+1
  }

  if (length(point_list) == 1) { # if there is a last point
    if (is.na(Jumps_oneparam$Group[[point_list]])){ # and this last point is not in any group so far
    Jumps_oneparam <- Jumps_oneparam %>% mutate(Group = replace(Group, ID == point_list, group_name)) #attribute the last group name to the last point
    }
  }
  
return(Jumps_oneparam)
}
```


```{r run group_jumps function to define groups}

# The function needs to run separately for each set of parameters since the list of jumps is not the same!
Jump_groups <- group_jumps(jumps, gap_size = 15)

#Check on a map!
ggplot(data = states) +
  geom_point(data = Jump_groups %>% filter(Group %in% c(5, 4, 18, 16)),
             aes(x = longitude_rounded, y = latitude_rounded, col = as.factor(Group)), 
             size = 2) +
  geom_text(data = states,
            aes(X, Y, label = code), size = 4) +
  labs(x = "Longitude", y = "Latitude")+
  geom_sf(data = states, alpha = 0) + 
  coord_sf(xlim = c(-82, -72), ylim = c(38, 43), expand = FALSE) + 
  theme(legend.position="right")

#Check how many points there are per group
Jump_groups_cumul <- Jump_groups %>% group_by(bio_year, Group) %>% summarise(Nb = n()) %>% arrange(-Nb) %>% filter(Nb > 1)
dim(Jump_groups_cumul)

write.csv(Jump_groups, file.path(here(), "exported-data", "jumps_groups.csv"), row.names=F)

```




Most jump events occurred in Harrisburg, PA, and Winchester, VA, in 2018. They might be true independent jumps, i.e. SLF hitchhiked multiple times to these locations the same year. Alternatively, they might be the result of SLF quickly spreading from a single jump event. Finally, they can be a mix between these two hypotheses. For the rest of the analyses, we will test the two most contrasted hypotheses in parallel in order to test whether results vary. For the first hypothesis (all points are independent introductions), the dataset consists of all jump points. For the second hypothesis (only one introduction in Harrisburg and Winchester), the dataset consists of each "grouped jumps" summarized each by their most central point.


```{r rarefy_groups function aka generate rarified dataset}

rarefy_groups <- function(Jump_groups) {

  # Create a dataset with centroids for groups
  Jumpers_centroids <- Jump_groups %>% group_by(bio_year, Group) %>% 
    summarise(latitude_rounded = mean(latitude_rounded), longitude_rounded = mean(longitude_rounded)) %>% 
    ungroup()
  
  # Prep a column in Jump_groups to store the distance of each point to the centroid (and find the closest one)
  Jump_groups %>% add_column(DistToCentroid = NA)
  
  # Map centroids to see how they fit in the group
  # ggplot(data = states) +
  #   geom_point(data = Jump_groups,
  #              aes(x = longitude_rounded, y = latitude_rounded, col = as.factor(Group)), 
  #              size = 2) +
  #   geom_point(data = Jumpers_centroids %>% filter(Group %in% Jump_groups_cumul$Group), #here I filtered only groups that had N > 1 point
  #              aes(x = longitude_rounded, y = latitude_rounded)) +
  #   geom_text(data = states,
  #             aes(X, Y, label = code), size = 4) +
  #   labs(x = "Longitude", y = "Latitude")+
  #   geom_sf(data = states, alpha = 0) + 
  #   coord_sf(xlim = c(-82, -72), ylim = c(38, 43), expand = FALSE) + 
  #   theme(legend.position="right")
  
  
  ##### Find the point closest to that centroid. 
  # Create the shapefiles with jumpers with unique points
  # Object with all jumps: Jump_groups
  Jumps_layer <- st_as_sf(x = Jump_groups, coords = c("longitude_rounded", "latitude_rounded"), crs = 4326, remove = F)
  Jumpers_proj <- st_transform(Jumps_layer)
  
  # Object with centroids: Jumpers_centroids
  Centroids_layer <- st_as_sf(x = Jumpers_centroids, coords = c("longitude_rounded", "latitude_rounded"), crs = 4326, remove = F)
  Centroids_proj <- st_transform(Centroids_layer)
  
  # Calculate their distance to the centroid
  for (j in 1:length(Jump_groups$Group)){ 
    Jumper_group = Jumpers_proj$Group[j]
    Jumpers_proj$DistToCentroid[j] <- st_distance(x = Jumpers_proj[j,], y = Centroids_proj %>% filter(Group == Jumper_group))
  }
  
  # Keep the closest point
  st_geometry(Jumpers_proj) <- NULL
  Jumpers_unique <- Jumpers_proj %>% group_by(Group) %>% slice(which.min(DistToCentroid)) %>% ungroup()

return(Jumpers_unique)
}


```


Assemble the datasets to have only one big dataset with all jumpers: different sets of parameters, full, and reduced.

```{r run rarify_groups function on all datasets}
Jumps_unique <- rarefy_groups(Jump_groups) %>% add_column(Rarefied = TRUE)

# Map it
map_rarified <- ggplot(data = states) +
  geom_sf(data = states, fill = "white") +
  coord_sf(xlim = c(-82, -72), ylim = c(38, 43), expand = FALSE) +
  geom_point(data = Jump_groups,
             aes(x = longitude_rounded, y = latitude_rounded, col = as.factor(Group)), shape = 19, size = 3) +
  geom_point(data = Jumps_unique, aes(x = longitude_rounded, y = latitude_rounded)) +
  labs(x = "Longitude", y = "Latitude")+
  theme(legend.position="right")

```

```{r assemble all datasets into one}

Jumps_unique %<>% select("longitude_rounded", "latitude_rounded", "bio_year", "Rarefied")
dim(Jump)
Jumps_full_rarefied <- merge(Results$Jump, Jumps_unique, by = c("latitude_rounded", "longitude_rounded", "bio_year"),
                             all = T)

dim(Results$Jump)[1] == dim(Jumps_full_rarefied)[1]
dim(Jumps_full_rarefied %>% filter(Rarefied == TRUE))[1] == dim(Jumps_unique)[1]

write.csv(Jumps_full_rarefied, "./exported-data/jumps_full_rarefied.csv", row.names = F)
```

## Figure 2

```{r load dataset}
Jumps <- read.csv(file.path(here(), "exported-data", "jumps_full_rarefied.csv"))
head(Jumps)
Jumps %<>% mutate(Rarefied = ifelse(is.na(Rarefied), "Full", "Rarefied"))
dim(Jumps)

Jumps$Rarefied <- factor(Jumps$Rarefied, levels = c( "Full", "Rarefied"))

#2nd version with colors
Jumps_year <- Jumps %>% group_by(bio_year, Rarefied) %>% summarise(n = n())
 
jumps_plot <- ggplot() +
  geom_bar(data = Jumps_year %>% filter(Rarefied == "Rarefied"), aes(x = bio_year, y = n, fill = as.factor(bio_year), group = Rarefied, col = as.factor(bio_year)#, alpha = Rarefied
                                                                     ), stat = "identity", lwd = .25) +
  scale_fill_manual(values = c("gold2", "firebrick3", "#0072B2", "#009E73")) +
  # scale_alpha_manual(values = c(0, 1)) +
  scale_color_manual(values = c("gold2", "firebrick3", "#0072B2", "#009E73")) +
  xlab("Biological year") +
  ylab("Number of jumps") +
  theme_classic() +
  guides(alpha = "none", fill = "none", col = "none") +
  theme(text = element_text(size = 10), legend.position = "top")

jumps_plot

ggsave("./figures/vignette_quadrants/jumps_plotv2.jpg", jumps_plot, height = 2.5, width = 4)



grid_data <- read.csv("./exported-data/grid_data.csv")
# The number of rows must be 58,915 with 2021 preliminary data
grid_data %<>% filter(!bio_year == 2021, longitude_rounded > -90)
centroid <- c(-75.675340, 40.415240)


# Map it
map_rarified <- ggplot(data = grid_data) +
  geom_sf(data = states, fill = "white") +
  geom_point(data = grid_data %>% filter(Status == "Established"), aes(x = longitude_rounded, y = latitude_rounded), col = "lightgrey") +
    geom_sf(data = states, alpha = 0) +
    geom_point(data = grid_data, aes(x = centroid[1], y = centroid[2]), col = "black", shape = 4, size = 5) +
  coord_sf(xlim = c(-81, -73), ylim = c(38, 43)) +
  geom_point(data = Jumps,
             aes(x = longitude_rounded, y = latitude_rounded, 
                 col = as.factor(bio_year), group = Rarefied, stroke = Rarefied, shape = Rarefied), size = 3) +
  scale_discrete_manual(aesthetics = "stroke", values = c('Rarefied' = 1, 'Full' = 2)) +
  scale_discrete_manual(aesthetics = "shape", values = c('Rarefied' = 19, 'Full' = 21)) +
  scale_color_manual(values = c("gold2", "firebrick3", "#0072B2", "#009E73")) +
  scale_fill_manual(values = c("gold2", "firebrick3", "#0072B2", "#009E73")) +
  scale_alpha_manual(values = c(0.5, 1)) +
  labs(x = "Longitude", y = "Latitude") +
  theme(legend.position="right", text = element_text(size = 10),
        panel.background = element_rect(fill = "white"),
        legend.key = element_rect(fill = "white")) +
   guides(colour = guide_legend("Biological year"), alpha = guide_legend("Dataset"), fill = guide_legend("Biological year"))

map_rarified

ggsave("./figures/vignette_quadrants/jumps_map.jpg", map_rarified, height = 4, width = 4)

library(cowplot)
fig2 <- ggdraw() +
  draw_plot(optim_plot, 0, .5, 1, .5) +
  draw_plot(jumps_plot, 0, 0, .5, .5) +
  draw_plot(map_rarified, .5, 0, .5, .5) +
  draw_plot_label(c("(A)", "(B)", "(C)"), c(0, 0, 0.5), c(1, 0.5, 0.5), size = 15)
fig2
ggsave("./figures/vignette_quadrants/fig2b.jpg", fig2, height = 10, width = 10)


jumps <- plot_grid(jumps_plot, map_rarified, labels=c("A", "B"), ncol = 2, nrow = 1)
ggsave("./figures/vignette_quadrants/fig2c.jpg", jumps, height = 5, width = 10)




# Fig 2D: mean dist to the threshold per year
# run and save Results$Dist
Thresholds <- read.csv("./exported-data/thresholds.csv")
Thresholds %<>% filter(rotation_nb == 1) %>% 
  select(longitude_rounded, latitude_rounded, bio_year, sectors_nb) %>% 
  rename(sectors = sectors_nb,
         latitude_threshold = latitude_rounded,
         longitude_threshold = longitude_rounded)
Thresholds

# Attribute sectors to each jump: find sector for rotation 0
Jumps
Jumps %>% filter(Rarefied == TRUE)
Jumps_sectors <- attribute_sectors(Jumps, nb_sectors = 20, centroid = c(-75.675340, 40.415240), rotation = 1) 
Jumps_sectors %<>% select(-c(theta.1, sectors)) %>% rename(sectors = rotation1)
Jumps_sectors %>% filter(Rarefied == TRUE)
dim(Jumps_sectors)

# Find corresponding threshold per bio year 
library(purrr)
JumpLength <- left_join(Jumps_sectors, Thresholds)
JumpLength %<>% rowwise() %>%  mutate(DistToThreshold = as.vector(distm(c(longitude_rounded, latitude_rounded), c(longitude_threshold, latitude_threshold), fun = distGeo))/1000)

names(JumpLength)   
JumpLength_tot <- rbind(JumpLength %>% filter(Rarefied == "Rarefied"),
                        JumpLength %>% mutate(Rarefied = "Full"))


MeanDist_jump <- ggplot() +
  geom_jitter(data = JumpLength,
             aes(x = as.factor(bio_year), y = DistToThreshold, 
                 # col = as.factor(bio_year), 
                 # fill = as.factor(bio_year),
                 shape = Rarefied), size = 3, show.legend = F) +
  scale_alpha_manual(values = c(0.8,0)) +
  scale_discrete_manual(aesthetics = "shape", values = c('Rarefied' = 19, 'Full' = 21)) +
  scale_color_manual(values = c("gold2", "firebrick3", "#0072B2", "#009E73")) +
  scale_fill_manual(values = c("gold2", "firebrick3", "#0072B2", "#009E73")) +
  labs(x = "Dataset", y = "Distance to the invasion front (km)") +
  theme_classic() +
  theme(legend.position="top", text = element_text(size = 10), plot.tag.position = c(0.01, 1)) +
  guides(fill = "none", shape = "none", alpha = "none")
MeanDist_jump



# Same but boxplots

JumpLength_tot$Rarefied <- factor(JumpLength_tot$Rarefied, levels = c("Rarefied", "Full"))
MeanDist_jump <- ggplot() +
  geom_boxplot(data = JumpLength %>% filter(Rarefied == "Rarefied"),
             aes(x = as.factor(bio_year), y = DistToThreshold, 
                 col = as.factor(bio_year),
                 fill = as.factor(bio_year),
                 alpha = Rarefied)) +
  scale_alpha_manual(values = c(0.7,0)) +
  scale_discrete_manual(aesthetics = "shape", values = c('Rarefied' = 19, 'Full' = 21)) +
  scale_color_manual(values = c("gold2", "firebrick3", "#0072B2", "#009E73")) +
  scale_fill_manual(values = c("gold2", "firebrick3", "#0072B2", "#009E73")) +
  labs(x = "Dataset", y = "Distance to the invasion front (km)") +
  theme_classic() +
  theme(legend.position="top", text = element_text(size = 10), plot.tag.position = c(0.01, 1)) +
  guides(fill = "none", shape = "none", col = "none", alpha = "none")
MeanDist_jump

JumpLength_tot %>% group_by(Rarefied, bio_year) %>% summarise(mean(DistToThreshold))

summary(aov(DistToThreshold ~ bio_year, data = JumpLength))
ggsave("./figures/vignette_quadrants/distv2.jpg", MeanDist_jump, height = 2.5, width = 4)


library(cowplot)
fig2 <- ggdraw() +
  draw_plot(map_rarified, x = 0, y = .33, width = 1, height = .66) +
  draw_plot(jumps_plot, 0, 0, .5, .33) +
  draw_plot(MeanDist_jump, .5, 0, .5, .33) +
  draw_plot_label(c("(a)", "(b)", "(c)"), c(0, 0, 0.5), c(1, 0.35, 0.35), size = 15) +
  theme(plot.background = element_rect(fill="#FFFFFF", color = NA))
fig2
ggsave("./figures/vignette_quadrants/fig2h.jpg", fig2, height = 10, width = 10)


library(gridExtra)
lay <- rbind(c(1, 1),
              c(1, 1),
              c(2, 3))
grid.arrange(grobs = list(map_rarified, jumps_plot, MeanDist_jump), 
             layout_matrix = lay,
             widths = c(5,5), heights = c(3.33,3.33,3.33))

```

\newpage

## Evolution of the radius of diffusive spread and jumps over time

We can now look at how the radius of the invasion increases over time, when differentiating diffusive spread and jump dispersal (Figure 6). In the westernmost disk portions, jump dispersal is responsible for the very high increase in the invasion radius. In the other disk portions, the spread seems to be mostly linked to diffusive dispersal. 

```{r figure total spread/diffusive spread for slf established, fig.cap = "Evolution of the radius of the invasion over time, when diffusive spread and jump dispersal are separated", echo=FALSE, message = FALSE}

#Data on total spread
spread_distances_tab <- grid_data_rotate0 %>% filter(Status == "Established" & bio_year != 2021) %>%
  group_by(Status, bio_year, portion) %>% 
  summarise(MaxDist = max(DistToIntro))


spread <- ggplot(data = spread_distances_tab, aes(x = bio_year, y = MaxDist)) +
  facet_wrap(~portion, ncol = 4) +
    geom_bar(stat="identity", fill = "white", col = "black") +
  geom_bar(data = Results_rotate4$Dist, aes(x = bio_year , y = DistToIntro), fill = "grey", col = "black", stat = "identity") +
  ylab("Invasion radius (km)")+
  xlab("Year") +
  theme_classic()

spread
```

```{r save evolution of spread radius, eval = params$savefiles}
ggsave("../figures/vignette_quadrants/evolution_spread_radius85_15.jpg", spread, width = 8, height = 6)
```

\newpage

# 5. Conclusion

The spread of the spotted lanternfly in the US is likely due both to diffusive spread and human-assisted jump dispersal. 75 jump occurrences have been identified, and most of them are situated in Winchester (north VA) and western Pennsylvania (especially Harrisburg). 

Jump events are likely be caused by SLF hitchhiking on human transports, and establishing near transport infrastructures: railroads, roads, and airports. In the next vignette, we will test the significance of the proximity between jump events and transport infrastructures by a comparison with a random distribution. We will also compare these distances to those of diffusers (SLF spread through diffusive spread) and of points where SLF were not detected, to check for a potential bias in survey locations.